{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA classifier with a committee of 48 networks\n",
    "# Optimal configuration from Kuntzer's paper https://arxiv.org/abs/1605.03201\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '/Users/ec270266/Documents/Phd/Euclid/dev/output/psf_dataset/'\n",
    "dataset = np.load(data_path + 'PCA_dataset2B24.npy', allow_pickle=True)[()]\n",
    "\n",
    "x_train = dataset['train_stars_pca']\n",
    "x_val = dataset['validation_stars_pca']\n",
    "x_test = dataset['test_stars_pca']\n",
    "y_train = dataset['train_C']\n",
    "y_val = dataset['validation_C']\n",
    "y_test = dataset['test_C']\n",
    "SED_test = dataset['test_SEDs']\n",
    "SED_train= dataset['train_SEDs']\n",
    "SED_val= dataset['validation_SEDs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24308,)\n",
      "(7692,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEDlisttoC(SED_list):\n",
    "    \"\"\"Converts a stellar class (1 to 13) to the regression parameter C.\"\"\"\n",
    "    sed_array = np.array(SED_list)\n",
    "    return sed_array*0.5 + 1.5\n",
    "\n",
    "def CtoSEDarray(c_values, variance):\n",
    "    \"\"\"Converts the regression parameter C back to a stellar class.\n",
    "    If the C is out of bounds or its variance is too high it is classified as an anomaly (stellar class 20).\"\"\"\n",
    "    sed_classes = np.rint(((c_values - 1.5) * 2)).astype(int)\n",
    "    sed_classes = np.where((c_values < 1.5) | (c_values > 7.5), 13, sed_classes)\n",
    "    sed_classes = np.where((variance > 1.00), 13, sed_classes)\n",
    "    return sed_classes\n",
    "\n",
    "def calculate_success_rate(confusion_matrix):\n",
    "    \"\"\"Metric that contemplates success as the true spectral class with a tolerance of one adjacent class.\"\"\"\n",
    "    diagonal = np.trace(confusion_matrix)\n",
    "    diagonal_neighbors = np.sum(np.diagonal(confusion_matrix, offset=1)) + np.sum(np.diagonal(confusion_matrix, offset=-1))\n",
    "    total_classified = np.sum(confusion_matrix)\n",
    "    \n",
    "    success_rate = (diagonal + diagonal_neighbors) / total_classified\n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "PCA_components = 24\n",
    "model_learning_rate = 0.1\n",
    "N_epochs = 100\n",
    "N_committee = 48\n",
    "patience_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network # 0\n",
      "Epoch 1/100\n",
      "116/760 [===>..........................] - ETA: 0s - loss: 3.9977   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ec270266/anaconda3/envs/wavediff/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 1s 547us/step - loss: 2.2324 - val_loss: 0.4400\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 581us/step - loss: 0.4495 - val_loss: 0.4010\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.3463 - val_loss: 0.3234\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.2746 - val_loss: 0.3303\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2475 - val_loss: 0.2203\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.2365 - val_loss: 0.2215\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.2113 - val_loss: 0.1826\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1988 - val_loss: 0.2418\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1864 - val_loss: 0.1906\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 545us/step - loss: 0.1800 - val_loss: 0.1657\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1722 - val_loss: 0.1574\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1686 - val_loss: 0.1638\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1629 - val_loss: 0.2021\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1592 - val_loss: 0.1464\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1511 - val_loss: 0.1539\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1481 - val_loss: 0.1948\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.1460 - val_loss: 0.1335\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1455 - val_loss: 0.1376\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1408 - val_loss: 0.1264\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1409 - val_loss: 0.1523\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1426 - val_loss: 0.1353\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1333 - val_loss: 0.1380\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1346 - val_loss: 0.1202\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1305 - val_loss: 0.1149\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1325 - val_loss: 0.1119\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1300 - val_loss: 0.1270\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 572us/step - loss: 0.1279 - val_loss: 0.1265\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1269 - val_loss: 0.1457\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1319 - val_loss: 0.1186\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1267 - val_loss: 0.1258\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 569us/step - loss: 0.1315 - val_loss: 0.1171\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1232 - val_loss: 0.1149\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1270 - val_loss: 0.1247\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1237 - val_loss: 0.1327\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1226 - val_loss: 0.1160\n",
      "Training completed. Number of epochs: 35 , Final training loss: 0.12264630198478699 , Final validation loss: 0.1159953698515892\n",
      "Training network # 1\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 541us/step - loss: 2.3610 - val_loss: 0.4338\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.4514 - val_loss: 0.4187\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 486us/step - loss: 0.3910 - val_loss: 0.2970\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2903 - val_loss: 0.2640\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.2433 - val_loss: 0.2059\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.2387 - val_loss: 0.3251\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 544us/step - loss: 0.2152 - val_loss: 0.1934\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1978 - val_loss: 0.1870\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1853 - val_loss: 0.1707\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1812 - val_loss: 0.1765\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1660 - val_loss: 0.1398\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1580 - val_loss: 0.1396\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1566 - val_loss: 0.1943\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1470 - val_loss: 0.1336\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.1453 - val_loss: 0.1394\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1456 - val_loss: 0.1652\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1402 - val_loss: 0.1538\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1390 - val_loss: 0.1374\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1333 - val_loss: 0.1196\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1324 - val_loss: 0.1539\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1370 - val_loss: 0.1181\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1279 - val_loss: 0.1438\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1288 - val_loss: 0.1092\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1307 - val_loss: 0.1126\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1243 - val_loss: 0.1106\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1281 - val_loss: 0.1229\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1247 - val_loss: 0.1115\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1241 - val_loss: 0.1108\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1232 - val_loss: 0.1132\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1244 - val_loss: 0.1062\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1227 - val_loss: 0.1290\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 486us/step - loss: 0.1183 - val_loss: 0.1264\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1199 - val_loss: 0.1239\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1210 - val_loss: 0.1158\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1184 - val_loss: 0.1066\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1191 - val_loss: 0.1289\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1191 - val_loss: 0.1025\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1164 - val_loss: 0.1782\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1194 - val_loss: 0.1200\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1168 - val_loss: 0.1139\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1156 - val_loss: 0.1073\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1148 - val_loss: 0.1254\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1145 - val_loss: 0.1153\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1125 - val_loss: 0.1256\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1149 - val_loss: 0.1253\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1118 - val_loss: 0.1018\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1156 - val_loss: 0.1040\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1104 - val_loss: 0.1094\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1113 - val_loss: 0.1195\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1108 - val_loss: 0.1324\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1120 - val_loss: 0.1041\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1117 - val_loss: 0.1093\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1111 - val_loss: 0.1323\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1092 - val_loss: 0.1308\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1127 - val_loss: 0.1094\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.1127 - val_loss: 0.1077\n",
      "Training completed. Number of epochs: 56 , Final training loss: 0.11265981942415237 , Final validation loss: 0.10774880647659302\n",
      "Training network # 2\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 2.6399 - val_loss: 0.5184\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.4464 - val_loss: 0.4070\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.3298 - val_loss: 0.2576\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.2658 - val_loss: 0.2104\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 488us/step - loss: 0.2391 - val_loss: 0.2098\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.2137 - val_loss: 0.2594\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.2048 - val_loss: 0.1846\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1914 - val_loss: 0.1657\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1841 - val_loss: 0.1575\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1805 - val_loss: 0.1430\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1690 - val_loss: 0.1776\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1661 - val_loss: 0.1756\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1563 - val_loss: 0.2017\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1569 - val_loss: 0.1539\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1559 - val_loss: 0.1394\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1493 - val_loss: 0.1392\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1476 - val_loss: 0.1255\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1442 - val_loss: 0.1300\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1418 - val_loss: 0.1319\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1374 - val_loss: 0.1460\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1365 - val_loss: 0.1828\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1409 - val_loss: 0.1242\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1363 - val_loss: 0.1323\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1320 - val_loss: 0.1246\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1284 - val_loss: 0.1159\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1326 - val_loss: 0.1136\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1274 - val_loss: 0.1172\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1305 - val_loss: 0.1173\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1265 - val_loss: 0.1150\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1257 - val_loss: 0.1244\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1257 - val_loss: 0.1080\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1264 - val_loss: 0.1494\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1272 - val_loss: 0.1207\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1269 - val_loss: 0.1468\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.1232 - val_loss: 0.1183\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1228 - val_loss: 0.1223\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1232 - val_loss: 0.1234\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1219 - val_loss: 0.1327\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1215 - val_loss: 0.1017\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1218 - val_loss: 0.1057\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1189 - val_loss: 0.1202\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1193 - val_loss: 0.1740\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1227 - val_loss: 0.1087\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1203 - val_loss: 0.1077\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1152 - val_loss: 0.1129\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1159 - val_loss: 0.1134\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1168 - val_loss: 0.1144\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1187 - val_loss: 0.1045\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 488us/step - loss: 0.1183 - val_loss: 0.1096\n",
      "Training completed. Number of epochs: 49 , Final training loss: 0.11828212440013885 , Final validation loss: 0.1096084713935852\n",
      "Training network # 3\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 539us/step - loss: 3.6691 - val_loss: 3.5459\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 3.6233 - val_loss: 4.2438\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 3.6218 - val_loss: 2.8652\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.5471 - val_loss: 0.3336\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.3118 - val_loss: 0.2683\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2684 - val_loss: 0.2922\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.2401 - val_loss: 0.2348\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 486us/step - loss: 0.2202 - val_loss: 0.2507\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2041 - val_loss: 0.1725\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1976 - val_loss: 0.2022\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.1827 - val_loss: 0.1898\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1727 - val_loss: 0.1434\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1630 - val_loss: 0.1690\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1665 - val_loss: 0.1588\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1569 - val_loss: 0.1637\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1520 - val_loss: 0.1403\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1475 - val_loss: 0.1269\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 545us/step - loss: 0.1466 - val_loss: 0.1199\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1408 - val_loss: 0.1327\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1405 - val_loss: 0.1181\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1360 - val_loss: 0.1183\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1350 - val_loss: 0.1384\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.1331 - val_loss: 0.1229\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1313 - val_loss: 0.1349\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1277 - val_loss: 0.2298\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1284 - val_loss: 0.1173\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1300 - val_loss: 0.1167\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1297 - val_loss: 0.1266\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1266 - val_loss: 0.1119\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1215 - val_loss: 0.1156\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1191 - val_loss: 0.1162\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1197 - val_loss: 0.1124\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1240 - val_loss: 0.1078\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1170 - val_loss: 0.1048\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1214 - val_loss: 0.1439\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1147 - val_loss: 0.1037\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1179 - val_loss: 0.1177\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1162 - val_loss: 0.1067\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1173 - val_loss: 0.1553\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1143 - val_loss: 0.1025\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1197 - val_loss: 0.1457\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1139 - val_loss: 0.1046\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1145 - val_loss: 0.1236\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1156 - val_loss: 0.1105\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1141 - val_loss: 0.1031\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1144 - val_loss: 0.1228\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1140 - val_loss: 0.1052\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1133 - val_loss: 0.1021\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1132 - val_loss: 0.1070\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.1123 - val_loss: 0.1017\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1108 - val_loss: 0.0978\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.1098 - val_loss: 0.0984\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1116 - val_loss: 0.1012\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1091 - val_loss: 0.1005\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1096 - val_loss: 0.0994\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1117 - val_loss: 0.1128\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1083 - val_loss: 0.1066\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1092 - val_loss: 0.1044\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1089 - val_loss: 0.0985\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1099 - val_loss: 0.1026\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1106 - val_loss: 0.1069\n",
      "Training completed. Number of epochs: 61 , Final training loss: 0.11057989299297333 , Final validation loss: 0.10686561465263367\n",
      "Training network # 4\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 526us/step - loss: 1.1779 - val_loss: 0.6762\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.4948 - val_loss: 0.4472\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.3995 - val_loss: 0.3413\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.3227 - val_loss: 0.2826\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2691 - val_loss: 0.2998\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.2531 - val_loss: 0.2123\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.2346 - val_loss: 0.2416\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.2185 - val_loss: 0.1876\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.2003 - val_loss: 0.1795\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1932 - val_loss: 0.1768\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1849 - val_loss: 0.1696\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1819 - val_loss: 0.1721\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1793 - val_loss: 0.1657\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1669 - val_loss: 0.1881\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1730 - val_loss: 0.1702\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1616 - val_loss: 0.1738\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1678 - val_loss: 0.1841\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 464us/step - loss: 0.1587 - val_loss: 0.1620\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1599 - val_loss: 0.1500\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1597 - val_loss: 0.1437\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1545 - val_loss: 0.1442\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1541 - val_loss: 0.1340\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1540 - val_loss: 0.1373\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1507 - val_loss: 0.1456\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1486 - val_loss: 0.1566\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1462 - val_loss: 0.1515\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.1450 - val_loss: 0.1425\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1482 - val_loss: 0.1564\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1407 - val_loss: 0.1321\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1483 - val_loss: 0.1542\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1399 - val_loss: 0.1447\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1437 - val_loss: 0.1407\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1441 - val_loss: 0.1356\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1403 - val_loss: 0.1347\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1385 - val_loss: 0.1369\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1429 - val_loss: 0.1258\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1411 - val_loss: 0.1400\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1403 - val_loss: 0.1656\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1385 - val_loss: 0.1296\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1382 - val_loss: 0.1332\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1355 - val_loss: 0.1176\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1395 - val_loss: 0.1619\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1379 - val_loss: 0.1235\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1377 - val_loss: 0.1483\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1333 - val_loss: 0.1230\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1389 - val_loss: 0.1273\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1334 - val_loss: 0.1289\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1332 - val_loss: 0.1277\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1362 - val_loss: 0.1274\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1357 - val_loss: 0.1330\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1344 - val_loss: 0.1487\n",
      "Training completed. Number of epochs: 51 , Final training loss: 0.13442669808864594 , Final validation loss: 0.14874301850795746\n",
      "Training network # 5\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 3.6634 - val_loss: 3.7908\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 3.6061 - val_loss: 3.5432\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 1.7646 - val_loss: 0.5673\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.3775 - val_loss: 0.3694\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2714 - val_loss: 0.2430\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.2427 - val_loss: 0.2174\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.2220 - val_loss: 0.2211\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.2150 - val_loss: 0.1835\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1983 - val_loss: 0.1600\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1905 - val_loss: 0.1916\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1748 - val_loss: 0.1600\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1664 - val_loss: 0.1533\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1594 - val_loss: 0.2017\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 464us/step - loss: 0.1630 - val_loss: 0.1981\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1420 - val_loss: 0.1249\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1421 - val_loss: 0.1594\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1443 - val_loss: 0.1961\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1388 - val_loss: 0.1275\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1314 - val_loss: 0.1229\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1333 - val_loss: 0.1303\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1345 - val_loss: 0.1650\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1274 - val_loss: 0.1251\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1272 - val_loss: 0.1100\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1279 - val_loss: 0.1272\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1269 - val_loss: 0.1207\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1269 - val_loss: 0.1211\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1236 - val_loss: 0.1153\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1199 - val_loss: 0.1078\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1250 - val_loss: 0.1069\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1194 - val_loss: 0.1149\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1193 - val_loss: 0.1156\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1214 - val_loss: 0.1298\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1206 - val_loss: 0.1093\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1181 - val_loss: 0.1214\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1154 - val_loss: 0.1061\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1173 - val_loss: 0.1048\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1116 - val_loss: 0.1051\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1163 - val_loss: 0.1047\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1145 - val_loss: 0.1149\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1119 - val_loss: 0.1079\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1134 - val_loss: 0.1225\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1141 - val_loss: 0.1049\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1134 - val_loss: 0.1207\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1123 - val_loss: 0.1153\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1113 - val_loss: 0.1073\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1110 - val_loss: 0.1013\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1103 - val_loss: 0.0958\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1104 - val_loss: 0.1173\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1103 - val_loss: 0.1142\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1091 - val_loss: 0.0950\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1088 - val_loss: 0.1019\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1096 - val_loss: 0.0975\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1097 - val_loss: 0.1225\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1103 - val_loss: 0.0957\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1130 - val_loss: 0.0954\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1063 - val_loss: 0.0988\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1080 - val_loss: 0.1035\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1091 - val_loss: 0.1116\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1077 - val_loss: 0.0997\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1075 - val_loss: 0.1066\n",
      "Training completed. Number of epochs: 60 , Final training loss: 0.10750097781419754 , Final validation loss: 0.1065710261464119\n",
      "Training network # 6\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 1.2298 - val_loss: 0.4317\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.4623 - val_loss: 0.4196\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.4015 - val_loss: 0.3315\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.3273 - val_loss: 0.4152\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.2698 - val_loss: 0.2515\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 466us/step - loss: 0.2474 - val_loss: 0.2477\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.2238 - val_loss: 0.1912\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2113 - val_loss: 0.1808\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1931 - val_loss: 0.2360\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 486us/step - loss: 0.1904 - val_loss: 0.2465\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1829 - val_loss: 0.1683\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1799 - val_loss: 0.1687\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1736 - val_loss: 0.1513\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1761 - val_loss: 0.1643\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1672 - val_loss: 0.1496\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1625 - val_loss: 0.2027\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1642 - val_loss: 0.1463\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1609 - val_loss: 0.1388\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1556 - val_loss: 0.1386\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1516 - val_loss: 0.2233\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1535 - val_loss: 0.1393\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1498 - val_loss: 0.1276\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1478 - val_loss: 0.1407\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1450 - val_loss: 0.2727\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1455 - val_loss: 0.1288\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1464 - val_loss: 0.1640\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1421 - val_loss: 0.1247\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1385 - val_loss: 0.1260\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1445 - val_loss: 0.1272\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 466us/step - loss: 0.1431 - val_loss: 0.1324\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1380 - val_loss: 0.1221\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1359 - val_loss: 0.1346\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1355 - val_loss: 0.1395\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1354 - val_loss: 0.1671\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1349 - val_loss: 0.1193\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1352 - val_loss: 0.1145\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 468us/step - loss: 0.1338 - val_loss: 0.1330\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 467us/step - loss: 0.1334 - val_loss: 0.1323\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1290 - val_loss: 0.1352\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.1291 - val_loss: 0.1323\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1359 - val_loss: 0.1321\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1291 - val_loss: 0.1201\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1321 - val_loss: 0.1233\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1294 - val_loss: 0.1131\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 470us/step - loss: 0.1265 - val_loss: 0.1105\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1283 - val_loss: 0.1163\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1314 - val_loss: 0.1317\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1271 - val_loss: 0.1268\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1255 - val_loss: 0.1420\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1292 - val_loss: 0.1188\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1254 - val_loss: 0.1273\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1254 - val_loss: 0.1286\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1237 - val_loss: 0.1317\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1262 - val_loss: 0.1340\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1235 - val_loss: 0.1081\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1256 - val_loss: 0.1455\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1209 - val_loss: 0.1168\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1229 - val_loss: 0.1052\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1191 - val_loss: 0.1121\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 472us/step - loss: 0.1206 - val_loss: 0.1125\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1205 - val_loss: 0.1166\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1213 - val_loss: 0.1212\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1230 - val_loss: 0.1261\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1180 - val_loss: 0.1454\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 469us/step - loss: 0.1164 - val_loss: 0.1060\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1190 - val_loss: 0.1133\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1174 - val_loss: 0.1053\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1199 - val_loss: 0.1614\n",
      "Training completed. Number of epochs: 68 , Final training loss: 0.11992694437503815 , Final validation loss: 0.1614210307598114\n",
      "Training network # 7\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 547us/step - loss: 1.6730 - val_loss: 0.4509\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.4513 - val_loss: 0.4742\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.3722 - val_loss: 0.3156\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2875 - val_loss: 0.2796\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.2519 - val_loss: 0.2126\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.2379 - val_loss: 0.1903\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.2177 - val_loss: 0.1840\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.2043 - val_loss: 0.1745\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.2000 - val_loss: 0.1668\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1906 - val_loss: 0.1514\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1807 - val_loss: 0.1652\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.1684 - val_loss: 0.1560\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1678 - val_loss: 0.1612\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.1725 - val_loss: 0.1537\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1668 - val_loss: 0.1658\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1680 - val_loss: 0.1426\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1642 - val_loss: 0.1542\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1582 - val_loss: 0.2128\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 487us/step - loss: 0.1561 - val_loss: 0.1626\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1528 - val_loss: 0.2056\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.1524 - val_loss: 0.1370\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1494 - val_loss: 0.1851\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1439 - val_loss: 0.1284\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1433 - val_loss: 0.1412\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1477 - val_loss: 0.1229\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1444 - val_loss: 0.1292\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1415 - val_loss: 0.1464\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.1368 - val_loss: 0.1309\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1406 - val_loss: 0.1206\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1373 - val_loss: 0.1258\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1363 - val_loss: 0.1145\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1331 - val_loss: 0.1204\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1371 - val_loss: 0.1161\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1336 - val_loss: 0.1362\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1317 - val_loss: 0.1136\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1286 - val_loss: 0.1244\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1273 - val_loss: 0.1169\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1310 - val_loss: 0.1158\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1292 - val_loss: 0.1256\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1261 - val_loss: 0.1456\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1273 - val_loss: 0.1193\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1269 - val_loss: 0.1186\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1262 - val_loss: 0.1347\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1263 - val_loss: 0.1218\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.1274 - val_loss: 0.1226\n",
      "Training completed. Number of epochs: 45 , Final training loss: 0.12744246423244476 , Final validation loss: 0.1225801631808281\n",
      "Training network # 8\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 532us/step - loss: 1.0773 - val_loss: 0.4689\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.5042 - val_loss: 0.4396\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.4571 - val_loss: 0.4359\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.4090 - val_loss: 0.3408\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.3280 - val_loss: 0.3209\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.2796 - val_loss: 0.2395\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.2476 - val_loss: 0.2259\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.2196 - val_loss: 0.1880\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.2113 - val_loss: 0.1883\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1939 - val_loss: 0.1765\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.1885 - val_loss: 0.1937\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1862 - val_loss: 0.1646\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1842 - val_loss: 0.1702\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1714 - val_loss: 0.1679\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 475us/step - loss: 0.1703 - val_loss: 0.1605\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1693 - val_loss: 0.1696\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1674 - val_loss: 0.1490\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1637 - val_loss: 0.2284\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1650 - val_loss: 0.1697\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1572 - val_loss: 0.1534\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1564 - val_loss: 0.1458\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1550 - val_loss: 0.2102\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.1537 - val_loss: 0.1413\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1530 - val_loss: 0.1615\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1506 - val_loss: 0.1314\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1502 - val_loss: 0.1422\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1471 - val_loss: 0.1687\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1486 - val_loss: 0.1725\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 480us/step - loss: 0.1443 - val_loss: 0.1492\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1484 - val_loss: 0.1719\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1419 - val_loss: 0.1414\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1439 - val_loss: 0.1292\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1406 - val_loss: 0.1606\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.1461 - val_loss: 0.1323\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 473us/step - loss: 0.1383 - val_loss: 0.1410\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1428 - val_loss: 0.1409\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.1370 - val_loss: 0.1327\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 471us/step - loss: 0.1369 - val_loss: 0.1343\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1382 - val_loss: 0.1227\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1361 - val_loss: 0.1431\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1388 - val_loss: 0.1445\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 477us/step - loss: 0.1321 - val_loss: 0.1206\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 476us/step - loss: 0.1337 - val_loss: 0.1159\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1322 - val_loss: 0.1220\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 483us/step - loss: 0.1322 - val_loss: 0.1220\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.1317 - val_loss: 0.1856\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1338 - val_loss: 0.1171\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 0.1360 - val_loss: 0.1324\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1289 - val_loss: 0.1174\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 478us/step - loss: 0.1296 - val_loss: 0.1165\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1289 - val_loss: 0.1333\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 474us/step - loss: 0.1316 - val_loss: 0.1517\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1310 - val_loss: 0.1252\n",
      "Training completed. Number of epochs: 53 , Final training loss: 0.13096073269844055 , Final validation loss: 0.1251535415649414\n",
      "Training network # 9\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 541us/step - loss: 3.6601 - val_loss: 3.5493\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 479us/step - loss: 3.6240 - val_loss: 3.5802\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 2.3461 - val_loss: 0.5037\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 485us/step - loss: 0.3768 - val_loss: 0.2889\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 484us/step - loss: 0.2800 - val_loss: 0.2414\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.2428 - val_loss: 0.2017\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2204 - val_loss: 0.2383\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 488us/step - loss: 0.1986 - val_loss: 0.1714\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 481us/step - loss: 0.2015 - val_loss: 0.2235\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 482us/step - loss: 0.1871 - val_loss: 0.1560\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1707 - val_loss: 0.1500\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1705 - val_loss: 0.2248\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1577 - val_loss: 0.1459\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1629 - val_loss: 0.1354\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 545us/step - loss: 0.1598 - val_loss: 0.1293\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1473 - val_loss: 0.1314\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.1480 - val_loss: 0.2197\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1446 - val_loss: 0.1300\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 557us/step - loss: 0.1419 - val_loss: 0.1395\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 550us/step - loss: 0.1377 - val_loss: 0.1507\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 548us/step - loss: 0.1332 - val_loss: 0.1295\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1358 - val_loss: 0.1210\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1369 - val_loss: 0.1222\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1348 - val_loss: 0.1179\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1294 - val_loss: 0.1504\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 488us/step - loss: 0.1309 - val_loss: 0.1225\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 576us/step - loss: 0.1286 - val_loss: 0.1106\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1246 - val_loss: 0.1325\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 551us/step - loss: 0.1239 - val_loss: 0.1116\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1241 - val_loss: 0.1281\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 594us/step - loss: 0.1306 - val_loss: 0.1210\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1236 - val_loss: 0.1135\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1235 - val_loss: 0.1360\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.1254 - val_loss: 0.1202\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.1216 - val_loss: 0.1194\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 598us/step - loss: 0.1209 - val_loss: 0.1160\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1193 - val_loss: 0.1106\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1221 - val_loss: 0.1154\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1199 - val_loss: 0.1184\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1193 - val_loss: 0.1408\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1210 - val_loss: 0.1163\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1173 - val_loss: 0.1487\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1222 - val_loss: 0.1113\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1183 - val_loss: 0.1027\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1172 - val_loss: 0.1321\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1166 - val_loss: 0.1017\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1153 - val_loss: 0.1147\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1134 - val_loss: 0.1718\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1180 - val_loss: 0.1011\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1153 - val_loss: 0.1256\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1115 - val_loss: 0.1245\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1157 - val_loss: 0.1211\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1111 - val_loss: 0.1047\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1180 - val_loss: 0.1004\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1130 - val_loss: 0.1047\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1132 - val_loss: 0.1054\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1139 - val_loss: 0.1099\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1139 - val_loss: 0.1029\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1131 - val_loss: 0.1026\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1135 - val_loss: 0.1037\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1123 - val_loss: 0.1250\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1114 - val_loss: 0.1002\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1104 - val_loss: 0.1022\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1135 - val_loss: 0.1194\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1127 - val_loss: 0.1138\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.1114 - val_loss: 0.1195\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1130 - val_loss: 0.1257\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1101 - val_loss: 0.1668\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1116 - val_loss: 0.1473\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1107 - val_loss: 0.0970\n",
      "Epoch 71/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1089 - val_loss: 0.1013\n",
      "Epoch 72/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1100 - val_loss: 0.1147\n",
      "Epoch 73/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1107 - val_loss: 0.1048\n",
      "Epoch 74/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1102 - val_loss: 0.1144\n",
      "Epoch 75/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1081 - val_loss: 0.0939\n",
      "Epoch 76/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1095 - val_loss: 0.1180\n",
      "Epoch 77/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1105 - val_loss: 0.1038\n",
      "Epoch 78/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1084 - val_loss: 0.1096\n",
      "Epoch 79/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1078 - val_loss: 0.1028\n",
      "Epoch 80/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1078 - val_loss: 0.1026\n",
      "Epoch 81/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1125 - val_loss: 0.1141\n",
      "Epoch 82/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1102 - val_loss: 0.1209\n",
      "Epoch 83/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1075 - val_loss: 0.1075\n",
      "Epoch 84/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1095 - val_loss: 0.0984\n",
      "Epoch 85/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1094 - val_loss: 0.1077\n",
      "Training completed. Number of epochs: 85 , Final training loss: 0.1093757301568985 , Final validation loss: 0.107707180082798\n",
      "Training network # 10\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 557us/step - loss: 3.6935 - val_loss: 3.5618\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 3.6144 - val_loss: 3.7089\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 1.9391 - val_loss: 0.4266\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.3565 - val_loss: 0.2764\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2844 - val_loss: 0.2391\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2505 - val_loss: 0.2188\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.2279 - val_loss: 0.2074\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2204 - val_loss: 0.3162\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.2013 - val_loss: 0.1672\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1810 - val_loss: 0.1537\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1727 - val_loss: 0.1680\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1662 - val_loss: 0.1786\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1519 - val_loss: 0.1414\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1512 - val_loss: 0.1760\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1468 - val_loss: 0.1193\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1404 - val_loss: 0.1200\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1343 - val_loss: 0.1349\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1372 - val_loss: 0.1409\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1340 - val_loss: 0.1188\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1297 - val_loss: 0.1497\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1340 - val_loss: 0.1115\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1289 - val_loss: 0.1338\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1284 - val_loss: 0.1214\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1288 - val_loss: 0.1194\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1247 - val_loss: 0.1199\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1253 - val_loss: 0.1503\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1260 - val_loss: 0.1057\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1243 - val_loss: 0.1057\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1239 - val_loss: 0.1314\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1208 - val_loss: 0.1074\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1222 - val_loss: 0.1347\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1176 - val_loss: 0.1044\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1197 - val_loss: 0.1100\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1219 - val_loss: 0.1069\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1204 - val_loss: 0.1324\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1159 - val_loss: 0.1265\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1166 - val_loss: 0.1336\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1205 - val_loss: 0.1178\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1152 - val_loss: 0.1051\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1166 - val_loss: 0.1001\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1160 - val_loss: 0.1163\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1144 - val_loss: 0.1438\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1189 - val_loss: 0.1082\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1128 - val_loss: 0.1130\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1140 - val_loss: 0.1093\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1150 - val_loss: 0.1061\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1125 - val_loss: 0.1021\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1131 - val_loss: 0.1054\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1127 - val_loss: 0.1040\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1113 - val_loss: 0.1104\n",
      "Training completed. Number of epochs: 50 , Final training loss: 0.11134867370128632 , Final validation loss: 0.11039931327104568\n",
      "Training network # 11\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 555us/step - loss: 3.7044 - val_loss: 3.6855\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 3.6159 - val_loss: 3.5660\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 3.6053 - val_loss: 3.6687\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 3.2074 - val_loss: 0.4628\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.4036 - val_loss: 0.5540\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2759 - val_loss: 0.2254\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.2345 - val_loss: 0.2005\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2226 - val_loss: 0.3847\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1999 - val_loss: 0.1771\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1891 - val_loss: 0.1636\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1790 - val_loss: 0.1541\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1739 - val_loss: 0.1531\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1717 - val_loss: 0.1625\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1604 - val_loss: 0.2430\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1609 - val_loss: 0.1449\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1535 - val_loss: 0.1318\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1502 - val_loss: 0.1399\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1505 - val_loss: 0.1217\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1422 - val_loss: 0.1451\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1440 - val_loss: 0.1429\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1327 - val_loss: 0.1228\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1372 - val_loss: 0.1294\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1354 - val_loss: 0.1381\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1332 - val_loss: 0.1396\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1318 - val_loss: 0.1242\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1303 - val_loss: 0.1108\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1310 - val_loss: 0.1180\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1247 - val_loss: 0.1217\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1239 - val_loss: 0.1390\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1258 - val_loss: 0.1126\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1219 - val_loss: 0.1401\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1269 - val_loss: 0.1307\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1218 - val_loss: 0.1139\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1203 - val_loss: 0.1407\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1217 - val_loss: 0.1090\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1211 - val_loss: 0.1233\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1188 - val_loss: 0.1077\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1189 - val_loss: 0.1181\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1182 - val_loss: 0.1220\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1179 - val_loss: 0.1071\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1183 - val_loss: 0.1120\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1185 - val_loss: 0.1337\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1167 - val_loss: 0.1393\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1153 - val_loss: 0.1095\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1169 - val_loss: 0.1059\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1134 - val_loss: 0.1034\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1123 - val_loss: 0.1079\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1163 - val_loss: 0.1015\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1148 - val_loss: 0.1078\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1161 - val_loss: 0.1117\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1168 - val_loss: 0.1945\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1128 - val_loss: 0.1013\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1133 - val_loss: 0.1044\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1107 - val_loss: 0.1274\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1134 - val_loss: 0.1168\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1132 - val_loss: 0.1058\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1135 - val_loss: 0.1055\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1109 - val_loss: 0.1042\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1110 - val_loss: 0.1125\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1144 - val_loss: 0.0965\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1093 - val_loss: 0.1183\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1126 - val_loss: 0.1247\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1138 - val_loss: 0.1278\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1111 - val_loss: 0.1053\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1073 - val_loss: 0.1412\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1107 - val_loss: 0.1086\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1125 - val_loss: 0.1016\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1071 - val_loss: 0.1105\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1102 - val_loss: 0.1169\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1083 - val_loss: 0.1061\n",
      "Training completed. Number of epochs: 70 , Final training loss: 0.10825969278812408 , Final validation loss: 0.10605419427156448\n",
      "Training network # 12\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 573us/step - loss: 1.4931 - val_loss: 0.4454\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.4715 - val_loss: 0.4165\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.3965 - val_loss: 0.2938\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2878 - val_loss: 0.2458\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.2577 - val_loss: 0.2562\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2419 - val_loss: 0.2018\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.2302 - val_loss: 0.2522\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.2209 - val_loss: 0.1992\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.2111 - val_loss: 0.2332\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2013 - val_loss: 0.1707\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1997 - val_loss: 0.1828\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1881 - val_loss: 0.1542\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1855 - val_loss: 0.2465\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1793 - val_loss: 0.1640\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1666 - val_loss: 0.1572\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1682 - val_loss: 0.1821\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1684 - val_loss: 0.1933\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1630 - val_loss: 0.1618\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1644 - val_loss: 0.1266\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1524 - val_loss: 0.1596\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1602 - val_loss: 0.1459\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1522 - val_loss: 0.1496\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 557us/step - loss: 0.1490 - val_loss: 0.1558\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 552us/step - loss: 0.1512 - val_loss: 0.1279\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1437 - val_loss: 0.1257\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1452 - val_loss: 0.1524\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1437 - val_loss: 0.1473\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1392 - val_loss: 0.1200\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1347 - val_loss: 0.1443\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.1370 - val_loss: 0.1223\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1366 - val_loss: 0.1297\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1342 - val_loss: 0.1548\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1338 - val_loss: 0.1386\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1327 - val_loss: 0.1095\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1311 - val_loss: 0.1271\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1290 - val_loss: 0.1138\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1297 - val_loss: 0.1172\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1303 - val_loss: 0.1310\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1274 - val_loss: 0.1308\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1234 - val_loss: 0.1159\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1228 - val_loss: 0.1129\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1268 - val_loss: 0.1086\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1209 - val_loss: 0.1484\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1247 - val_loss: 0.1180\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1222 - val_loss: 0.1273\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1219 - val_loss: 0.1231\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1208 - val_loss: 0.1162\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1204 - val_loss: 0.1094\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1199 - val_loss: 0.1049\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1197 - val_loss: 0.1413\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1181 - val_loss: 0.1120\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1184 - val_loss: 0.1131\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1200 - val_loss: 0.1070\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1161 - val_loss: 0.1116\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1155 - val_loss: 0.1147\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1161 - val_loss: 0.1108\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1169 - val_loss: 0.1144\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1153 - val_loss: 0.1034\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1139 - val_loss: 0.1037\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1146 - val_loss: 0.1033\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1134 - val_loss: 0.1089\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1136 - val_loss: 0.1306\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1130 - val_loss: 0.1146\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1138 - val_loss: 0.1039\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1094 - val_loss: 0.1177\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1126 - val_loss: 0.1042\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1153 - val_loss: 0.0988\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1127 - val_loss: 0.1054\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1124 - val_loss: 0.1351\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1134 - val_loss: 0.0991\n",
      "Epoch 71/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1132 - val_loss: 0.1048\n",
      "Epoch 72/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.1126 - val_loss: 0.0976\n",
      "Epoch 73/100\n",
      "760/760 [==============================] - 0s 613us/step - loss: 0.1117 - val_loss: 0.1151\n",
      "Epoch 74/100\n",
      "760/760 [==============================] - 0s 547us/step - loss: 0.1093 - val_loss: 0.1128\n",
      "Epoch 75/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1177 - val_loss: 0.1169\n",
      "Epoch 76/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1124 - val_loss: 0.1008\n",
      "Epoch 77/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1102 - val_loss: 0.1146\n",
      "Epoch 78/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1119 - val_loss: 0.0992\n",
      "Epoch 79/100\n",
      "760/760 [==============================] - 0s 550us/step - loss: 0.1099 - val_loss: 0.1143\n",
      "Epoch 80/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1098 - val_loss: 0.1503\n",
      "Epoch 81/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1112 - val_loss: 0.1115\n",
      "Epoch 82/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1110 - val_loss: 0.1051\n",
      "Training completed. Number of epochs: 82 , Final training loss: 0.11098886281251907 , Final validation loss: 0.10512183606624603\n",
      "Training network # 13\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 556us/step - loss: 3.6526 - val_loss: 3.7483\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 1.2836 - val_loss: 0.4725\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.3292 - val_loss: 0.2709\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2782 - val_loss: 0.2658\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2515 - val_loss: 0.2347\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2247 - val_loss: 0.1967\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2319 - val_loss: 0.1824\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1934 - val_loss: 0.1840\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.1840 - val_loss: 0.1901\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 552us/step - loss: 0.1795 - val_loss: 0.1569\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 540us/step - loss: 0.1743 - val_loss: 0.1473\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1617 - val_loss: 0.1772\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1536 - val_loss: 0.1547\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1575 - val_loss: 0.1220\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1412 - val_loss: 0.1370\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1412 - val_loss: 0.1267\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1409 - val_loss: 0.1346\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1362 - val_loss: 0.1273\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1349 - val_loss: 0.1265\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1343 - val_loss: 0.1761\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1289 - val_loss: 0.1308\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1308 - val_loss: 0.1341\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1276 - val_loss: 0.1077\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1303 - val_loss: 0.1437\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1304 - val_loss: 0.1294\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1222 - val_loss: 0.1127\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1304 - val_loss: 0.1088\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1221 - val_loss: 0.1095\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1243 - val_loss: 0.1940\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1249 - val_loss: 0.1388\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1251 - val_loss: 0.1102\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1234 - val_loss: 0.1063\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1212 - val_loss: 0.1108\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1221 - val_loss: 0.1101\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1179 - val_loss: 0.1164\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1201 - val_loss: 0.1154\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1168 - val_loss: 0.1101\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1169 - val_loss: 0.1012\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1179 - val_loss: 0.1017\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1149 - val_loss: 0.1286\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1147 - val_loss: 0.1126\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1237 - val_loss: 0.1063\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1199 - val_loss: 0.1264\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1141 - val_loss: 0.1283\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1165 - val_loss: 0.1140\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1149 - val_loss: 0.1044\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1173 - val_loss: 0.1054\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1129 - val_loss: 0.1088\n",
      "Training completed. Number of epochs: 48 , Final training loss: 0.11285506933927536 , Final validation loss: 0.10877135396003723\n",
      "Training network # 14\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 555us/step - loss: 1.2912 - val_loss: 0.4810\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.4410 - val_loss: 0.4111\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.3410 - val_loss: 0.2965\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.2709 - val_loss: 0.2349\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2412 - val_loss: 0.2237\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.2287 - val_loss: 0.1990\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2101 - val_loss: 0.2243\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1911 - val_loss: 0.1630\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1845 - val_loss: 0.2920\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1778 - val_loss: 0.1515\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1805 - val_loss: 0.1812\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1697 - val_loss: 0.1530\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1660 - val_loss: 0.1397\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1646 - val_loss: 0.1502\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1612 - val_loss: 0.2046\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1598 - val_loss: 0.1493\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1523 - val_loss: 0.1324\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1538 - val_loss: 0.1443\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1527 - val_loss: 0.1501\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1506 - val_loss: 0.1637\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1465 - val_loss: 0.1405\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1452 - val_loss: 0.1297\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1490 - val_loss: 0.1345\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1436 - val_loss: 0.1283\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1417 - val_loss: 0.1249\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1389 - val_loss: 0.1474\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1382 - val_loss: 0.1471\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1388 - val_loss: 0.1208\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1390 - val_loss: 0.1278\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1373 - val_loss: 0.1168\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1308 - val_loss: 0.1369\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1347 - val_loss: 0.1284\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1349 - val_loss: 0.1192\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1352 - val_loss: 0.1141\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1281 - val_loss: 0.1417\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1320 - val_loss: 0.1486\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1297 - val_loss: 0.1324\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1300 - val_loss: 0.1153\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1269 - val_loss: 0.1475\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1307 - val_loss: 0.1412\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1298 - val_loss: 0.1231\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1247 - val_loss: 0.1345\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1241 - val_loss: 0.1062\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1234 - val_loss: 0.1381\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1232 - val_loss: 0.1148\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1186 - val_loss: 0.1069\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1243 - val_loss: 0.1255\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1236 - val_loss: 0.1146\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1182 - val_loss: 0.1135\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1224 - val_loss: 0.1069\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1198 - val_loss: 0.1245\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1184 - val_loss: 0.1046\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1199 - val_loss: 0.1099\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1186 - val_loss: 0.1025\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1177 - val_loss: 0.1060\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1166 - val_loss: 0.1194\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1149 - val_loss: 0.1131\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1165 - val_loss: 0.1275\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1126 - val_loss: 0.0993\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1144 - val_loss: 0.1097\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1145 - val_loss: 0.1063\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1143 - val_loss: 0.1002\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1148 - val_loss: 0.1025\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1110 - val_loss: 0.1019\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1136 - val_loss: 0.1055\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1123 - val_loss: 0.1207\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1118 - val_loss: 0.1069\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1108 - val_loss: 0.1236\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1108 - val_loss: 0.1107\n",
      "Training completed. Number of epochs: 69 , Final training loss: 0.11077924817800522 , Final validation loss: 0.11066427081823349\n",
      "Training network # 15\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 549us/step - loss: 1.7433 - val_loss: 0.4731\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.4531 - val_loss: 0.3223\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.3071 - val_loss: 0.2591\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.2492 - val_loss: 0.3567\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2316 - val_loss: 0.3013\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.2142 - val_loss: 0.1841\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2057 - val_loss: 0.2299\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2017 - val_loss: 0.1749\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2002 - val_loss: 0.1547\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1805 - val_loss: 0.1654\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1860 - val_loss: 0.1871\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1788 - val_loss: 0.1618\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1668 - val_loss: 0.1928\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1663 - val_loss: 0.2090\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1698 - val_loss: 0.1493\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1612 - val_loss: 0.1946\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1587 - val_loss: 0.1447\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1609 - val_loss: 0.1802\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1500 - val_loss: 0.1377\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1479 - val_loss: 0.1399\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1450 - val_loss: 0.1591\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1370 - val_loss: 0.1481\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1430 - val_loss: 0.1489\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1365 - val_loss: 0.1337\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1386 - val_loss: 0.1634\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1338 - val_loss: 0.1352\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1350 - val_loss: 0.1558\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1299 - val_loss: 0.1160\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1284 - val_loss: 0.1149\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1277 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1293 - val_loss: 0.1097\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1283 - val_loss: 0.1420\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1247 - val_loss: 0.1180\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1245 - val_loss: 0.1098\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1227 - val_loss: 0.1441\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1233 - val_loss: 0.1408\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1223 - val_loss: 0.1050\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1224 - val_loss: 0.1113\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1199 - val_loss: 0.1069\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1187 - val_loss: 0.1303\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1193 - val_loss: 0.1147\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1165 - val_loss: 0.1303\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1173 - val_loss: 0.1188\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1194 - val_loss: 0.1170\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1154 - val_loss: 0.1174\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1148 - val_loss: 0.1105\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1146 - val_loss: 0.1072\n",
      "Training completed. Number of epochs: 47 , Final training loss: 0.11457864940166473 , Final validation loss: 0.10716776549816132\n",
      "Training network # 16\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 550us/step - loss: 3.6935 - val_loss: 3.5472\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 3.6246 - val_loss: 3.5443\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 3.6762 - val_loss: 3.5405\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 1.2461 - val_loss: 0.6084\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.3086 - val_loss: 0.2445\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2481 - val_loss: 0.2019\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2161 - val_loss: 0.1997\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2072 - val_loss: 0.2701\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1901 - val_loss: 0.1602\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1810 - val_loss: 0.1579\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1721 - val_loss: 0.1942\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1683 - val_loss: 0.1438\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1602 - val_loss: 0.1442\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1610 - val_loss: 0.1366\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1517 - val_loss: 0.1536\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1513 - val_loss: 0.1334\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1430 - val_loss: 0.1273\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1440 - val_loss: 0.1317\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1397 - val_loss: 0.1227\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1353 - val_loss: 0.1368\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1350 - val_loss: 0.1148\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1381 - val_loss: 0.1269\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1318 - val_loss: 0.1573\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1319 - val_loss: 0.1335\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1288 - val_loss: 0.1452\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1343 - val_loss: 0.1299\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1255 - val_loss: 0.1613\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1263 - val_loss: 0.1129\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1270 - val_loss: 0.1325\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1253 - val_loss: 0.1145\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1213 - val_loss: 0.1054\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1238 - val_loss: 0.1059\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1236 - val_loss: 0.1212\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1217 - val_loss: 0.1597\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1218 - val_loss: 0.1317\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1226 - val_loss: 0.1087\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1186 - val_loss: 0.1033\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1197 - val_loss: 0.1062\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1214 - val_loss: 0.1057\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1146 - val_loss: 0.1561\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1233 - val_loss: 0.1167\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1170 - val_loss: 0.1152\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 590us/step - loss: 0.1183 - val_loss: 0.1235\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 593us/step - loss: 0.1165 - val_loss: 0.1375\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 608us/step - loss: 0.1137 - val_loss: 0.1164\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1151 - val_loss: 0.0988\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 602us/step - loss: 0.1120 - val_loss: 0.1075\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1172 - val_loss: 0.1143\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1124 - val_loss: 0.1112\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1147 - val_loss: 0.1103\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 540us/step - loss: 0.1152 - val_loss: 0.1117\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1159 - val_loss: 0.1022\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 577us/step - loss: 0.1144 - val_loss: 0.1350\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1102 - val_loss: 0.0985\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1130 - val_loss: 0.0981\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1114 - val_loss: 0.1440\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 588us/step - loss: 0.1142 - val_loss: 0.1374\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1101 - val_loss: 0.1027\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1114 - val_loss: 0.1025\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1107 - val_loss: 0.1123\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1081 - val_loss: 0.1025\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1130 - val_loss: 0.1027\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1098 - val_loss: 0.0957\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1104 - val_loss: 0.1015\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1091 - val_loss: 0.0964\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1096 - val_loss: 0.0999\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1313 - val_loss: 0.1023\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1087 - val_loss: 0.0938\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1096 - val_loss: 0.1236\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1101 - val_loss: 0.1116\n",
      "Epoch 71/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1134 - val_loss: 0.1051\n",
      "Epoch 72/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1079 - val_loss: 0.1300\n",
      "Epoch 73/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1106 - val_loss: 0.1083\n",
      "Epoch 74/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1087 - val_loss: 0.1059\n",
      "Epoch 75/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1095 - val_loss: 0.0959\n",
      "Epoch 76/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1090 - val_loss: 0.1137\n",
      "Epoch 77/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1064 - val_loss: 0.0918\n",
      "Epoch 78/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1104 - val_loss: 0.0981\n",
      "Epoch 79/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1080 - val_loss: 0.0979\n",
      "Epoch 80/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1107 - val_loss: 0.0957\n",
      "Epoch 81/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1058 - val_loss: 0.0936\n",
      "Epoch 82/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1075 - val_loss: 0.1228\n",
      "Epoch 83/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1069 - val_loss: 0.1048\n",
      "Epoch 84/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1048 - val_loss: 0.1089\n",
      "Epoch 85/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1074 - val_loss: 0.0941\n",
      "Epoch 86/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1076 - val_loss: 0.0996\n",
      "Epoch 87/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1037 - val_loss: 0.0971\n",
      "Training completed. Number of epochs: 87 , Final training loss: 0.1036716103553772 , Final validation loss: 0.09707816690206528\n",
      "Training network # 17\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 552us/step - loss: 3.6422 - val_loss: 3.6688\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 3.6398 - val_loss: 3.5555\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 2.8448 - val_loss: 0.6473\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.3729 - val_loss: 0.2646\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.2600 - val_loss: 0.2215\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.2305 - val_loss: 0.1961\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2074 - val_loss: 0.1980\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1960 - val_loss: 0.2061\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1831 - val_loss: 0.2027\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1753 - val_loss: 0.1540\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1646 - val_loss: 0.1624\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1542 - val_loss: 0.1817\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1502 - val_loss: 0.1392\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1470 - val_loss: 0.1948\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1477 - val_loss: 0.1289\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1387 - val_loss: 0.1280\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1400 - val_loss: 0.1517\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1295 - val_loss: 0.1371\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1328 - val_loss: 0.1223\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1303 - val_loss: 0.1335\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1268 - val_loss: 0.1657\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1283 - val_loss: 0.1217\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1263 - val_loss: 0.1230\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1271 - val_loss: 0.1546\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1230 - val_loss: 0.1376\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1219 - val_loss: 0.1050\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1208 - val_loss: 0.1254\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1227 - val_loss: 0.1057\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1207 - val_loss: 0.1182\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1164 - val_loss: 0.1080\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1216 - val_loss: 0.1235\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1207 - val_loss: 0.1070\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1171 - val_loss: 0.1015\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1192 - val_loss: 0.1074\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1195 - val_loss: 0.1337\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1163 - val_loss: 0.1197\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1147 - val_loss: 0.1275\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1140 - val_loss: 0.1019\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1160 - val_loss: 0.1205\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1141 - val_loss: 0.1080\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1155 - val_loss: 0.1485\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1149 - val_loss: 0.1305\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1127 - val_loss: 0.1124\n",
      "Training completed. Number of epochs: 43 , Final training loss: 0.11266693472862244 , Final validation loss: 0.11239732056856155\n",
      "Training network # 18\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 560us/step - loss: 1.1704 - val_loss: 0.4829\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.4454 - val_loss: 0.3831\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.3805 - val_loss: 0.4057\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.3068 - val_loss: 0.2643\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.2758 - val_loss: 0.2457\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.2405 - val_loss: 0.2109\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.2145 - val_loss: 0.1969\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.2046 - val_loss: 0.1978\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.2010 - val_loss: 0.1737\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1865 - val_loss: 0.1599\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1861 - val_loss: 0.1776\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1700 - val_loss: 0.1620\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1736 - val_loss: 0.1789\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1671 - val_loss: 0.2133\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1658 - val_loss: 0.1664\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1619 - val_loss: 0.1479\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1572 - val_loss: 0.1969\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1586 - val_loss: 0.2036\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1575 - val_loss: 0.1574\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1559 - val_loss: 0.1380\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1514 - val_loss: 0.1452\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1491 - val_loss: 0.2613\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1503 - val_loss: 0.1364\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1463 - val_loss: 0.1421\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1457 - val_loss: 0.1345\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1469 - val_loss: 0.1380\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1467 - val_loss: 0.1306\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1424 - val_loss: 0.1295\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1467 - val_loss: 0.1279\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1403 - val_loss: 0.1279\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1395 - val_loss: 0.1361\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1418 - val_loss: 0.1345\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1385 - val_loss: 0.1274\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1434 - val_loss: 0.1256\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1384 - val_loss: 0.1320\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1363 - val_loss: 0.1275\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1431 - val_loss: 0.1384\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1340 - val_loss: 0.1574\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1379 - val_loss: 0.1306\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1385 - val_loss: 0.1214\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1355 - val_loss: 0.1230\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1343 - val_loss: 0.1222\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1328 - val_loss: 0.1290\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1337 - val_loss: 0.1230\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1336 - val_loss: 0.1523\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1334 - val_loss: 0.1450\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1331 - val_loss: 0.1356\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1322 - val_loss: 0.1501\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1328 - val_loss: 0.1201\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1323 - val_loss: 0.1209\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1301 - val_loss: 0.1383\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1290 - val_loss: 0.1244\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1291 - val_loss: 0.1289\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1316 - val_loss: 0.1194\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1313 - val_loss: 0.1193\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1316 - val_loss: 0.1244\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1274 - val_loss: 0.1346\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1292 - val_loss: 0.1354\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1276 - val_loss: 0.1199\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1299 - val_loss: 0.1271\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1279 - val_loss: 0.1850\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1268 - val_loss: 0.1211\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1285 - val_loss: 0.1229\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1268 - val_loss: 0.1156\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1324 - val_loss: 0.1124\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1264 - val_loss: 0.1839\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1300 - val_loss: 0.1262\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1275 - val_loss: 0.1156\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1275 - val_loss: 0.1236\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1475 - val_loss: 0.1307\n",
      "Epoch 71/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1255 - val_loss: 0.1197\n",
      "Epoch 72/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1262 - val_loss: 0.1108\n",
      "Epoch 73/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1274 - val_loss: 0.1182\n",
      "Epoch 74/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1260 - val_loss: 0.1228\n",
      "Epoch 75/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1302 - val_loss: 0.1288\n",
      "Epoch 76/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1252 - val_loss: 0.1141\n",
      "Epoch 77/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1294 - val_loss: 0.1271\n",
      "Epoch 78/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1262 - val_loss: 0.1198\n",
      "Epoch 79/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1248 - val_loss: 0.1548\n",
      "Epoch 80/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1242 - val_loss: 0.1272\n",
      "Epoch 81/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1254 - val_loss: 0.1186\n",
      "Epoch 82/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1247 - val_loss: 0.1238\n",
      "Training completed. Number of epochs: 82 , Final training loss: 0.12474247068166733 , Final validation loss: 0.12379053235054016\n",
      "Training network # 19\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 649us/step - loss: 3.6431 - val_loss: 3.7499\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 586us/step - loss: 3.1391 - val_loss: 0.6415\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 618us/step - loss: 0.4677 - val_loss: 0.3326\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.3184 - val_loss: 0.3858\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 550us/step - loss: 0.2674 - val_loss: 0.2308\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 553us/step - loss: 0.2317 - val_loss: 0.2075\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 543us/step - loss: 0.2141 - val_loss: 0.2634\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2009 - val_loss: 0.1978\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1868 - val_loss: 0.1657\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1728 - val_loss: 0.2651\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1626 - val_loss: 0.1554\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1531 - val_loss: 0.1372\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1513 - val_loss: 0.1257\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1458 - val_loss: 0.1518\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1441 - val_loss: 0.1946\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1354 - val_loss: 0.1342\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1358 - val_loss: 0.1411\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1337 - val_loss: 0.1192\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1310 - val_loss: 0.1265\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1319 - val_loss: 0.1107\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1277 - val_loss: 0.1257\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1275 - val_loss: 0.1152\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 557us/step - loss: 0.1271 - val_loss: 0.1412\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 595us/step - loss: 0.1280 - val_loss: 0.1606\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1288 - val_loss: 0.1196\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 565us/step - loss: 0.1248 - val_loss: 0.1241\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 586us/step - loss: 0.1238 - val_loss: 0.1332\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.1250 - val_loss: 0.1122\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 606us/step - loss: 0.1238 - val_loss: 0.1148\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 566us/step - loss: 0.1234 - val_loss: 0.1127\n",
      "Training completed. Number of epochs: 30 , Final training loss: 0.12339141964912415 , Final validation loss: 0.11272598803043365\n",
      "Training network # 20\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 595us/step - loss: 1.1624 - val_loss: 0.6179\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.4567 - val_loss: 0.4119\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.4140 - val_loss: 0.4275\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.3497 - val_loss: 0.3135\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.2763 - val_loss: 0.2943\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 543us/step - loss: 0.2537 - val_loss: 0.2547\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2358 - val_loss: 0.2406\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.2172 - val_loss: 0.1823\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2021 - val_loss: 0.2350\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1927 - val_loss: 0.2244\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1864 - val_loss: 0.3212\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1843 - val_loss: 0.1660\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1796 - val_loss: 0.1745\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1749 - val_loss: 0.1542\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1677 - val_loss: 0.2093\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1700 - val_loss: 0.1535\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1648 - val_loss: 0.1419\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1605 - val_loss: 0.1442\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1544 - val_loss: 0.1483\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1547 - val_loss: 0.1578\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1523 - val_loss: 0.1432\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1468 - val_loss: 0.1336\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1517 - val_loss: 0.1400\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1455 - val_loss: 0.1619\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1455 - val_loss: 0.1685\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1444 - val_loss: 0.1506\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1462 - val_loss: 0.1336\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1410 - val_loss: 0.1350\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1395 - val_loss: 0.1566\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1401 - val_loss: 0.1336\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1418 - val_loss: 0.1237\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1393 - val_loss: 0.1531\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1372 - val_loss: 0.1759\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1352 - val_loss: 0.1225\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1364 - val_loss: 0.1553\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1362 - val_loss: 0.1363\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1364 - val_loss: 0.1406\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1355 - val_loss: 0.1385\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1352 - val_loss: 0.1255\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1382 - val_loss: 0.1205\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1331 - val_loss: 0.1178\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1344 - val_loss: 0.1324\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1354 - val_loss: 0.1269\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1327 - val_loss: 0.1906\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1354 - val_loss: 0.1204\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1317 - val_loss: 0.1483\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1327 - val_loss: 0.1268\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1312 - val_loss: 0.1248\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1310 - val_loss: 0.1456\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1314 - val_loss: 0.1200\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1314 - val_loss: 0.1397\n",
      "Training completed. Number of epochs: 51 , Final training loss: 0.13136537373065948 , Final validation loss: 0.13969050347805023\n",
      "Training network # 21\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 554us/step - loss: 2.1660 - val_loss: 0.7564\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.4743 - val_loss: 0.4837\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.4132 - val_loss: 0.3766\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.3071 - val_loss: 0.2587\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.2612 - val_loss: 0.2749\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.2442 - val_loss: 0.2686\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2138 - val_loss: 0.1765\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.2111 - val_loss: 0.1860\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1920 - val_loss: 0.1539\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1781 - val_loss: 0.1526\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1766 - val_loss: 0.1572\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1685 - val_loss: 0.1543\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1641 - val_loss: 0.1453\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1547 - val_loss: 0.1299\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1542 - val_loss: 0.1537\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1551 - val_loss: 0.2104\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1505 - val_loss: 0.1889\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 489us/step - loss: 0.1506 - val_loss: 0.1497\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1394 - val_loss: 0.1277\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1470 - val_loss: 0.1824\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1424 - val_loss: 0.1360\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1427 - val_loss: 0.2211\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1408 - val_loss: 0.1329\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1399 - val_loss: 0.1605\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1358 - val_loss: 0.1207\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1361 - val_loss: 0.1084\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1275 - val_loss: 0.1143\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1281 - val_loss: 0.1064\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1263 - val_loss: 0.1548\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1277 - val_loss: 0.1208\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1285 - val_loss: 0.1094\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1240 - val_loss: 0.1130\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1246 - val_loss: 0.1143\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1230 - val_loss: 0.1185\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1200 - val_loss: 0.1146\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1227 - val_loss: 0.1126\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 492us/step - loss: 0.1230 - val_loss: 0.1062\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1195 - val_loss: 0.1372\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1183 - val_loss: 0.1020\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1207 - val_loss: 0.1048\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1204 - val_loss: 0.1480\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1142 - val_loss: 0.1180\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1180 - val_loss: 0.1058\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1192 - val_loss: 0.1172\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1166 - val_loss: 0.1052\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1192 - val_loss: 0.1167\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1131 - val_loss: 0.1103\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1157 - val_loss: 0.1053\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1141 - val_loss: 0.1111\n",
      "Training completed. Number of epochs: 49 , Final training loss: 0.11413011699914932 , Final validation loss: 0.11112429946660995\n",
      "Training network # 22\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 564us/step - loss: 1.4831 - val_loss: 0.4345\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 566us/step - loss: 0.4547 - val_loss: 0.3976\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.3712 - val_loss: 0.2686\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 570us/step - loss: 0.2816 - val_loss: 0.3541\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.2480 - val_loss: 0.2858\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2356 - val_loss: 0.2019\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.2196 - val_loss: 0.2093\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2108 - val_loss: 0.1763\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1941 - val_loss: 0.1841\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1790 - val_loss: 0.1499\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1758 - val_loss: 0.1658\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1729 - val_loss: 0.1689\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1630 - val_loss: 0.1523\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1560 - val_loss: 0.1424\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1568 - val_loss: 0.1568\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1560 - val_loss: 0.2210\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1520 - val_loss: 0.1520\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1492 - val_loss: 0.1288\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1484 - val_loss: 0.2124\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1515 - val_loss: 0.1360\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1443 - val_loss: 0.1484\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1421 - val_loss: 0.1243\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1416 - val_loss: 0.1314\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1397 - val_loss: 0.1384\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1419 - val_loss: 0.1632\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1392 - val_loss: 0.1407\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1333 - val_loss: 0.1308\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1350 - val_loss: 0.1218\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1343 - val_loss: 0.1658\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1317 - val_loss: 0.1228\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 547us/step - loss: 0.1332 - val_loss: 0.1299\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 558us/step - loss: 0.1283 - val_loss: 0.1212\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 577us/step - loss: 0.1274 - val_loss: 0.1127\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1248 - val_loss: 0.1613\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 561us/step - loss: 0.1252 - val_loss: 0.1182\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 552us/step - loss: 0.1245 - val_loss: 0.1222\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.1285 - val_loss: 0.1266\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1269 - val_loss: 0.1497\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1227 - val_loss: 0.1186\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1224 - val_loss: 0.1101\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1227 - val_loss: 0.1283\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1243 - val_loss: 0.1403\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1194 - val_loss: 0.1076\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1198 - val_loss: 0.1093\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1210 - val_loss: 0.1100\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1236 - val_loss: 0.1224\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1197 - val_loss: 0.1130\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1172 - val_loss: 0.1026\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1199 - val_loss: 0.1115\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1171 - val_loss: 0.1070\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1213 - val_loss: 0.1133\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1161 - val_loss: 0.1295\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1168 - val_loss: 0.1081\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1171 - val_loss: 0.1022\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1172 - val_loss: 0.1230\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1128 - val_loss: 0.1108\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1140 - val_loss: 0.1044\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1138 - val_loss: 0.1225\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1135 - val_loss: 0.1017\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1125 - val_loss: 0.0961\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1150 - val_loss: 0.1255\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1135 - val_loss: 0.1052\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1122 - val_loss: 0.0998\n",
      "Epoch 64/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1121 - val_loss: 0.1095\n",
      "Epoch 65/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1132 - val_loss: 0.1031\n",
      "Epoch 66/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1116 - val_loss: 0.1087\n",
      "Epoch 67/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1132 - val_loss: 0.1021\n",
      "Epoch 68/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1141 - val_loss: 0.1084\n",
      "Epoch 69/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1110 - val_loss: 0.1021\n",
      "Epoch 70/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1128 - val_loss: 0.1001\n",
      "Training completed. Number of epochs: 70 , Final training loss: 0.11275973170995712 , Final validation loss: 0.10014639049768448\n",
      "Training network # 23\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 552us/step - loss: 1.1557 - val_loss: 0.4698\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.4813 - val_loss: 0.4360\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.4388 - val_loss: 0.4078\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.3610 - val_loss: 0.3049\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.3079 - val_loss: 0.2531\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.2647 - val_loss: 0.2531\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.2496 - val_loss: 0.2995\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2272 - val_loss: 0.2235\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.2123 - val_loss: 0.1819\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.2058 - val_loss: 0.1736\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1989 - val_loss: 0.1747\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1854 - val_loss: 0.1870\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1886 - val_loss: 0.1699\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1792 - val_loss: 0.1796\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1757 - val_loss: 0.1609\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1708 - val_loss: 0.1459\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1599 - val_loss: 0.1477\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1597 - val_loss: 0.1481\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1595 - val_loss: 0.2057\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1601 - val_loss: 0.1359\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1532 - val_loss: 0.1397\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1545 - val_loss: 0.1382\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1485 - val_loss: 0.1620\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1493 - val_loss: 0.1674\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1484 - val_loss: 0.1346\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.1462 - val_loss: 0.1279\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1500 - val_loss: 0.1351\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1440 - val_loss: 0.1436\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1417 - val_loss: 0.1485\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 545us/step - loss: 0.1436 - val_loss: 0.1343\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1461 - val_loss: 0.1415\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1411 - val_loss: 0.1449\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1412 - val_loss: 0.1288\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1440 - val_loss: 0.1460\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1413 - val_loss: 0.1312\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1392 - val_loss: 0.1221\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1386 - val_loss: 0.1295\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1383 - val_loss: 0.1389\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1371 - val_loss: 0.1476\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 544us/step - loss: 0.1358 - val_loss: 0.1326\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1370 - val_loss: 0.1222\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.1362 - val_loss: 0.1343\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 547us/step - loss: 0.1350 - val_loss: 0.1377\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1376 - val_loss: 0.1253\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1341 - val_loss: 0.1258\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1332 - val_loss: 0.1343\n",
      "Training completed. Number of epochs: 46 , Final training loss: 0.13323554396629333 , Final validation loss: 0.13428033888339996\n",
      "Training network # 24\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 568us/step - loss: 1.9106 - val_loss: 0.4345\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.4623 - val_loss: 0.4380\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.3487 - val_loss: 0.2826\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.2792 - val_loss: 0.4560\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 548us/step - loss: 0.2581 - val_loss: 0.2239\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 549us/step - loss: 0.2208 - val_loss: 0.1975\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 573us/step - loss: 0.2213 - val_loss: 0.2120\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 543us/step - loss: 0.2029 - val_loss: 0.1716\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1903 - val_loss: 0.1723\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1918 - val_loss: 0.1778\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1809 - val_loss: 0.1939\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1735 - val_loss: 0.1559\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1740 - val_loss: 0.1562\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1640 - val_loss: 0.1431\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1625 - val_loss: 0.1364\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1547 - val_loss: 0.1374\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1593 - val_loss: 0.1424\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1542 - val_loss: 0.2562\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1545 - val_loss: 0.1383\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1507 - val_loss: 0.1298\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 555us/step - loss: 0.1481 - val_loss: 0.1343\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1484 - val_loss: 0.1422\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1485 - val_loss: 0.2466\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1475 - val_loss: 0.1305\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1421 - val_loss: 0.1615\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 493us/step - loss: 0.1395 - val_loss: 0.1602\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.1449 - val_loss: 0.1458\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1375 - val_loss: 0.1805\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1423 - val_loss: 0.1647\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1368 - val_loss: 0.1305\n",
      "Training completed. Number of epochs: 30 , Final training loss: 0.13680629432201385 , Final validation loss: 0.1305338740348816\n",
      "Training network # 25\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 697us/step - loss: 1.6867 - val_loss: 0.4423\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.4604 - val_loss: 0.4796\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.3252 - val_loss: 0.2477\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2689 - val_loss: 0.3275\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.2410 - val_loss: 0.2040\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.2255 - val_loss: 0.1915\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 541us/step - loss: 0.2170 - val_loss: 0.1867\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2015 - val_loss: 0.1818\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1942 - val_loss: 0.1881\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1889 - val_loss: 0.1654\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1774 - val_loss: 0.1501\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 544us/step - loss: 0.1725 - val_loss: 0.1898\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 571us/step - loss: 0.1709 - val_loss: 0.1657\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 582us/step - loss: 0.1630 - val_loss: 0.1541\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 559us/step - loss: 0.1649 - val_loss: 0.1588\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 587us/step - loss: 0.1553 - val_loss: 0.1471\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1608 - val_loss: 0.1792\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1533 - val_loss: 0.1331\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1509 - val_loss: 0.1381\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1504 - val_loss: 0.1287\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1487 - val_loss: 0.1350\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1464 - val_loss: 0.1321\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1449 - val_loss: 0.1347\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1435 - val_loss: 0.1385\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1448 - val_loss: 0.1274\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1382 - val_loss: 0.1395\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1406 - val_loss: 0.1548\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1365 - val_loss: 0.1228\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1379 - val_loss: 0.1345\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1410 - val_loss: 0.1186\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1371 - val_loss: 0.1274\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1350 - val_loss: 0.1838\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1326 - val_loss: 0.1204\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1322 - val_loss: 0.1179\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1319 - val_loss: 0.1191\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1311 - val_loss: 0.2096\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1326 - val_loss: 0.1203\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1290 - val_loss: 0.1217\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1327 - val_loss: 0.1259\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1296 - val_loss: 0.1253\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1305 - val_loss: 0.1132\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1313 - val_loss: 0.1228\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1285 - val_loss: 0.1126\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 557us/step - loss: 0.1264 - val_loss: 0.1129\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1252 - val_loss: 0.1146\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1262 - val_loss: 0.1215\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1256 - val_loss: 0.1169\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1260 - val_loss: 0.1188\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1259 - val_loss: 0.1236\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1253 - val_loss: 0.1197\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1198 - val_loss: 0.1177\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1235 - val_loss: 0.1073\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1222 - val_loss: 0.1249\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1240 - val_loss: 0.1160\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1209 - val_loss: 0.1094\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1207 - val_loss: 0.1090\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1173 - val_loss: 0.1078\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1231 - val_loss: 0.1239\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1193 - val_loss: 0.1172\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1172 - val_loss: 0.1264\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1200 - val_loss: 0.1082\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1209 - val_loss: 0.1200\n",
      "Training completed. Number of epochs: 62 , Final training loss: 0.12093324214220047 , Final validation loss: 0.11999349296092987\n",
      "Training network # 26\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 566us/step - loss: 1.2211 - val_loss: 0.4724\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.4651 - val_loss: 0.4229\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.4341 - val_loss: 0.3826\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.3619 - val_loss: 0.2980\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2832 - val_loss: 0.2494\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.2430 - val_loss: 0.2887\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2258 - val_loss: 0.1947\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.2138 - val_loss: 0.1746\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1907 - val_loss: 0.2126\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1794 - val_loss: 0.1556\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1776 - val_loss: 0.1570\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1715 - val_loss: 0.1655\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1653 - val_loss: 0.1559\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1613 - val_loss: 0.1520\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1568 - val_loss: 0.1571\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1602 - val_loss: 0.2294\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1545 - val_loss: 0.1403\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1557 - val_loss: 0.1511\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1477 - val_loss: 0.1555\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1528 - val_loss: 0.1392\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1536 - val_loss: 0.1271\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1448 - val_loss: 0.1705\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1461 - val_loss: 0.1473\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1441 - val_loss: 0.1336\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1393 - val_loss: 0.1831\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1412 - val_loss: 0.1339\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1435 - val_loss: 0.1341\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1408 - val_loss: 0.1315\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1390 - val_loss: 0.1319\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1381 - val_loss: 0.1286\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1351 - val_loss: 0.1186\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1396 - val_loss: 0.1199\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1362 - val_loss: 0.1190\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1372 - val_loss: 0.1328\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1403 - val_loss: 0.1242\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1335 - val_loss: 0.1193\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1324 - val_loss: 0.1364\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1319 - val_loss: 0.1370\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1346 - val_loss: 0.1234\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1332 - val_loss: 0.1204\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1316 - val_loss: 0.1196\n",
      "Training completed. Number of epochs: 41 , Final training loss: 0.1315835565328598 , Final validation loss: 0.1195736825466156\n",
      "Training network # 27\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 589us/step - loss: 1.1729 - val_loss: 0.8678\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 562us/step - loss: 0.4522 - val_loss: 0.3953\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 563us/step - loss: 0.3711 - val_loss: 0.3919\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.3054 - val_loss: 0.2826\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.2752 - val_loss: 0.2468\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2549 - val_loss: 0.2735\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.2387 - val_loss: 0.2231\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.2227 - val_loss: 0.2047\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.2184 - val_loss: 0.2114\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.2115 - val_loss: 0.2086\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 562us/step - loss: 0.2035 - val_loss: 0.1872\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1966 - val_loss: 0.1938\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1920 - val_loss: 0.1822\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1857 - val_loss: 0.2048\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1856 - val_loss: 0.1700\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1733 - val_loss: 0.1690\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 552us/step - loss: 0.1786 - val_loss: 0.2041\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 540us/step - loss: 0.1627 - val_loss: 0.1389\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 594us/step - loss: 0.1623 - val_loss: 0.1516\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1610 - val_loss: 0.1374\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1511 - val_loss: 0.1483\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1554 - val_loss: 0.1628\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1504 - val_loss: 0.1296\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1450 - val_loss: 0.1317\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1460 - val_loss: 0.1218\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 541us/step - loss: 0.1427 - val_loss: 0.1537\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1396 - val_loss: 0.1255\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 545us/step - loss: 0.1442 - val_loss: 0.1276\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1376 - val_loss: 0.1270\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1387 - val_loss: 0.1251\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1339 - val_loss: 0.1321\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1377 - val_loss: 0.1600\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 583us/step - loss: 0.1344 - val_loss: 0.1549\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1314 - val_loss: 0.1437\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1319 - val_loss: 0.1115\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1355 - val_loss: 0.1159\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1288 - val_loss: 0.1312\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1316 - val_loss: 0.1236\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1305 - val_loss: 0.1205\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1286 - val_loss: 0.1204\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1312 - val_loss: 0.1266\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1254 - val_loss: 0.1208\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1266 - val_loss: 0.1320\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1295 - val_loss: 0.1131\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1283 - val_loss: 0.1126\n",
      "Training completed. Number of epochs: 45 , Final training loss: 0.12825311720371246 , Final validation loss: 0.11261463910341263\n",
      "Training network # 28\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 554us/step - loss: 1.1556 - val_loss: 0.5086\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.4782 - val_loss: 0.5437\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.4292 - val_loss: 0.5078\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.3648 - val_loss: 0.3826\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.3015 - val_loss: 0.2406\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.2551 - val_loss: 0.2203\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.2223 - val_loss: 0.1916\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2044 - val_loss: 0.1809\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.2029 - val_loss: 0.2106\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1886 - val_loss: 0.1862\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1805 - val_loss: 0.2422\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1731 - val_loss: 0.2141\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1767 - val_loss: 0.1561\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1644 - val_loss: 0.1455\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1619 - val_loss: 0.1474\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1628 - val_loss: 0.1453\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1544 - val_loss: 0.1552\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1531 - val_loss: 0.1542\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1518 - val_loss: 0.1412\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1534 - val_loss: 0.1431\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1453 - val_loss: 0.1365\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1473 - val_loss: 0.1440\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1454 - val_loss: 0.1439\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1461 - val_loss: 0.1424\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1402 - val_loss: 0.1380\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1425 - val_loss: 0.1679\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1423 - val_loss: 0.1415\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1392 - val_loss: 0.1196\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1392 - val_loss: 0.1534\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1379 - val_loss: 0.1319\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1383 - val_loss: 0.1208\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1352 - val_loss: 0.1713\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 547us/step - loss: 0.1367 - val_loss: 0.1281\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1354 - val_loss: 0.1255\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1336 - val_loss: 0.1591\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1417 - val_loss: 0.1372\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1357 - val_loss: 0.1493\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1333 - val_loss: 0.1200\n",
      "Training completed. Number of epochs: 38 , Final training loss: 0.13331463932991028 , Final validation loss: 0.11996431648731232\n",
      "Training network # 29\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 589us/step - loss: 3.7002 - val_loss: 3.6007\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 1.0643 - val_loss: 0.4420\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 608us/step - loss: 0.3488 - val_loss: 0.3331\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 566us/step - loss: 0.2852 - val_loss: 0.2574\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 540us/step - loss: 0.2482 - val_loss: 0.3306\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2335 - val_loss: 0.2017\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.2230 - val_loss: 0.1896\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.2008 - val_loss: 0.2155\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1978 - val_loss: 0.1781\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1860 - val_loss: 0.1872\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1796 - val_loss: 0.1548\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1790 - val_loss: 0.1748\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1762 - val_loss: 0.1390\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1642 - val_loss: 0.1944\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1653 - val_loss: 0.1605\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1623 - val_loss: 0.2083\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 546us/step - loss: 0.1542 - val_loss: 0.1955\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1527 - val_loss: 0.1374\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1570 - val_loss: 0.1580\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1526 - val_loss: 0.1304\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1484 - val_loss: 0.1307\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1451 - val_loss: 0.1888\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1461 - val_loss: 0.1564\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1435 - val_loss: 0.2064\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1433 - val_loss: 0.1307\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1388 - val_loss: 0.2094\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1406 - val_loss: 0.1255\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1364 - val_loss: 0.1317\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1347 - val_loss: 0.1227\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1395 - val_loss: 0.1326\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1352 - val_loss: 0.2724\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1321 - val_loss: 0.1279\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1364 - val_loss: 0.1150\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 550us/step - loss: 0.1275 - val_loss: 0.1186\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 570us/step - loss: 0.1315 - val_loss: 0.2003\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1281 - val_loss: 0.1296\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1285 - val_loss: 0.1319\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1295 - val_loss: 0.1260\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1262 - val_loss: 0.1238\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1280 - val_loss: 0.1422\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 543us/step - loss: 0.1261 - val_loss: 0.1117\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1221 - val_loss: 0.1091\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1262 - val_loss: 0.1130\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1232 - val_loss: 0.1070\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1220 - val_loss: 0.1118\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1242 - val_loss: 0.1138\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1226 - val_loss: 0.1170\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1223 - val_loss: 0.1126\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1243 - val_loss: 0.1281\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1199 - val_loss: 0.1285\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1226 - val_loss: 0.1050\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1230 - val_loss: 0.1128\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1205 - val_loss: 0.1117\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1207 - val_loss: 0.1194\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1190 - val_loss: 0.1130\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 565us/step - loss: 0.1208 - val_loss: 0.1113\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1185 - val_loss: 0.1119\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 570us/step - loss: 0.1181 - val_loss: 0.1245\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1213 - val_loss: 0.1263\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 538us/step - loss: 0.1155 - val_loss: 0.1987\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1200 - val_loss: 0.1154\n",
      "Training completed. Number of epochs: 61 , Final training loss: 0.12000951915979385 , Final validation loss: 0.11541447043418884\n",
      "Training network # 30\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 549us/step - loss: 1.2692 - val_loss: 0.4492\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.4577 - val_loss: 0.4431\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.3630 - val_loss: 0.3351\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 553us/step - loss: 0.2942 - val_loss: 0.2418\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.2590 - val_loss: 0.2143\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.2344 - val_loss: 0.2534\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2229 - val_loss: 0.2510\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.2084 - val_loss: 0.1909\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1998 - val_loss: 0.1840\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1875 - val_loss: 0.1606\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1821 - val_loss: 0.1584\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1769 - val_loss: 0.1432\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1730 - val_loss: 0.2049\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1636 - val_loss: 0.1439\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1597 - val_loss: 0.1357\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 490us/step - loss: 0.1569 - val_loss: 0.1599\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1527 - val_loss: 0.1363\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1501 - val_loss: 0.1278\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1535 - val_loss: 0.2042\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1494 - val_loss: 0.1455\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1449 - val_loss: 0.1345\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1427 - val_loss: 0.1268\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1397 - val_loss: 0.1620\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1422 - val_loss: 0.1263\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1432 - val_loss: 0.1282\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1365 - val_loss: 0.1565\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1360 - val_loss: 0.1337\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1346 - val_loss: 0.1244\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1344 - val_loss: 0.1118\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1416 - val_loss: 0.1467\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1314 - val_loss: 0.1393\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1314 - val_loss: 0.1738\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1285 - val_loss: 0.1161\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1282 - val_loss: 0.2029\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1267 - val_loss: 0.1133\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1302 - val_loss: 0.1134\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1264 - val_loss: 0.1224\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1246 - val_loss: 0.1238\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1290 - val_loss: 0.1276\n",
      "Training completed. Number of epochs: 39 , Final training loss: 0.12900026142597198 , Final validation loss: 0.12756972014904022\n",
      "Training network # 31\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 564us/step - loss: 3.6380 - val_loss: 3.7740\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 3.4403 - val_loss: 0.7580\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.4482 - val_loss: 0.4310\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.3127 - val_loss: 0.2728\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2647 - val_loss: 0.3373\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.2208 - val_loss: 0.2627\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.2145 - val_loss: 0.1795\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1980 - val_loss: 0.1757\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1817 - val_loss: 0.1636\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1743 - val_loss: 0.1694\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1627 - val_loss: 0.1380\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1585 - val_loss: 0.1504\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1497 - val_loss: 0.2068\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1497 - val_loss: 0.1235\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1416 - val_loss: 0.1189\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1389 - val_loss: 0.1200\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1359 - val_loss: 0.1211\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1364 - val_loss: 0.1180\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1349 - val_loss: 0.1295\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1330 - val_loss: 0.1453\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1346 - val_loss: 0.1443\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1251 - val_loss: 0.1157\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1266 - val_loss: 0.1166\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1270 - val_loss: 0.1105\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1220 - val_loss: 0.1361\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1294 - val_loss: 0.1273\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1228 - val_loss: 0.1625\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1244 - val_loss: 0.1100\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1215 - val_loss: 0.1134\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1248 - val_loss: 0.1510\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1207 - val_loss: 0.1087\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1243 - val_loss: 0.1195\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1201 - val_loss: 0.1148\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1170 - val_loss: 0.1063\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1199 - val_loss: 0.1565\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1196 - val_loss: 0.1070\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1197 - val_loss: 0.1214\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1180 - val_loss: 0.1013\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1157 - val_loss: 0.1485\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1186 - val_loss: 0.1104\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1147 - val_loss: 0.0998\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1153 - val_loss: 0.1019\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1151 - val_loss: 0.1028\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1147 - val_loss: 0.1032\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1157 - val_loss: 0.1326\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1158 - val_loss: 0.1245\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1148 - val_loss: 0.1302\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1149 - val_loss: 0.0971\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1131 - val_loss: 0.1063\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1132 - val_loss: 0.1000\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1121 - val_loss: 0.1192\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1108 - val_loss: 0.0963\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1115 - val_loss: 0.1005\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1145 - val_loss: 0.1332\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1128 - val_loss: 0.1079\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1094 - val_loss: 0.1054\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1103 - val_loss: 0.1049\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1107 - val_loss: 0.1143\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1078 - val_loss: 0.1161\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1100 - val_loss: 0.1102\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1151 - val_loss: 0.1069\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1098 - val_loss: 0.0968\n",
      "Training completed. Number of epochs: 62 , Final training loss: 0.10981748253107071 , Final validation loss: 0.09683216363191605\n",
      "Training network # 32\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 576us/step - loss: 2.0288 - val_loss: 0.4272\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.4248 - val_loss: 0.4003\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.3257 - val_loss: 0.2486\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.2638 - val_loss: 0.2879\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.2467 - val_loss: 0.2493\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.2313 - val_loss: 0.1896\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.2118 - val_loss: 0.2215\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.2073 - val_loss: 0.1815\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.2050 - val_loss: 0.1717\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.2008 - val_loss: 0.1657\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1813 - val_loss: 0.1591\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1805 - val_loss: 0.1650\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1759 - val_loss: 0.1705\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1759 - val_loss: 0.1396\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1621 - val_loss: 0.1764\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1686 - val_loss: 0.1434\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1639 - val_loss: 0.1728\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1604 - val_loss: 0.1516\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1599 - val_loss: 0.1601\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1534 - val_loss: 0.1345\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1488 - val_loss: 0.1287\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1455 - val_loss: 0.1697\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1432 - val_loss: 0.1293\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1413 - val_loss: 0.1291\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1382 - val_loss: 0.1258\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1423 - val_loss: 0.1813\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1365 - val_loss: 0.1279\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1366 - val_loss: 0.1252\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1322 - val_loss: 0.1223\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1318 - val_loss: 0.1278\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1324 - val_loss: 0.1213\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1324 - val_loss: 0.1259\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1309 - val_loss: 0.1134\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1285 - val_loss: 0.1297\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1286 - val_loss: 0.1090\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1277 - val_loss: 0.1171\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1298 - val_loss: 0.1249\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1238 - val_loss: 0.1109\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1281 - val_loss: 0.1165\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1247 - val_loss: 0.1139\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1231 - val_loss: 0.1296\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1195 - val_loss: 0.1200\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1234 - val_loss: 0.1165\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1244 - val_loss: 0.1061\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 550us/step - loss: 0.1210 - val_loss: 0.1150\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 567us/step - loss: 0.1217 - val_loss: 0.1223\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 565us/step - loss: 0.1209 - val_loss: 0.1223\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 540us/step - loss: 0.1205 - val_loss: 0.1043\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 563us/step - loss: 0.1235 - val_loss: 0.1130\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1206 - val_loss: 0.1079\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 567us/step - loss: 0.1198 - val_loss: 0.1140\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1223 - val_loss: 0.1121\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1173 - val_loss: 0.1131\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1198 - val_loss: 0.1129\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1189 - val_loss: 0.1174\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1156 - val_loss: 0.1177\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1184 - val_loss: 0.1054\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1152 - val_loss: 0.1082\n",
      "Training completed. Number of epochs: 58 , Final training loss: 0.11518412828445435 , Final validation loss: 0.10816218703985214\n",
      "Training network # 33\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 554us/step - loss: 1.1561 - val_loss: 0.4662\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.4638 - val_loss: 0.4136\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.3945 - val_loss: 0.3586\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.3279 - val_loss: 0.3069\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.2773 - val_loss: 0.2371\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.2427 - val_loss: 0.2761\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.2282 - val_loss: 0.3004\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2134 - val_loss: 0.2514\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.2034 - val_loss: 0.2157\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.2037 - val_loss: 0.2191\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1944 - val_loss: 0.1686\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1916 - val_loss: 0.1614\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1845 - val_loss: 0.2595\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1832 - val_loss: 0.1550\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1747 - val_loss: 0.1498\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1711 - val_loss: 0.2451\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1717 - val_loss: 0.1516\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1699 - val_loss: 0.1478\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1604 - val_loss: 0.1464\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1581 - val_loss: 0.1518\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1635 - val_loss: 0.1532\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1605 - val_loss: 0.2817\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1537 - val_loss: 0.1830\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1541 - val_loss: 0.1541\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1537 - val_loss: 0.1442\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1507 - val_loss: 0.1346\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1467 - val_loss: 0.1444\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1489 - val_loss: 0.1579\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1481 - val_loss: 0.1432\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1471 - val_loss: 0.1520\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1470 - val_loss: 0.1497\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1453 - val_loss: 0.1435\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1406 - val_loss: 0.1718\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1459 - val_loss: 0.1319\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1399 - val_loss: 0.1415\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1430 - val_loss: 0.1277\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1412 - val_loss: 0.1420\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1406 - val_loss: 0.1249\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1403 - val_loss: 0.1344\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1411 - val_loss: 0.1620\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1405 - val_loss: 0.1268\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1355 - val_loss: 0.1504\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1374 - val_loss: 0.1324\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1356 - val_loss: 0.1204\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1427 - val_loss: 0.1249\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1366 - val_loss: 0.1569\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1383 - val_loss: 0.1458\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1356 - val_loss: 0.1296\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1336 - val_loss: 0.1218\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1355 - val_loss: 0.1295\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1328 - val_loss: 0.1222\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1347 - val_loss: 0.1481\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1341 - val_loss: 0.1216\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1337 - val_loss: 0.1405\n",
      "Training completed. Number of epochs: 54 , Final training loss: 0.1337047517299652 , Final validation loss: 0.14051425457000732\n",
      "Training network # 34\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 554us/step - loss: 1.5115 - val_loss: 0.6653\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.4386 - val_loss: 0.3803\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.3667 - val_loss: 0.2876\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2908 - val_loss: 0.2723\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.2410 - val_loss: 0.3335\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.2244 - val_loss: 0.1825\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.2151 - val_loss: 0.1761\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.2060 - val_loss: 0.2165\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1840 - val_loss: 0.1932\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1785 - val_loss: 0.1637\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1838 - val_loss: 0.1856\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1684 - val_loss: 0.1510\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1621 - val_loss: 0.1506\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1571 - val_loss: 0.1753\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1599 - val_loss: 0.1475\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1543 - val_loss: 0.1427\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1491 - val_loss: 0.1276\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1526 - val_loss: 0.1338\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1448 - val_loss: 0.1510\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1416 - val_loss: 0.1373\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1518 - val_loss: 0.1299\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1423 - val_loss: 0.1235\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1375 - val_loss: 0.1340\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1370 - val_loss: 0.1661\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1378 - val_loss: 0.1603\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1403 - val_loss: 0.1717\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1355 - val_loss: 0.1252\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1340 - val_loss: 0.1177\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1356 - val_loss: 0.1195\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1325 - val_loss: 0.1481\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1295 - val_loss: 0.1163\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1278 - val_loss: 0.1270\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1308 - val_loss: 0.1274\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1294 - val_loss: 0.1405\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1256 - val_loss: 0.1682\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1255 - val_loss: 0.1068\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1244 - val_loss: 0.1149\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1224 - val_loss: 0.1192\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1249 - val_loss: 0.1199\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1225 - val_loss: 0.1309\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1267 - val_loss: 0.1222\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1277 - val_loss: 0.1110\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1206 - val_loss: 0.1308\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1227 - val_loss: 0.1070\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1238 - val_loss: 0.1069\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1208 - val_loss: 0.1063\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1230 - val_loss: 0.1051\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1195 - val_loss: 0.1064\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1182 - val_loss: 0.1069\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1213 - val_loss: 0.1086\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1202 - val_loss: 0.1142\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1167 - val_loss: 0.1092\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1193 - val_loss: 0.1490\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1197 - val_loss: 0.1128\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1166 - val_loss: 0.1359\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1179 - val_loss: 0.1262\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1179 - val_loss: 0.1096\n",
      "Training completed. Number of epochs: 57 , Final training loss: 0.11785707622766495 , Final validation loss: 0.10957138985395432\n",
      "Training network # 35\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 576us/step - loss: 3.6886 - val_loss: 4.2193\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 3.6481 - val_loss: 3.5788\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 1.3605 - val_loss: 0.3786\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.3400 - val_loss: 0.2701\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.2703 - val_loss: 0.3761\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2516 - val_loss: 0.2170\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.2231 - val_loss: 0.1987\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.2101 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.2036 - val_loss: 0.1619\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1869 - val_loss: 0.1762\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1764 - val_loss: 0.1570\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1694 - val_loss: 0.1454\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1681 - val_loss: 0.1773\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1529 - val_loss: 0.1415\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1492 - val_loss: 0.2456\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1497 - val_loss: 0.1543\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1419 - val_loss: 0.1301\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1383 - val_loss: 0.1276\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1364 - val_loss: 0.1742\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1322 - val_loss: 0.1538\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1338 - val_loss: 0.1242\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1314 - val_loss: 0.1254\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1264 - val_loss: 0.1082\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1268 - val_loss: 0.1104\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1288 - val_loss: 0.1154\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1244 - val_loss: 0.1199\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1263 - val_loss: 0.1055\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1240 - val_loss: 0.1095\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1212 - val_loss: 0.1202\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1207 - val_loss: 0.1140\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1236 - val_loss: 0.1085\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1240 - val_loss: 0.1097\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1219 - val_loss: 0.1262\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1189 - val_loss: 0.1246\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1195 - val_loss: 0.1172\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1204 - val_loss: 0.1023\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1187 - val_loss: 0.1596\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1149 - val_loss: 0.1188\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1154 - val_loss: 0.1177\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1172 - val_loss: 0.1137\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1166 - val_loss: 0.2131\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1197 - val_loss: 0.1082\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1137 - val_loss: 0.1017\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1183 - val_loss: 0.1031\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1134 - val_loss: 0.1249\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1141 - val_loss: 0.1068\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1156 - val_loss: 0.1061\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1165 - val_loss: 0.1006\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1162 - val_loss: 0.1040\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1158 - val_loss: 0.1165\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1122 - val_loss: 0.1044\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1115 - val_loss: 0.1044\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1115 - val_loss: 0.0950\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1146 - val_loss: 0.1621\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1144 - val_loss: 0.1122\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1150 - val_loss: 0.1007\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1092 - val_loss: 0.1045\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1119 - val_loss: 0.0968\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.1105 - val_loss: 0.1161\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1098 - val_loss: 0.0978\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1104 - val_loss: 0.1056\n",
      "Epoch 62/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1101 - val_loss: 0.1161\n",
      "Epoch 63/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1104 - val_loss: 0.1130\n",
      "Training completed. Number of epochs: 63 , Final training loss: 0.11037074774503708 , Final validation loss: 0.11302099376916885\n",
      "Training network # 36\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 553us/step - loss: 3.6867 - val_loss: 3.5422\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 3.6303 - val_loss: 3.6610\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 3.6160 - val_loss: 3.8079\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 1.2034 - val_loss: 0.3826\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.3390 - val_loss: 0.2856\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.2707 - val_loss: 0.2307\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2440 - val_loss: 0.2097\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.2217 - val_loss: 0.2247\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2111 - val_loss: 0.1714\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1920 - val_loss: 0.2005\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1813 - val_loss: 0.1571\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1729 - val_loss: 0.1561\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1673 - val_loss: 0.1421\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1591 - val_loss: 0.1362\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1534 - val_loss: 0.1373\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1509 - val_loss: 0.1293\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1446 - val_loss: 0.1378\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1426 - val_loss: 0.1413\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1378 - val_loss: 0.1638\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1388 - val_loss: 0.1225\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1350 - val_loss: 0.1950\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1311 - val_loss: 0.1111\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1284 - val_loss: 0.1451\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1279 - val_loss: 0.1191\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1236 - val_loss: 0.1118\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1273 - val_loss: 0.1241\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 500us/step - loss: 0.1262 - val_loss: 0.1138\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1200 - val_loss: 0.1153\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1201 - val_loss: 0.1705\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1217 - val_loss: 0.1307\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1240 - val_loss: 0.1388\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.1208 - val_loss: 0.1103\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1186 - val_loss: 0.1075\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1182 - val_loss: 0.1135\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1174 - val_loss: 0.1033\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1156 - val_loss: 0.1082\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1185 - val_loss: 0.1071\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1149 - val_loss: 0.1950\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 497us/step - loss: 0.1173 - val_loss: 0.1077\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1162 - val_loss: 0.1065\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1143 - val_loss: 0.1076\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1118 - val_loss: 0.1112\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1142 - val_loss: 0.1201\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1124 - val_loss: 0.1126\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 494us/step - loss: 0.1113 - val_loss: 0.1119\n",
      "Training completed. Number of epochs: 45 , Final training loss: 0.11134134978055954 , Final validation loss: 0.11189132928848267\n",
      "Training network # 37\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 569us/step - loss: 3.4979 - val_loss: 0.9323\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.4590 - val_loss: 0.3884\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.3251 - val_loss: 0.2608\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.2556 - val_loss: 0.2122\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.2370 - val_loss: 0.1870\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.2151 - val_loss: 0.1884\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.2104 - val_loss: 0.2074\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1950 - val_loss: 0.1613\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1815 - val_loss: 0.1660\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1793 - val_loss: 0.1777\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1700 - val_loss: 0.1562\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1597 - val_loss: 0.1399\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1563 - val_loss: 0.1316\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1495 - val_loss: 0.1595\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1473 - val_loss: 0.1401\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1420 - val_loss: 0.1343\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1370 - val_loss: 0.1310\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1391 - val_loss: 0.1630\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1329 - val_loss: 0.1238\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1346 - val_loss: 0.1260\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1303 - val_loss: 0.1264\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1291 - val_loss: 0.1154\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1319 - val_loss: 0.1243\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1315 - val_loss: 0.1172\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1243 - val_loss: 0.1192\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1245 - val_loss: 0.1094\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1242 - val_loss: 0.1196\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1254 - val_loss: 0.1177\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1276 - val_loss: 0.1409\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1231 - val_loss: 0.1064\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1177 - val_loss: 0.1274\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1234 - val_loss: 0.1151\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1214 - val_loss: 0.1103\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1185 - val_loss: 0.1066\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1200 - val_loss: 0.1201\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1188 - val_loss: 0.1078\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1173 - val_loss: 0.1104\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1178 - val_loss: 0.1138\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1160 - val_loss: 0.1224\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1185 - val_loss: 0.1018\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1158 - val_loss: 0.1120\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1140 - val_loss: 0.1040\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1172 - val_loss: 0.1053\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1142 - val_loss: 0.1251\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1116 - val_loss: 0.0984\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1134 - val_loss: 0.1020\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1140 - val_loss: 0.0950\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1149 - val_loss: 0.1176\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1109 - val_loss: 0.0993\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1138 - val_loss: 0.1063\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1105 - val_loss: 0.0995\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1105 - val_loss: 0.1103\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 499us/step - loss: 0.1120 - val_loss: 0.0990\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1090 - val_loss: 0.1443\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1137 - val_loss: 0.1065\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1130 - val_loss: 0.1044\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1121 - val_loss: 0.1079\n",
      "Training completed. Number of epochs: 57 , Final training loss: 0.11210669577121735 , Final validation loss: 0.10791244357824326\n",
      "Training network # 38\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 574us/step - loss: 1.5050 - val_loss: 0.4789\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 495us/step - loss: 0.4765 - val_loss: 0.6451\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.3656 - val_loss: 0.3280\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.2773 - val_loss: 0.2312\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.2507 - val_loss: 0.2361\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.2245 - val_loss: 0.1903\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.2136 - val_loss: 0.1901\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.2070 - val_loss: 0.2078\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.2085 - val_loss: 0.2212\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1908 - val_loss: 0.1647\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1906 - val_loss: 0.1688\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1834 - val_loss: 0.1779\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1778 - val_loss: 0.1640\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1699 - val_loss: 0.1648\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 491us/step - loss: 0.1711 - val_loss: 0.1641\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1638 - val_loss: 0.1870\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1648 - val_loss: 0.1594\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1622 - val_loss: 0.1439\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1582 - val_loss: 0.1454\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1556 - val_loss: 0.1380\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1554 - val_loss: 0.1754\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1487 - val_loss: 0.1478\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1532 - val_loss: 0.1290\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1491 - val_loss: 0.1287\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1513 - val_loss: 0.2020\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1455 - val_loss: 0.1587\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1446 - val_loss: 0.1421\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1506 - val_loss: 0.1570\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1470 - val_loss: 0.1310\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1401 - val_loss: 0.1470\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1380 - val_loss: 0.1520\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1391 - val_loss: 0.1514\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 498us/step - loss: 0.1406 - val_loss: 0.1724\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1400 - val_loss: 0.1300\n",
      "Training completed. Number of epochs: 34 , Final training loss: 0.13999108970165253 , Final validation loss: 0.12997785210609436\n",
      "Training network # 39\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 574us/step - loss: 1.9641 - val_loss: 0.7520\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.4601 - val_loss: 0.4107\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.3617 - val_loss: 0.4716\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.2799 - val_loss: 0.2340\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2428 - val_loss: 0.2165\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.2369 - val_loss: 0.2247\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.2255 - val_loss: 0.1887\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.2022 - val_loss: 0.1916\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1920 - val_loss: 0.1861\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1833 - val_loss: 0.1552\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1821 - val_loss: 0.1825\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1663 - val_loss: 0.1628\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1685 - val_loss: 0.1503\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1579 - val_loss: 0.1713\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1637 - val_loss: 0.1415\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1587 - val_loss: 0.1553\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1540 - val_loss: 0.1619\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1548 - val_loss: 0.1353\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1476 - val_loss: 0.1333\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1471 - val_loss: 0.1426\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1498 - val_loss: 0.1442\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1488 - val_loss: 0.1325\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 560us/step - loss: 0.1446 - val_loss: 0.1229\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 552us/step - loss: 0.1441 - val_loss: 0.1402\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 591us/step - loss: 0.1370 - val_loss: 0.1584\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1369 - val_loss: 0.1562\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1382 - val_loss: 0.1642\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1370 - val_loss: 0.1441\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1341 - val_loss: 0.1144\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1319 - val_loss: 0.1241\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1358 - val_loss: 0.1198\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1297 - val_loss: 0.1238\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1297 - val_loss: 0.1230\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1290 - val_loss: 0.1580\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.1308 - val_loss: 0.1165\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1263 - val_loss: 0.1548\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1285 - val_loss: 0.1261\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1283 - val_loss: 0.1488\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1278 - val_loss: 0.1424\n",
      "Training completed. Number of epochs: 39 , Final training loss: 0.12780942022800446 , Final validation loss: 0.14236173033714294\n",
      "Training network # 40\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 562us/step - loss: 1.6751 - val_loss: 0.4405\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.4305 - val_loss: 0.3700\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 1s 689us/step - loss: 0.2922 - val_loss: 0.2219\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.2503 - val_loss: 0.2143\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.2271 - val_loss: 0.2138\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.2184 - val_loss: 0.1926\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.2095 - val_loss: 0.1805\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1958 - val_loss: 0.1759\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1906 - val_loss: 0.1811\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1774 - val_loss: 0.1975\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1755 - val_loss: 0.1750\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1752 - val_loss: 0.1445\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1696 - val_loss: 0.1567\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1707 - val_loss: 0.2901\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1566 - val_loss: 0.1640\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1548 - val_loss: 0.1334\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1565 - val_loss: 0.1347\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1476 - val_loss: 0.1283\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1446 - val_loss: 0.1248\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1400 - val_loss: 0.1342\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1391 - val_loss: 0.1142\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1382 - val_loss: 0.1416\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1308 - val_loss: 0.1190\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1381 - val_loss: 0.1409\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1331 - val_loss: 0.1378\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1292 - val_loss: 0.1139\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1255 - val_loss: 0.1126\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1278 - val_loss: 0.1132\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1266 - val_loss: 0.1459\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1261 - val_loss: 0.1054\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1247 - val_loss: 0.1203\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1235 - val_loss: 0.1059\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1191 - val_loss: 0.1146\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1202 - val_loss: 0.1370\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1198 - val_loss: 0.1107\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1226 - val_loss: 0.1086\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1166 - val_loss: 0.1181\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1164 - val_loss: 0.1043\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1161 - val_loss: 0.1189\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1202 - val_loss: 0.1085\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1158 - val_loss: 0.2494\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1224 - val_loss: 0.1045\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1146 - val_loss: 0.1103\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1128 - val_loss: 0.1199\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1139 - val_loss: 0.1084\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1155 - val_loss: 0.1079\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1117 - val_loss: 0.1129\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1147 - val_loss: 0.1196\n",
      "Training completed. Number of epochs: 48 , Final training loss: 0.11465340107679367 , Final validation loss: 0.11964596807956696\n",
      "Training network # 41\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 566us/step - loss: 1.9820 - val_loss: 0.4324\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.4272 - val_loss: 0.4653\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 539us/step - loss: 0.3390 - val_loss: 0.2744\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.2806 - val_loss: 0.2407\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.2534 - val_loss: 0.2303\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.2368 - val_loss: 0.1880\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.2180 - val_loss: 0.2345\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1940 - val_loss: 0.1745\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1892 - val_loss: 0.1574\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1798 - val_loss: 0.1655\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 593us/step - loss: 0.1757 - val_loss: 0.1486\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 590us/step - loss: 0.1730 - val_loss: 0.1621\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 575us/step - loss: 0.1764 - val_loss: 0.1679\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 561us/step - loss: 0.1608 - val_loss: 0.1446\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 573us/step - loss: 0.1581 - val_loss: 0.1487\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 579us/step - loss: 0.1540 - val_loss: 0.1562\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 588us/step - loss: 0.1491 - val_loss: 0.1616\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 562us/step - loss: 0.1513 - val_loss: 0.1334\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 578us/step - loss: 0.1415 - val_loss: 0.1602\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 579us/step - loss: 0.1424 - val_loss: 0.1293\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 575us/step - loss: 0.1395 - val_loss: 0.1269\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 562us/step - loss: 0.1360 - val_loss: 0.1393\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 583us/step - loss: 0.1364 - val_loss: 0.1314\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 574us/step - loss: 0.1316 - val_loss: 0.1157\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 569us/step - loss: 0.1348 - val_loss: 0.1453\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1324 - val_loss: 0.1645\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1297 - val_loss: 0.1162\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1268 - val_loss: 0.1712\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1296 - val_loss: 0.1206\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1310 - val_loss: 0.1322\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1251 - val_loss: 0.1277\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1288 - val_loss: 0.1366\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1296 - val_loss: 0.1143\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1248 - val_loss: 0.1111\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1239 - val_loss: 0.1437\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1212 - val_loss: 0.1104\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1198 - val_loss: 0.1319\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1225 - val_loss: 0.1128\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1197 - val_loss: 0.1299\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 536us/step - loss: 0.1182 - val_loss: 0.1090\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1220 - val_loss: 0.1163\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1177 - val_loss: 0.1066\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 514us/step - loss: 0.1172 - val_loss: 0.1096\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1166 - val_loss: 0.1005\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1163 - val_loss: 0.1111\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1194 - val_loss: 0.1192\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1153 - val_loss: 0.1102\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1187 - val_loss: 0.1054\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1149 - val_loss: 0.1147\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1160 - val_loss: 0.1150\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 542us/step - loss: 0.1158 - val_loss: 0.1027\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1130 - val_loss: 0.1196\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1157 - val_loss: 0.1065\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.1133 - val_loss: 0.1323\n",
      "Training completed. Number of epochs: 54 , Final training loss: 0.11326295137405396 , Final validation loss: 0.13234785199165344\n",
      "Training network # 42\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 567us/step - loss: 3.6801 - val_loss: 3.6230\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 3.6095 - val_loss: 3.5401\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 1.7229 - val_loss: 0.4603\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.3607 - val_loss: 0.3131\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.2812 - val_loss: 0.2394\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2425 - val_loss: 0.2098\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.2113 - val_loss: 0.1975\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.2000 - val_loss: 0.2847\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1830 - val_loss: 0.1828\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1783 - val_loss: 0.1421\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1666 - val_loss: 0.1378\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1585 - val_loss: 0.1479\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1484 - val_loss: 0.1295\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1459 - val_loss: 0.1326\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1389 - val_loss: 0.1186\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1400 - val_loss: 0.1505\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1387 - val_loss: 0.1332\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1348 - val_loss: 0.1160\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1294 - val_loss: 0.1122\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1294 - val_loss: 0.1132\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1293 - val_loss: 0.1263\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1275 - val_loss: 0.1197\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1311 - val_loss: 0.1106\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1257 - val_loss: 0.1221\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1240 - val_loss: 0.1186\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1271 - val_loss: 0.1166\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1234 - val_loss: 0.1177\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1184 - val_loss: 0.1254\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1219 - val_loss: 0.1265\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1188 - val_loss: 0.1232\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1221 - val_loss: 0.1115\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1195 - val_loss: 0.1197\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1172 - val_loss: 0.1112\n",
      "Training completed. Number of epochs: 33 , Final training loss: 0.11716810613870621 , Final validation loss: 0.11124485731124878\n",
      "Training network # 43\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 582us/step - loss: 1.3898 - val_loss: 0.4460\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.4512 - val_loss: 0.3681\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.3359 - val_loss: 0.2828\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.2606 - val_loss: 0.2729\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.2536 - val_loss: 0.2211\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2204 - val_loss: 0.1925\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.2118 - val_loss: 0.1984\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2043 - val_loss: 0.4430\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1955 - val_loss: 0.1671\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1937 - val_loss: 0.2615\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1861 - val_loss: 0.1611\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1802 - val_loss: 0.1535\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1736 - val_loss: 0.1513\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1672 - val_loss: 0.1640\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1672 - val_loss: 0.1386\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1603 - val_loss: 0.1420\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1605 - val_loss: 0.1567\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1549 - val_loss: 0.1737\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1568 - val_loss: 0.1509\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1507 - val_loss: 0.1385\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1503 - val_loss: 0.1471\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1472 - val_loss: 0.1368\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1542 - val_loss: 0.1376\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1449 - val_loss: 0.1214\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1452 - val_loss: 0.1273\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1367 - val_loss: 0.1213\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1357 - val_loss: 0.1401\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1394 - val_loss: 0.2774\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1402 - val_loss: 0.1393\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1368 - val_loss: 0.1213\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1358 - val_loss: 0.1459\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1343 - val_loss: 0.1478\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1356 - val_loss: 0.1139\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1320 - val_loss: 0.1561\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1297 - val_loss: 0.1135\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1331 - val_loss: 0.1155\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1277 - val_loss: 0.1211\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1286 - val_loss: 0.1248\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1295 - val_loss: 0.1199\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1326 - val_loss: 0.1445\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1254 - val_loss: 0.1325\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1284 - val_loss: 0.1123\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1267 - val_loss: 0.1611\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1274 - val_loss: 0.1145\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1255 - val_loss: 0.1101\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1225 - val_loss: 0.1143\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1232 - val_loss: 0.1532\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1249 - val_loss: 0.1229\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1233 - val_loss: 0.1178\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1203 - val_loss: 0.1104\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1210 - val_loss: 0.1358\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1221 - val_loss: 0.1198\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1242 - val_loss: 0.1327\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1248 - val_loss: 0.1213\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1179 - val_loss: 0.1301\n",
      "Training completed. Number of epochs: 55 , Final training loss: 0.11790672689676285 , Final validation loss: 0.13009501993656158\n",
      "Training network # 44\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 559us/step - loss: 1.3026 - val_loss: 0.4513\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.4777 - val_loss: 0.5073\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.4279 - val_loss: 0.3543\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.3200 - val_loss: 0.2524\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.2643 - val_loss: 0.2349\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.2345 - val_loss: 0.3325\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.2200 - val_loss: 0.1859\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.2079 - val_loss: 0.1850\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1943 - val_loss: 0.1666\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1950 - val_loss: 0.2002\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1793 - val_loss: 0.1715\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1740 - val_loss: 0.1593\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 502us/step - loss: 0.1752 - val_loss: 0.1706\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1661 - val_loss: 0.1789\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1646 - val_loss: 0.1432\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1606 - val_loss: 0.1404\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1572 - val_loss: 0.1406\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 530us/step - loss: 0.1562 - val_loss: 0.1780\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1587 - val_loss: 0.1463\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1525 - val_loss: 0.1833\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1502 - val_loss: 0.1311\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 512us/step - loss: 0.1505 - val_loss: 0.1579\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1480 - val_loss: 0.1449\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1474 - val_loss: 0.1593\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1420 - val_loss: 0.1534\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1416 - val_loss: 0.1378\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1449 - val_loss: 0.1241\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1394 - val_loss: 0.1231\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1396 - val_loss: 0.1393\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1403 - val_loss: 0.1660\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1361 - val_loss: 0.1254\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1398 - val_loss: 0.1262\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1378 - val_loss: 0.1219\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1403 - val_loss: 0.1195\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1380 - val_loss: 0.1193\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1405 - val_loss: 0.1267\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1340 - val_loss: 0.1233\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1334 - val_loss: 0.2295\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1329 - val_loss: 0.1226\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1322 - val_loss: 0.1210\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1349 - val_loss: 0.1180\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1346 - val_loss: 0.1420\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1319 - val_loss: 0.1317\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1302 - val_loss: 0.1204\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1346 - val_loss: 0.1424\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1324 - val_loss: 0.1284\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1304 - val_loss: 0.1321\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1288 - val_loss: 0.1188\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1334 - val_loss: 0.1469\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.1295 - val_loss: 0.1287\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1309 - val_loss: 0.1293\n",
      "Training completed. Number of epochs: 51 , Final training loss: 0.13091380894184113 , Final validation loss: 0.1293315887451172\n",
      "Training network # 45\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 589us/step - loss: 1.1970 - val_loss: 0.5390\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.4662 - val_loss: 0.4358\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.4041 - val_loss: 0.3133\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.3040 - val_loss: 0.3032\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.2624 - val_loss: 0.3014\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.2393 - val_loss: 0.3115\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2205 - val_loss: 0.1897\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.2021 - val_loss: 0.1911\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1965 - val_loss: 0.1680\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1941 - val_loss: 0.1650\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1874 - val_loss: 0.1892\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1818 - val_loss: 0.1735\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1788 - val_loss: 0.1937\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1726 - val_loss: 0.1545\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1698 - val_loss: 0.1804\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1661 - val_loss: 0.1647\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1648 - val_loss: 0.1464\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1623 - val_loss: 0.1650\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1589 - val_loss: 0.1507\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1548 - val_loss: 0.1363\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1580 - val_loss: 0.1733\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 541us/step - loss: 0.1527 - val_loss: 0.1396\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 509us/step - loss: 0.1512 - val_loss: 0.1414\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 537us/step - loss: 0.1501 - val_loss: 0.1379\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1480 - val_loss: 0.1306\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1472 - val_loss: 0.1351\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1484 - val_loss: 0.1363\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1450 - val_loss: 0.1481\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.1440 - val_loss: 0.1287\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1415 - val_loss: 0.1280\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1413 - val_loss: 0.2448\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1398 - val_loss: 0.1287\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1407 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1390 - val_loss: 0.1276\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1384 - val_loss: 0.1334\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1410 - val_loss: 0.1244\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1408 - val_loss: 0.1281\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1395 - val_loss: 0.1303\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1363 - val_loss: 0.1234\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1350 - val_loss: 0.1261\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1373 - val_loss: 0.1316\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1338 - val_loss: 0.1481\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1351 - val_loss: 0.1213\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1347 - val_loss: 0.1252\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1323 - val_loss: 0.1159\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1365 - val_loss: 0.1216\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1320 - val_loss: 0.1214\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1324 - val_loss: 0.1430\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1339 - val_loss: 0.1245\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 516us/step - loss: 0.1330 - val_loss: 0.1295\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1304 - val_loss: 0.1136\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1326 - val_loss: 0.1235\n",
      "Epoch 53/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1283 - val_loss: 0.1215\n",
      "Epoch 54/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1308 - val_loss: 0.1281\n",
      "Epoch 55/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1294 - val_loss: 0.1396\n",
      "Epoch 56/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1286 - val_loss: 0.1358\n",
      "Epoch 57/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1288 - val_loss: 0.1452\n",
      "Epoch 58/100\n",
      "760/760 [==============================] - 0s 517us/step - loss: 0.1302 - val_loss: 0.1176\n",
      "Epoch 59/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1272 - val_loss: 0.1398\n",
      "Epoch 60/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1292 - val_loss: 0.1806\n",
      "Epoch 61/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1287 - val_loss: 0.1174\n",
      "Training completed. Number of epochs: 61 , Final training loss: 0.12874837219715118 , Final validation loss: 0.11735032498836517\n",
      "Training network # 46\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 560us/step - loss: 3.6659 - val_loss: 3.7920\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 3.6084 - val_loss: 3.5458\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 2.2724 - val_loss: 0.4976\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 534us/step - loss: 0.3735 - val_loss: 0.5639\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 506us/step - loss: 0.2719 - val_loss: 0.2354\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2397 - val_loss: 0.3411\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 532us/step - loss: 0.2178 - val_loss: 0.1839\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.1985 - val_loss: 0.2053\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1827 - val_loss: 0.2096\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1766 - val_loss: 0.2027\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1625 - val_loss: 0.1750\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1528 - val_loss: 0.1707\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1437 - val_loss: 0.1450\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 533us/step - loss: 0.1397 - val_loss: 0.1573\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1387 - val_loss: 0.1163\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 511us/step - loss: 0.1411 - val_loss: 0.1772\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1347 - val_loss: 0.1263\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 523us/step - loss: 0.1326 - val_loss: 0.1240\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1334 - val_loss: 0.1197\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1301 - val_loss: 0.1082\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1257 - val_loss: 0.1198\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 520us/step - loss: 0.1228 - val_loss: 0.1175\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1233 - val_loss: 0.1033\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1233 - val_loss: 0.1233\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1220 - val_loss: 0.1199\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1186 - val_loss: 0.1226\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 508us/step - loss: 0.1198 - val_loss: 0.1163\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 529us/step - loss: 0.1211 - val_loss: 0.1731\n",
      "Epoch 29/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1162 - val_loss: 0.1024\n",
      "Epoch 30/100\n",
      "760/760 [==============================] - 0s 510us/step - loss: 0.1178 - val_loss: 0.1047\n",
      "Epoch 31/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1168 - val_loss: 0.1023\n",
      "Epoch 32/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1173 - val_loss: 0.1343\n",
      "Epoch 33/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1187 - val_loss: 0.1083\n",
      "Epoch 34/100\n",
      "760/760 [==============================] - 0s 507us/step - loss: 0.1162 - val_loss: 0.1211\n",
      "Epoch 35/100\n",
      "760/760 [==============================] - 0s 544us/step - loss: 0.1196 - val_loss: 0.1035\n",
      "Epoch 36/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.1111 - val_loss: 0.1123\n",
      "Epoch 37/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1139 - val_loss: 0.1063\n",
      "Epoch 38/100\n",
      "760/760 [==============================] - 0s 554us/step - loss: 0.1135 - val_loss: 0.1049\n",
      "Epoch 39/100\n",
      "760/760 [==============================] - 0s 541us/step - loss: 0.1124 - val_loss: 0.0965\n",
      "Epoch 40/100\n",
      "760/760 [==============================] - 0s 544us/step - loss: 0.1125 - val_loss: 0.1310\n",
      "Epoch 41/100\n",
      "760/760 [==============================] - 0s 535us/step - loss: 0.1150 - val_loss: 0.1105\n",
      "Epoch 42/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1127 - val_loss: 0.0949\n",
      "Epoch 43/100\n",
      "760/760 [==============================] - 0s 528us/step - loss: 0.1099 - val_loss: 0.1053\n",
      "Epoch 44/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1102 - val_loss: 0.1079\n",
      "Epoch 45/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1086 - val_loss: 0.1019\n",
      "Epoch 46/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1144 - val_loss: 0.1002\n",
      "Epoch 47/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1104 - val_loss: 0.1846\n",
      "Epoch 48/100\n",
      "760/760 [==============================] - 0s 513us/step - loss: 0.1107 - val_loss: 0.1202\n",
      "Epoch 49/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1103 - val_loss: 0.1211\n",
      "Epoch 50/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1124 - val_loss: 0.1156\n",
      "Epoch 51/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1090 - val_loss: 0.1034\n",
      "Epoch 52/100\n",
      "760/760 [==============================] - 0s 527us/step - loss: 0.1086 - val_loss: 0.1155\n",
      "Training completed. Number of epochs: 52 , Final training loss: 0.10861605405807495 , Final validation loss: 0.11553846299648285\n",
      "Training network # 47\n",
      "Epoch 1/100\n",
      "760/760 [==============================] - 1s 583us/step - loss: 3.7034 - val_loss: 3.5796\n",
      "Epoch 2/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 3.3565 - val_loss: 0.4802\n",
      "Epoch 3/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.4531 - val_loss: 0.3919\n",
      "Epoch 4/100\n",
      "760/760 [==============================] - 0s 519us/step - loss: 0.3448 - val_loss: 0.4535\n",
      "Epoch 5/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.2649 - val_loss: 0.2191\n",
      "Epoch 6/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.2327 - val_loss: 0.2023\n",
      "Epoch 7/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.2140 - val_loss: 0.1715\n",
      "Epoch 8/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1967 - val_loss: 0.3137\n",
      "Epoch 9/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1859 - val_loss: 0.1613\n",
      "Epoch 10/100\n",
      "760/760 [==============================] - 0s 501us/step - loss: 0.1765 - val_loss: 0.1987\n",
      "Epoch 11/100\n",
      "760/760 [==============================] - 0s 518us/step - loss: 0.1644 - val_loss: 0.1641\n",
      "Epoch 12/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1623 - val_loss: 0.1526\n",
      "Epoch 13/100\n",
      "760/760 [==============================] - 0s 515us/step - loss: 0.1569 - val_loss: 0.1591\n",
      "Epoch 14/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1551 - val_loss: 0.1397\n",
      "Epoch 15/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1485 - val_loss: 0.1292\n",
      "Epoch 16/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1468 - val_loss: 0.1317\n",
      "Epoch 17/100\n",
      "760/760 [==============================] - 0s 496us/step - loss: 0.1438 - val_loss: 0.1406\n",
      "Epoch 18/100\n",
      "760/760 [==============================] - 0s 524us/step - loss: 0.1456 - val_loss: 0.1192\n",
      "Epoch 19/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1362 - val_loss: 0.3656\n",
      "Epoch 20/100\n",
      "760/760 [==============================] - 0s 521us/step - loss: 0.1347 - val_loss: 0.1255\n",
      "Epoch 21/100\n",
      "760/760 [==============================] - 0s 505us/step - loss: 0.1346 - val_loss: 0.1258\n",
      "Epoch 22/100\n",
      "760/760 [==============================] - 0s 525us/step - loss: 0.1356 - val_loss: 0.1482\n",
      "Epoch 23/100\n",
      "760/760 [==============================] - 0s 526us/step - loss: 0.1304 - val_loss: 0.1948\n",
      "Epoch 24/100\n",
      "760/760 [==============================] - 0s 504us/step - loss: 0.1322 - val_loss: 0.1208\n",
      "Epoch 25/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1272 - val_loss: 0.1262\n",
      "Epoch 26/100\n",
      "760/760 [==============================] - 0s 522us/step - loss: 0.1279 - val_loss: 0.1599\n",
      "Epoch 27/100\n",
      "760/760 [==============================] - 0s 531us/step - loss: 0.1240 - val_loss: 0.1208\n",
      "Epoch 28/100\n",
      "760/760 [==============================] - 0s 503us/step - loss: 0.1253 - val_loss: 0.1367\n",
      "Training completed. Number of epochs: 28 , Final training loss: 0.12533016502857208 , Final validation loss: 0.136705219745636\n",
      "Total training time: 998.7971739768982 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    \"\"\"Creates a network with the original architecture.\"\"\"\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed = None)\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(26, input_shape=[PCA_components], activation='sigmoid', kernel_initializer= initializer),\n",
    "        layers.Dense(26, activation='sigmoid', kernel_initializer= initializer),\n",
    "        layers.Dense(1, activation = 'linear', kernel_initializer= initializer)\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = model_learning_rate)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingCompletionCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to display training information only at the end of the training of one network.\"\"\"\n",
    "    def on_train_end(self, logs=None):\n",
    "        epochs = len(self.model.history.history['loss'])\n",
    "        final_loss = self.model.history.history['loss'][-1]\n",
    "        final_val_loss = self.model.history.history['val_loss'][-1]\n",
    "\n",
    "        print(\"Training completed. Number of epochs:\", epochs, \", Final training loss:\", final_loss, \", Final validation loss:\", final_val_loss)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_epochs, restore_best_weights=True)\n",
    "completion_callback = TrainingCompletionCallback()\n",
    "\n",
    "start_time = time.time() # Measure training time\n",
    "\n",
    "# Train a committee of 48 neural networks\n",
    "committee = []\n",
    "for i in range(N_committee):\n",
    "    model = create_model()\n",
    "    print(\"Training network #\",i)\n",
    "    learning = model.fit(x_train, y_train, epochs= N_epochs, verbose = 1, callbacks = [completion_callback, early_stopping], validation_data=(x_val,y_val))\n",
    "    committee.append(model)   \n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Total training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the committee models\n",
    "\n",
    "# os.makedirs(\"../../../output/committee_models\", exist_ok=True)\n",
    "\n",
    "# for i, model in enumerate(committee):\n",
    "#     model.save(f\"../../../output/committee_models/model_{i}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a trained committee\n",
    "\n",
    "# committee = []\n",
    "# committee_path = '/Users/ec270266/Documents/Phd/Euclid/dev/output/committee_models'\n",
    "# # Load each model in the committee\n",
    "# for i in range(N_committee):\n",
    "#     model_path = os.path.join(committee_path, f'model_{i}.h5')\n",
    "#     loaded_model = tf.keras.models.load_model(model_path)\n",
    "#     committee.append(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0936504744404108\n",
      "Variance:  [0.00549508 0.03555429 0.00994009 ... 0.01394403 0.00466378 0.018275  ]\n",
      "\n",
      "F1 score: [0.51669368 0.59163124 0.57269947 0.53485425 0.63588317 0.61027533\n",
      " 0.60549451 0.45083289 0.48071097 0.48054608 0.93952484 0.98019272\n",
      " 0.99621417 0.        ]\n",
      "Average F1 score: 0.645811792283784\n",
      "\n",
      "Confusion matrix:\n",
      "tf.Tensor(\n",
      "[[ 797 1015   38    0    1    0    0    0    0    0    0    0    0    6]\n",
      " [ 421 1322  108    8    0    0    0    0    0    0    0    0    0    1]\n",
      " [   8  228 1030  528   45    5    0    1    0    0    0    0    0    0]\n",
      " [   2   39  534 1055  242   19    2    0    0    0    0    0    0    0]\n",
      " [   0    5   42  434 1143  234   16    3    0    0    0    0    0    0]\n",
      " [   0    0    0   24  252 1075  406   47    4    0    0    0    0    0]\n",
      " [   0    0    0    2   33  347 1102  384   30    1    0    0    0    0]\n",
      " [   0    0    0    1    2   24  162  839  795   47    0    0    0    0]\n",
      " [   0    0    0    0    0    8   46  471 1190  153    2    0    0    0]\n",
      " [   0    0    0    0    0    3    7  107 1059  704   11    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    3  134 1740   46    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0   28 1831    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0 1842   14]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]], shape=(14, 14), dtype=int32)\n",
      "\n",
      "Success rate: 0.9740003291097581\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "x=x_train\n",
    "y_true=y_train\n",
    "SED_true=SED_train\n",
    "\n",
    "committee_predictions = []\n",
    "for model in committee:\n",
    "    committee_predictions.append(model.predict(x, verbose = 0).reshape(-1)) # Predict the scalar parameter C using the committee   \n",
    "committee_predictions = np.array(committee_predictions)\n",
    "y_pred = np.mean(committee_predictions, axis=0)\n",
    "pred_variance = np.var(committee_predictions, axis=0)\n",
    "SED_pred = CtoSEDarray(y_pred,pred_variance)\n",
    "\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print('MSE:', mse)\n",
    "print(\"Variance: \", pred_variance)\n",
    "\n",
    "f1 = f1_score(SED_true, SED_pred, average = None)\n",
    "f1_mean = np.mean(f1[:13])\n",
    "print('\\nF1 score:', f1)\n",
    "print('Average F1 score:', f1_mean)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(SED_true, SED_pred) \n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "success_rate = calculate_success_rate(confusion_matrix)\n",
    "print('\\nSuccess rate:', success_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "star_class_labels = ['O5','B0','B5','A0','A5','F0','F5','G0','G5','K0','K5','M0','M5']\n",
    "\n",
    "plt.figure(figsize= (12,10))\n",
    "heatmap = plt.imshow(confusion_matrix[:13,:], cmap='Blues')\n",
    "plt.xticks(np.arange(14), star_class_labels + ['???'])\n",
    "plt.yticks(np.arange(13), star_class_labels)\n",
    "plt.colorbar(heatmap)\n",
    "plt.xlabel(\"Estimated spectral type\")\n",
    "plt.ylabel(\"True spectral type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x30b22f550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the metrics\n",
    "\n",
    "plt.figure(figsize = (9,5))\n",
    "plt.bar(np.arange(13), height = f1[:13], tick_label = star_class_labels ,label = \"F1 score\")\n",
    "plt.axhline(f1_mean, color='red', linestyle='--', label = 'F1 score average')\n",
    "plt.axhline(success_rate, color='purple', label = 'Success rate')\n",
    "plt.xlabel(\"Spectral class\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 998.7971739768982 seconds\n",
      "Training loss: 0.12533016502857208 , Validation loss: 0.136705219745636\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLot the loss function evolution\n",
    "\n",
    "loss_evolution = learning.history[\"loss\"]\n",
    "val_loss_evolution = learning.history[\"val_loss\"]\n",
    "\n",
    "plt.figure(figsize = (9,5))\n",
    "plt.plot(loss_evolution,label = \"Train set\")\n",
    "plt.plot(val_loss_evolution,label = \"Validation set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss function value\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss function evolution\")\n",
    "print(\"Total training time:\", training_time, \"seconds\")\n",
    "print(\"Training loss:\", loss_evolution[-1], \", Validation loss:\", val_loss_evolution[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m2 vs 2\u001b[0m\n",
      "3 vs 2\n",
      "9 vs 8\n",
      "\u001b[4m7 vs 7\u001b[0m\n",
      "0 vs 1\n",
      "3 vs 4\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "\u001b[4m12 vs 12\u001b[0m\n",
      "\u001b[4m10 vs 10\u001b[0m\n",
      "\u001b[4m1 vs 1\u001b[0m\n",
      "\u001b[4m1 vs 1\u001b[0m\n",
      "\u001b[4m7 vs 7\u001b[0m\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "5 vs 6\n",
      "9 vs 7\n",
      "\u001b[4m1 vs 1\u001b[0m\n",
      "\u001b[4m11 vs 11\u001b[0m\n",
      "2 vs 3\n",
      "\u001b[4m11 vs 11\u001b[0m\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "\u001b[4m1 vs 1\u001b[0m\n",
      "\u001b[4m4 vs 4\u001b[0m\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "\u001b[4m0 vs 0\u001b[0m\n",
      "\u001b[4m1 vs 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    pred=str(SED_pred[i])\n",
    "    test=str(SED_true[i])\n",
    "    if pred == test:\n",
    "        print('\\033[4m'+test+' vs '+pred+'\\033[0m')\n",
    "    else:\n",
    "        print(test+' vs '+pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
