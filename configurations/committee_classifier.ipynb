{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA classifier with a committee of 48 networks\n",
    "# Optimal configuration from Kuntzer's paper https://arxiv.org/abs/1605.03201\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '/Users/ec270266/Documents/Phd/Euclid/dev/output/psf_dataset/'\n",
    "dataset = np.load(data_path + 'PCA_dataset2B24.npy', allow_pickle=True)[()]\n",
    "\n",
    "x_train = dataset['train_stars_pca']\n",
    "x_val = dataset['validation_stars_pca']\n",
    "x_test = dataset['test_stars_pca']\n",
    "y_train = dataset['train_C']\n",
    "y_val = dataset['validation_C']\n",
    "y_test = dataset['test_C']\n",
    "SED_test = dataset['test_SEDs']\n",
    "SED_train= dataset['train_SEDs']\n",
    "SED_val= dataset['validation_SEDs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "(20000,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEDlisttoC(SED_list):\n",
    "    \"\"\"Converts a stellar class (1 to 13) to the regression parameter C.\"\"\"\n",
    "    sed_array = np.array(SED_list)\n",
    "    return sed_array*0.5 + 1.5\n",
    "\n",
    "def CtoSEDarray(c_values, variance):\n",
    "    \"\"\"Converts the regression parameter C back to a stellar class.\n",
    "    If the C is out of bounds or its variance is too high it is classified as an anomaly (stellar class 20).\"\"\"\n",
    "    sed_classes = np.rint(((c_values - 1.5) * 2)).astype(int)\n",
    "    sed_classes = np.where((c_values < 1.4) | (c_values > 7.6), 13, sed_classes)\n",
    "    sed_classes = np.where((variance > 1.00), 13, sed_classes)\n",
    "    return sed_classes\n",
    "\n",
    "def calculate_success_rate(confusion_matrix):\n",
    "    \"\"\"Metric that contemplates success as the true spectral class with a tolerance of one adjacent class.\"\"\"\n",
    "    diagonal = np.trace(confusion_matrix)\n",
    "    diagonal_neighbors = np.sum(np.diagonal(confusion_matrix, offset=1)) + np.sum(np.diagonal(confusion_matrix, offset=-1))\n",
    "    total_classified = np.sum(confusion_matrix)\n",
    "    \n",
    "    success_rate = (diagonal + diagonal_neighbors) / total_classified\n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "PCA_components = 24\n",
    "model_learning_rate = 0.1\n",
    "N_epochs = 100\n",
    "N_committee = 48\n",
    "patience_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network # 0\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ec270266/anaconda3/envs/wavediff/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 606us/step - loss: 1.3878 - val_loss: 0.4015\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.3991 - val_loss: 0.3151\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.3052 - val_loss: 0.2675\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.2445 - val_loss: 0.2600\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2262 - val_loss: 0.2091\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1983 - val_loss: 0.1683\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1959 - val_loss: 0.1918\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1814 - val_loss: 0.2013\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1782 - val_loss: 0.1620\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1724 - val_loss: 0.1619\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 654us/step - loss: 0.1633 - val_loss: 0.1594\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 533us/step - loss: 0.1644 - val_loss: 0.1501\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 534us/step - loss: 0.1518 - val_loss: 0.1460\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1497 - val_loss: 0.1279\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1469 - val_loss: 0.1298\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1442 - val_loss: 0.1274\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1380 - val_loss: 0.1262\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1411 - val_loss: 0.1280\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1366 - val_loss: 0.1339\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1363 - val_loss: 0.1356\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1341 - val_loss: 0.1196\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1327 - val_loss: 0.1165\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1306 - val_loss: 0.1221\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1295 - val_loss: 0.1154\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1277 - val_loss: 0.1121\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1303 - val_loss: 0.1329\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1271 - val_loss: 0.1365\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 0.1242 - val_loss: 0.1134\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1220 - val_loss: 0.1106\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1213 - val_loss: 0.1183\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.1237 - val_loss: 0.1026\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1192 - val_loss: 0.1024\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1204 - val_loss: 0.1079\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1170 - val_loss: 0.1397\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1217 - val_loss: 0.1091\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 547us/step - loss: 0.1198 - val_loss: 0.1246\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1166 - val_loss: 0.1087\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1181 - val_loss: 0.0967\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1200 - val_loss: 0.1051\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1164 - val_loss: 0.1037\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1163 - val_loss: 0.1074\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1152 - val_loss: 0.1083\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1160 - val_loss: 0.1197\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1124 - val_loss: 0.1143\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 0.1152 - val_loss: 0.1000\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1135 - val_loss: 0.1058\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1160 - val_loss: 0.0991\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1118 - val_loss: 0.1273\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1149 - val_loss: 0.1040\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1120 - val_loss: 0.1087\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1140 - val_loss: 0.1235\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1156 - val_loss: 0.1261\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1131 - val_loss: 0.1060\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1116 - val_loss: 0.1035\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1134 - val_loss: 0.1131\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1141 - val_loss: 0.1045\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1089 - val_loss: 0.1181\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1132 - val_loss: 0.1118\n",
      "Training completed. Number of epochs: 58 , Final training loss: 0.1131817176938057 , Final validation loss: 0.1117788553237915\n",
      "Training network # 1\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 1.0614 - val_loss: 0.4126\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.4387 - val_loss: 0.3533\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.3028 - val_loss: 0.2983\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.2608 - val_loss: 0.2195\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 531us/step - loss: 0.2378 - val_loss: 0.2431\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.2145 - val_loss: 0.1837\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.2069 - val_loss: 0.1964\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1936 - val_loss: 0.1841\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1859 - val_loss: 0.1736\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1823 - val_loss: 0.1730\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1757 - val_loss: 0.1539\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1671 - val_loss: 0.1691\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1611 - val_loss: 0.1617\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1600 - val_loss: 0.1927\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1549 - val_loss: 0.1341\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1562 - val_loss: 0.1443\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1505 - val_loss: 0.1520\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.1511 - val_loss: 0.1695\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 533us/step - loss: 0.1483 - val_loss: 0.1574\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1514 - val_loss: 0.1354\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1426 - val_loss: 0.1641\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1433 - val_loss: 0.1680\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1413 - val_loss: 0.1382\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1412 - val_loss: 0.1312\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 0.1382 - val_loss: 0.1322\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1391 - val_loss: 0.1281\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1370 - val_loss: 0.1314\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1357 - val_loss: 0.1245\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.1376 - val_loss: 0.1450\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1341 - val_loss: 0.1257\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1335 - val_loss: 0.1285\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1307 - val_loss: 0.1483\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 547us/step - loss: 0.1351 - val_loss: 0.1222\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1323 - val_loss: 0.1203\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1332 - val_loss: 0.1157\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1319 - val_loss: 0.1257\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1380 - val_loss: 0.1412\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1287 - val_loss: 0.1408\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1321 - val_loss: 0.1503\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1306 - val_loss: 0.1311\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1279 - val_loss: 0.1300\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1281 - val_loss: 0.1293\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1273 - val_loss: 0.1132\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1297 - val_loss: 0.1530\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1262 - val_loss: 0.1283\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1249 - val_loss: 0.1255\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1274 - val_loss: 0.1150\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1280 - val_loss: 0.1162\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1377 - val_loss: 0.1336\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1275 - val_loss: 0.1142\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1240 - val_loss: 0.1186\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1294 - val_loss: 0.1132\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1243 - val_loss: 0.1347\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1235 - val_loss: 0.1557\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1263 - val_loss: 0.1228\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1254 - val_loss: 0.1422\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1681 - val_loss: 0.1150\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1283 - val_loss: 0.1175\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.1432 - val_loss: 0.1218\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1282 - val_loss: 0.1218\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1262 - val_loss: 0.1234\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1254 - val_loss: 0.1480\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1277 - val_loss: 0.1773\n",
      "Training completed. Number of epochs: 63 , Final training loss: 0.1276993751525879 , Final validation loss: 0.17726381123065948\n",
      "Training network # 2\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 1.1352 - val_loss: 0.6761\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.4269 - val_loss: 0.2707\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.2698 - val_loss: 0.2159\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.2393 - val_loss: 0.2090\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.2188 - val_loss: 0.1813\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.2027 - val_loss: 0.1658\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1814 - val_loss: 0.1553\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1719 - val_loss: 0.1441\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1649 - val_loss: 0.1658\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 540us/step - loss: 0.1621 - val_loss: 0.1300\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1558 - val_loss: 0.1330\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1565 - val_loss: 0.1410\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1451 - val_loss: 0.1236\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1416 - val_loss: 0.1203\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1480 - val_loss: 0.1205\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1356 - val_loss: 0.1376\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1392 - val_loss: 0.1350\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1307 - val_loss: 0.1565\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1303 - val_loss: 0.1277\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1310 - val_loss: 0.1366\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1298 - val_loss: 0.1194\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1285 - val_loss: 0.1298\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1246 - val_loss: 0.1310\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1259 - val_loss: 0.1763\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1263 - val_loss: 0.1107\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1237 - val_loss: 0.1101\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1200 - val_loss: 0.1237\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1222 - val_loss: 0.1152\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1219 - val_loss: 0.1121\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1225 - val_loss: 0.1323\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1190 - val_loss: 0.1109\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1176 - val_loss: 0.1036\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1187 - val_loss: 0.2429\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1162 - val_loss: 0.1396\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1170 - val_loss: 0.1525\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1173 - val_loss: 0.1481\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1162 - val_loss: 0.1341\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1164 - val_loss: 0.1172\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1166 - val_loss: 0.1042\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 547us/step - loss: 0.1130 - val_loss: 0.1147\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1163 - val_loss: 0.1032\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1136 - val_loss: 0.1037\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 551us/step - loss: 0.1129 - val_loss: 0.1103\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1121 - val_loss: 0.1172\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 551us/step - loss: 0.1116 - val_loss: 0.1062\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1103 - val_loss: 0.1087\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1107 - val_loss: 0.1060\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1111 - val_loss: 0.1456\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1113 - val_loss: 0.1203\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1106 - val_loss: 0.1062\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1113 - val_loss: 0.1061\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1094 - val_loss: 0.1182\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1097 - val_loss: 0.1083\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1080 - val_loss: 0.1131\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1103 - val_loss: 0.1028\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1081 - val_loss: 0.1025\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1094 - val_loss: 0.1066\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1098 - val_loss: 0.1240\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1064 - val_loss: 0.1041\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1109 - val_loss: 0.1169\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1069 - val_loss: 0.0938\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1103 - val_loss: 0.1480\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1052 - val_loss: 0.1018\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.1081 - val_loss: 0.1275\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1104 - val_loss: 0.0925\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1068 - val_loss: 0.0969\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1083 - val_loss: 0.1043\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1066 - val_loss: 0.1138\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1072 - val_loss: 0.1030\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1046 - val_loss: 0.0976\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1059 - val_loss: 0.0955\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1071 - val_loss: 0.1081\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1059 - val_loss: 0.1022\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1036 - val_loss: 0.0989\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1049 - val_loss: 0.1104\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1056 - val_loss: 0.1146\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1040 - val_loss: 0.1055\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1044 - val_loss: 0.1594\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1066 - val_loss: 0.0956\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1050 - val_loss: 0.1135\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1038 - val_loss: 0.1001\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1040 - val_loss: 0.1132\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1028 - val_loss: 0.1019\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1016 - val_loss: 0.0979\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1048 - val_loss: 0.0968\n",
      "Training completed. Number of epochs: 85 , Final training loss: 0.10477129369974136 , Final validation loss: 0.09683240950107574\n",
      "Training network # 3\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 1.1403 - val_loss: 0.5581\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.4289 - val_loss: 0.3259\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.3052 - val_loss: 0.2288\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.2519 - val_loss: 0.2060\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2206 - val_loss: 0.1880\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.2142 - val_loss: 0.1779\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1961 - val_loss: 0.2609\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1894 - val_loss: 0.1813\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.1787 - val_loss: 0.1821\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1678 - val_loss: 0.1520\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1654 - val_loss: 0.1659\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1604 - val_loss: 0.1565\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1570 - val_loss: 0.1644\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1573 - val_loss: 0.1432\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.1567 - val_loss: 0.1505\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1527 - val_loss: 0.1849\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.1478 - val_loss: 0.1236\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1487 - val_loss: 0.1404\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 0.1455 - val_loss: 0.1574\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1404 - val_loss: 0.1289\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1405 - val_loss: 0.1217\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1361 - val_loss: 0.1349\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1372 - val_loss: 0.1255\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.1323 - val_loss: 0.1612\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 666us/step - loss: 0.1349 - val_loss: 0.1382\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1310 - val_loss: 0.1291\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1350 - val_loss: 0.1349\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1290 - val_loss: 0.1220\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1284 - val_loss: 0.1271\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1310 - val_loss: 0.1272\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1276 - val_loss: 0.1238\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1305 - val_loss: 0.1173\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1282 - val_loss: 0.1375\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1280 - val_loss: 0.1287\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1268 - val_loss: 0.1195\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1232 - val_loss: 0.1205\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1231 - val_loss: 0.1091\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1238 - val_loss: 0.1232\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 0.1234 - val_loss: 0.1208\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1240 - val_loss: 0.1504\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1262 - val_loss: 0.1133\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1219 - val_loss: 0.1132\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1215 - val_loss: 0.1418\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1224 - val_loss: 0.1209\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.1201 - val_loss: 0.1123\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 0.1233 - val_loss: 0.1088\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 646us/step - loss: 0.1218 - val_loss: 0.1336\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1198 - val_loss: 0.1141\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1226 - val_loss: 0.1098\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1216 - val_loss: 0.1154\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1212 - val_loss: 0.1378\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 0.1190 - val_loss: 0.1130\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1180 - val_loss: 0.1094\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1186 - val_loss: 0.1212\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1175 - val_loss: 0.1093\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1168 - val_loss: 0.1164\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1196 - val_loss: 0.1104\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1173 - val_loss: 0.1183\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1181 - val_loss: 0.1091\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1156 - val_loss: 0.1105\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1162 - val_loss: 0.1115\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1168 - val_loss: 0.1231\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1168 - val_loss: 0.1277\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1157 - val_loss: 0.1115\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1141 - val_loss: 0.1084\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1169 - val_loss: 0.1273\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1165 - val_loss: 0.1095\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1152 - val_loss: 0.1347\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1171 - val_loss: 0.1254\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1129 - val_loss: 0.1219\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1140 - val_loss: 0.1091\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1136 - val_loss: 0.1087\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1145 - val_loss: 0.1451\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1144 - val_loss: 0.1079\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1106 - val_loss: 0.1131\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1141 - val_loss: 0.0985\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1488 - val_loss: 0.1727\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1142 - val_loss: 0.1042\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1161 - val_loss: 0.1049\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1113 - val_loss: 0.1080\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1132 - val_loss: 0.1050\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1117 - val_loss: 0.1043\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1144 - val_loss: 0.1067\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1127 - val_loss: 0.1055\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1136 - val_loss: 0.1142\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1126 - val_loss: 0.1036\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1098 - val_loss: 0.1060\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1095 - val_loss: 0.1037\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1127 - val_loss: 0.1536\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1124 - val_loss: 0.1143\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1129 - val_loss: 0.1320\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1102 - val_loss: 0.1010\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 809us/step - loss: 0.1100 - val_loss: 0.1018\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1102 - val_loss: 0.1104\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1123 - val_loss: 0.1277\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1100 - val_loss: 0.1184\n",
      "Training completed. Number of epochs: 96 , Final training loss: 0.11003892868757248 , Final validation loss: 0.11839606612920761\n",
      "Training network # 4\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 660us/step - loss: 1.1661 - val_loss: 0.4134\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.4597 - val_loss: 0.5150\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.3299 - val_loss: 0.2383\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.2667 - val_loss: 0.2385\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.2419 - val_loss: 0.3598\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.2160 - val_loss: 0.1725\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1983 - val_loss: 0.1712\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1926 - val_loss: 0.1590\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1813 - val_loss: 0.1530\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1719 - val_loss: 0.1642\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1693 - val_loss: 0.1665\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1660 - val_loss: 0.1606\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1635 - val_loss: 0.1479\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1576 - val_loss: 0.1583\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1542 - val_loss: 0.1922\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1547 - val_loss: 0.1395\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1474 - val_loss: 0.1456\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1456 - val_loss: 0.1464\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1465 - val_loss: 0.1297\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1444 - val_loss: 0.1364\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1400 - val_loss: 0.1451\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1408 - val_loss: 0.1286\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1396 - val_loss: 0.1238\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1402 - val_loss: 0.1846\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1376 - val_loss: 0.1309\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1355 - val_loss: 0.1404\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1391 - val_loss: 0.1444\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1345 - val_loss: 0.1514\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1308 - val_loss: 0.1327\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1323 - val_loss: 0.1196\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1292 - val_loss: 0.1222\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1274 - val_loss: 0.1397\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1284 - val_loss: 0.1245\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1282 - val_loss: 0.1231\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1272 - val_loss: 0.1299\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1254 - val_loss: 0.1223\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1273 - val_loss: 0.1208\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1227 - val_loss: 0.1261\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1221 - val_loss: 0.1194\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1217 - val_loss: 0.1278\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1208 - val_loss: 0.1160\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1207 - val_loss: 0.1087\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1210 - val_loss: 0.1349\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1207 - val_loss: 0.1081\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1190 - val_loss: 0.1170\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1165 - val_loss: 0.1029\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1159 - val_loss: 0.1112\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1175 - val_loss: 0.1108\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1145 - val_loss: 0.1111\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1162 - val_loss: 0.1070\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1161 - val_loss: 0.1233\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1142 - val_loss: 0.1388\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1124 - val_loss: 0.1068\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1145 - val_loss: 0.1341\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1106 - val_loss: 0.1034\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1144 - val_loss: 0.1138\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1151 - val_loss: 0.1124\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1121 - val_loss: 0.1009\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1111 - val_loss: 0.1488\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1084 - val_loss: 0.0996\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1119 - val_loss: 0.1211\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1102 - val_loss: 0.1168\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1083 - val_loss: 0.1331\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1119 - val_loss: 0.1064\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1082 - val_loss: 0.1099\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1185 - val_loss: 0.1045\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1104 - val_loss: 0.1052\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1097 - val_loss: 0.1091\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1084 - val_loss: 0.0997\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1083 - val_loss: 0.1027\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1093 - val_loss: 0.1111\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1086 - val_loss: 0.1003\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1067 - val_loss: 0.0993\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1093 - val_loss: 0.1109\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1083 - val_loss: 0.1043\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1089 - val_loss: 0.1032\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1070 - val_loss: 0.1004\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1055 - val_loss: 0.1006\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1092 - val_loss: 0.1162\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1053 - val_loss: 0.0975\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1094 - val_loss: 0.1019\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1082 - val_loss: 0.1032\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1058 - val_loss: 0.0972\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1064 - val_loss: 0.1237\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1067 - val_loss: 0.1005\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1046 - val_loss: 0.1002\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1062 - val_loss: 0.2050\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1073 - val_loss: 0.1050\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1058 - val_loss: 0.1163\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1053 - val_loss: 0.1262\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1061 - val_loss: 0.1099\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1049 - val_loss: 0.1067\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1088 - val_loss: 0.1136\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1084 - val_loss: 0.1272\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1058 - val_loss: 0.1187\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1070 - val_loss: 0.1073\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1048 - val_loss: 0.1374\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1034 - val_loss: 0.1106\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1050 - val_loss: 0.1004\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1052 - val_loss: 0.0954\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10518569499254227 , Final validation loss: 0.09541758894920349\n",
      "Training network # 5\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 1.1761 - val_loss: 0.4835\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.4365 - val_loss: 0.3368\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.3347 - val_loss: 0.2963\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.2699 - val_loss: 0.3162\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.2279 - val_loss: 0.1847\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.2135 - val_loss: 0.2507\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1988 - val_loss: 0.1714\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 646us/step - loss: 0.1855 - val_loss: 0.1789\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1799 - val_loss: 0.1520\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1825 - val_loss: 0.1546\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1722 - val_loss: 0.1965\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1657 - val_loss: 0.1595\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1656 - val_loss: 0.1986\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1612 - val_loss: 0.1652\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1622 - val_loss: 0.1392\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1524 - val_loss: 0.1365\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1552 - val_loss: 0.1446\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1554 - val_loss: 0.1848\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1509 - val_loss: 0.1428\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1474 - val_loss: 0.1352\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1487 - val_loss: 0.1511\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1474 - val_loss: 0.1370\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1449 - val_loss: 0.1328\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1469 - val_loss: 0.1567\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1447 - val_loss: 0.2185\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1444 - val_loss: 0.1517\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1454 - val_loss: 0.1392\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1406 - val_loss: 0.1706\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1425 - val_loss: 0.1709\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1390 - val_loss: 0.1305\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1370 - val_loss: 0.1361\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1407 - val_loss: 0.1416\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1394 - val_loss: 0.1319\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1360 - val_loss: 0.1467\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1386 - val_loss: 0.1309\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1360 - val_loss: 0.1190\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1352 - val_loss: 0.1427\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1384 - val_loss: 0.1332\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1359 - val_loss: 0.1618\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1338 - val_loss: 0.1351\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1332 - val_loss: 0.1401\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1344 - val_loss: 0.1301\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1480 - val_loss: 0.1473\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1341 - val_loss: 0.1309\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1357 - val_loss: 0.1441\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1342 - val_loss: 0.1612\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1324 - val_loss: 0.1468\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1301 - val_loss: 0.1274\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1319 - val_loss: 0.1230\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1289 - val_loss: 0.1385\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1295 - val_loss: 0.1305\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1317 - val_loss: 0.1316\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1326 - val_loss: 0.1557\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1263 - val_loss: 0.1242\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1273 - val_loss: 0.1291\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1308 - val_loss: 0.1272\n",
      "Training completed. Number of epochs: 56 , Final training loss: 0.1307966411113739 , Final validation loss: 0.12717978656291962\n",
      "Training network # 6\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0115 - val_loss: 0.4614\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.4677 - val_loss: 0.7266\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.3687 - val_loss: 0.3301\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2728 - val_loss: 0.2168\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.2243 - val_loss: 0.2324\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.2108 - val_loss: 0.1738\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1927 - val_loss: 0.2960\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1861 - val_loss: 0.1718\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1809 - val_loss: 0.1559\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1731 - val_loss: 0.1656\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1722 - val_loss: 0.1438\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1662 - val_loss: 0.1568\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1674 - val_loss: 0.1676\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1571 - val_loss: 0.1487\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1582 - val_loss: 0.1520\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1525 - val_loss: 0.1502\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1546 - val_loss: 0.1365\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1490 - val_loss: 0.1363\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1472 - val_loss: 0.1340\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1464 - val_loss: 0.1396\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1409 - val_loss: 0.1528\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1449 - val_loss: 0.1378\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1443 - val_loss: 0.1290\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1424 - val_loss: 0.1403\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1396 - val_loss: 0.1735\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1364 - val_loss: 0.1374\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1408 - val_loss: 0.1790\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1395 - val_loss: 0.1584\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1376 - val_loss: 0.1329\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1335 - val_loss: 0.1309\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1371 - val_loss: 0.1896\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1351 - val_loss: 0.1254\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1328 - val_loss: 0.1249\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1320 - val_loss: 0.1235\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1346 - val_loss: 0.1292\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1350 - val_loss: 0.1380\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1335 - val_loss: 0.1237\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1316 - val_loss: 0.1334\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1306 - val_loss: 0.1322\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1338 - val_loss: 0.1247\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1284 - val_loss: 0.1134\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1328 - val_loss: 0.1426\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1289 - val_loss: 0.1195\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1279 - val_loss: 0.1284\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1274 - val_loss: 0.1462\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1302 - val_loss: 0.1460\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1265 - val_loss: 0.1181\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1276 - val_loss: 0.1383\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1287 - val_loss: 0.1399\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1344 - val_loss: 0.1297\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1271 - val_loss: 0.1227\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1287 - val_loss: 0.1399\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1272 - val_loss: 0.1133\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1255 - val_loss: 0.1174\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1287 - val_loss: 0.1211\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1258 - val_loss: 0.1242\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1271 - val_loss: 0.1238\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1278 - val_loss: 0.1333\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1268 - val_loss: 0.1176\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1234 - val_loss: 0.1175\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1343 - val_loss: 0.1383\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1320 - val_loss: 0.1162\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1255 - val_loss: 0.1162\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1269 - val_loss: 0.1111\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1248 - val_loss: 0.1201\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1257 - val_loss: 0.1192\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1266 - val_loss: 0.1231\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1231 - val_loss: 0.1396\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1267 - val_loss: 0.1193\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1218 - val_loss: 0.1192\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1218 - val_loss: 0.1289\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1232 - val_loss: 0.1239\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1237 - val_loss: 0.1158\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1232 - val_loss: 0.1147\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1228 - val_loss: 0.1566\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1231 - val_loss: 0.1467\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1218 - val_loss: 0.1420\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1232 - val_loss: 0.1158\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1239 - val_loss: 0.1248\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1457 - val_loss: 0.4932\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1956 - val_loss: 0.1767\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1339 - val_loss: 0.1246\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 646us/step - loss: 0.1308 - val_loss: 0.2163\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1588 - val_loss: 0.2181\n",
      "Training completed. Number of epochs: 84 , Final training loss: 0.15877948701381683 , Final validation loss: 0.21807169914245605\n",
      "Training network # 7\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 1.3973 - val_loss: 0.4356\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.3991 - val_loss: 0.3170\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.2954 - val_loss: 0.2540\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.2532 - val_loss: 0.2576\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.2364 - val_loss: 0.2172\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.2145 - val_loss: 0.2248\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1950 - val_loss: 0.1753\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1835 - val_loss: 0.1623\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1741 - val_loss: 0.2115\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1679 - val_loss: 0.1397\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1629 - val_loss: 0.1532\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1542 - val_loss: 0.1575\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1536 - val_loss: 0.1429\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1490 - val_loss: 0.1326\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1470 - val_loss: 0.1837\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1429 - val_loss: 0.1297\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1422 - val_loss: 0.1233\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1386 - val_loss: 0.1440\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1354 - val_loss: 0.1426\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1391 - val_loss: 0.1452\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1330 - val_loss: 0.1280\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.1356 - val_loss: 0.1217\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1302 - val_loss: 0.1118\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1350 - val_loss: 0.1192\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1311 - val_loss: 0.1196\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1294 - val_loss: 0.1335\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1299 - val_loss: 0.1115\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1254 - val_loss: 0.1199\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1246 - val_loss: 0.1243\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1292 - val_loss: 0.1704\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1270 - val_loss: 0.1231\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1274 - val_loss: 0.1235\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1223 - val_loss: 0.1173\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1259 - val_loss: 0.1481\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1254 - val_loss: 0.1274\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1249 - val_loss: 0.1122\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1254 - val_loss: 0.1116\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1222 - val_loss: 0.1169\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1224 - val_loss: 0.1294\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1214 - val_loss: 0.1107\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1232 - val_loss: 0.1474\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1223 - val_loss: 0.1159\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1198 - val_loss: 0.1340\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1213 - val_loss: 0.1263\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1222 - val_loss: 0.1117\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1195 - val_loss: 0.1133\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1214 - val_loss: 0.1058\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1187 - val_loss: 0.1063\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1194 - val_loss: 0.1171\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1176 - val_loss: 0.1271\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1208 - val_loss: 0.1293\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1184 - val_loss: 0.1185\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1177 - val_loss: 0.1050\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1206 - val_loss: 0.1158\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1181 - val_loss: 0.1121\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1191 - val_loss: 0.1427\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1188 - val_loss: 0.1156\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1188 - val_loss: 0.1116\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1153 - val_loss: 0.1173\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1152 - val_loss: 0.1116\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1157 - val_loss: 0.1067\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1152 - val_loss: 0.1140\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1190 - val_loss: 0.1151\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1162 - val_loss: 0.1061\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1134 - val_loss: 0.1243\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1142 - val_loss: 0.1203\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1171 - val_loss: 0.1048\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1149 - val_loss: 0.1249\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1142 - val_loss: 0.1097\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1139 - val_loss: 0.1118\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1150 - val_loss: 0.1115\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1131 - val_loss: 0.1189\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1131 - val_loss: 0.1081\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1135 - val_loss: 0.1185\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1132 - val_loss: 0.1489\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1132 - val_loss: 0.1318\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1123 - val_loss: 0.1283\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1144 - val_loss: 0.1035\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1136 - val_loss: 0.1055\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1455 - val_loss: 0.1483\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1202 - val_loss: 0.1068\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1119 - val_loss: 0.1055\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1114 - val_loss: 0.0996\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1114 - val_loss: 0.1298\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1104 - val_loss: 0.1175\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1116 - val_loss: 0.1161\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1113 - val_loss: 0.1082\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1116 - val_loss: 0.0999\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1103 - val_loss: 0.1065\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1098 - val_loss: 0.1232\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1104 - val_loss: 0.1118\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1107 - val_loss: 0.1184\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1112 - val_loss: 0.0971\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1093 - val_loss: 0.1287\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1105 - val_loss: 0.1054\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1099 - val_loss: 0.1058\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1111 - val_loss: 0.1048\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1093 - val_loss: 0.1069\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1094 - val_loss: 0.1061\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1112 - val_loss: 0.1090\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11116860806941986 , Final validation loss: 0.10903486609458923\n",
      "Training network # 8\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 1.0537 - val_loss: 0.4214\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.4389 - val_loss: 0.2947\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.2916 - val_loss: 0.2275\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.2467 - val_loss: 0.2264\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.2305 - val_loss: 0.2079\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.2079 - val_loss: 0.2554\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1965 - val_loss: 0.1826\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1887 - val_loss: 0.1608\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1867 - val_loss: 0.1783\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1740 - val_loss: 0.1887\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1676 - val_loss: 0.1567\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1672 - val_loss: 0.1616\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1675 - val_loss: 0.1633\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1598 - val_loss: 0.1520\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1609 - val_loss: 0.1446\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1604 - val_loss: 0.1418\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1526 - val_loss: 0.1584\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1583 - val_loss: 0.1459\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1609 - val_loss: 0.1916\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1521 - val_loss: 0.1374\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1495 - val_loss: 0.1404\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1499 - val_loss: 0.1644\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1491 - val_loss: 0.1645\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1468 - val_loss: 0.1280\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1449 - val_loss: 0.1322\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1450 - val_loss: 0.1438\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1454 - val_loss: 0.1336\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1416 - val_loss: 0.1295\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1451 - val_loss: 0.1402\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1408 - val_loss: 0.1580\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1413 - val_loss: 0.1476\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1408 - val_loss: 0.1195\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1400 - val_loss: 0.1253\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1378 - val_loss: 0.1206\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1394 - val_loss: 0.1217\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1364 - val_loss: 0.1246\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1408 - val_loss: 0.1944\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1332 - val_loss: 0.1319\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1360 - val_loss: 0.1237\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1333 - val_loss: 0.1457\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1349 - val_loss: 0.1224\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1325 - val_loss: 0.1265\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1359 - val_loss: 0.1335\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1338 - val_loss: 0.1344\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1338 - val_loss: 0.1249\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1321 - val_loss: 0.2152\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1332 - val_loss: 0.1415\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1297 - val_loss: 0.1379\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1317 - val_loss: 0.1172\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1332 - val_loss: 0.1126\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1295 - val_loss: 0.1365\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1349 - val_loss: 0.1252\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1274 - val_loss: 0.1261\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1306 - val_loss: 0.1345\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1312 - val_loss: 0.1461\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1287 - val_loss: 0.1140\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1278 - val_loss: 0.1156\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1278 - val_loss: 0.1395\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1264 - val_loss: 0.1192\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1287 - val_loss: 0.1535\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1279 - val_loss: 0.1419\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1258 - val_loss: 0.1198\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1281 - val_loss: 0.1140\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1256 - val_loss: 0.1198\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1258 - val_loss: 0.1257\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1253 - val_loss: 0.1137\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1228 - val_loss: 0.1114\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1248 - val_loss: 0.1251\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 0.1238 - val_loss: 0.1183\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1254 - val_loss: 0.1367\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1233 - val_loss: 0.1091\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1226 - val_loss: 0.1310\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1240 - val_loss: 0.1202\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1222 - val_loss: 0.1219\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1217 - val_loss: 0.1104\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1218 - val_loss: 0.1090\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1195 - val_loss: 0.1123\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1194 - val_loss: 0.1068\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1187 - val_loss: 0.1070\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1184 - val_loss: 0.1174\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1183 - val_loss: 0.1249\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1230 - val_loss: 0.1137\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1164 - val_loss: 0.1110\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1160 - val_loss: 0.1177\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1186 - val_loss: 0.1114\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1169 - val_loss: 0.1216\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1164 - val_loss: 0.1283\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1187 - val_loss: 0.1155\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1156 - val_loss: 0.1088\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1150 - val_loss: 0.1088\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1164 - val_loss: 0.1281\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1159 - val_loss: 0.1135\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1135 - val_loss: 0.1402\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1145 - val_loss: 0.1052\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1128 - val_loss: 0.1050\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1145 - val_loss: 0.1372\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1164 - val_loss: 0.1068\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1130 - val_loss: 0.1054\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1142 - val_loss: 0.1063\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1158 - val_loss: 0.1085\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11582284420728683 , Final validation loss: 0.10851112008094788\n",
      "Training network # 9\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 1.2254 - val_loss: 0.4241\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.4010 - val_loss: 0.2897\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.2946 - val_loss: 0.2511\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.2609 - val_loss: 0.2663\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.2289 - val_loss: 0.1879\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.2181 - val_loss: 0.2256\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.2035 - val_loss: 0.1812\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1874 - val_loss: 0.1972\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1803 - val_loss: 0.1657\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1706 - val_loss: 0.1501\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1670 - val_loss: 0.1669\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1629 - val_loss: 0.1439\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1569 - val_loss: 0.1369\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1548 - val_loss: 0.1511\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1518 - val_loss: 0.1790\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.1506 - val_loss: 0.1605\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1445 - val_loss: 0.1320\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1461 - val_loss: 0.1509\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1411 - val_loss: 0.1466\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1417 - val_loss: 0.1391\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1402 - val_loss: 0.1304\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1375 - val_loss: 0.1453\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1372 - val_loss: 0.1186\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1386 - val_loss: 0.1202\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1339 - val_loss: 0.1289\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1322 - val_loss: 0.1342\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1367 - val_loss: 0.1259\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1294 - val_loss: 0.1189\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1352 - val_loss: 0.1222\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1283 - val_loss: 0.1133\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1301 - val_loss: 0.1122\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1249 - val_loss: 0.1303\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1235 - val_loss: 0.1107\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1259 - val_loss: 0.1451\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1248 - val_loss: 0.1235\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1212 - val_loss: 0.1342\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1206 - val_loss: 0.1116\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1221 - val_loss: 0.1062\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1203 - val_loss: 0.1085\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1185 - val_loss: 0.1326\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1197 - val_loss: 0.1286\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1193 - val_loss: 0.1196\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1172 - val_loss: 0.1198\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1191 - val_loss: 0.1106\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1160 - val_loss: 0.1143\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1165 - val_loss: 0.1087\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1148 - val_loss: 0.1180\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1178 - val_loss: 0.1085\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1132 - val_loss: 0.1054\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1175 - val_loss: 0.1472\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1161 - val_loss: 0.1105\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1212 - val_loss: 0.1754\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1152 - val_loss: 0.1582\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1144 - val_loss: 0.1093\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1154 - val_loss: 0.1114\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1154 - val_loss: 0.1045\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1131 - val_loss: 0.1129\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1136 - val_loss: 0.1043\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1139 - val_loss: 0.1085\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1125 - val_loss: 0.1227\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1118 - val_loss: 0.1050\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1132 - val_loss: 0.1062\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1106 - val_loss: 0.1104\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1114 - val_loss: 0.0992\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1119 - val_loss: 0.1148\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1108 - val_loss: 0.1050\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1112 - val_loss: 0.1065\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1117 - val_loss: 0.1075\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1097 - val_loss: 0.0974\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1109 - val_loss: 0.1046\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1124 - val_loss: 0.1041\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1068 - val_loss: 0.1018\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1119 - val_loss: 0.1058\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1093 - val_loss: 0.1101\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1098 - val_loss: 0.1067\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1102 - val_loss: 0.0996\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1087 - val_loss: 0.1625\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1089 - val_loss: 0.1002\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1083 - val_loss: 0.0999\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1105 - val_loss: 0.1304\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1098 - val_loss: 0.1057\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1087 - val_loss: 0.0986\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1088 - val_loss: 0.1020\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1093 - val_loss: 0.0983\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1085 - val_loss: 0.1113\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1068 - val_loss: 0.1082\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1073 - val_loss: 0.1015\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1082 - val_loss: 0.0999\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1112 - val_loss: 0.1191\n",
      "Training completed. Number of epochs: 89 , Final training loss: 0.1112128496170044 , Final validation loss: 0.11914011090993881\n",
      "Training network # 10\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 1.0515 - val_loss: 0.6705\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.4677 - val_loss: 0.3647\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.3562 - val_loss: 0.2872\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.2674 - val_loss: 0.2202\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.2413 - val_loss: 0.2086\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.2173 - val_loss: 0.2508\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.2117 - val_loss: 0.2735\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1911 - val_loss: 0.2882\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1831 - val_loss: 0.1822\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1795 - val_loss: 0.1559\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1753 - val_loss: 0.1503\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1664 - val_loss: 0.1599\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1641 - val_loss: 0.1454\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1606 - val_loss: 0.1397\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1562 - val_loss: 0.1412\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1510 - val_loss: 0.1369\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1539 - val_loss: 0.1422\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1469 - val_loss: 0.1921\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1506 - val_loss: 0.1329\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1461 - val_loss: 0.1371\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1458 - val_loss: 0.1386\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1416 - val_loss: 0.1938\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1407 - val_loss: 0.1375\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1421 - val_loss: 0.1359\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1404 - val_loss: 0.1271\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1391 - val_loss: 0.1233\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1380 - val_loss: 0.1388\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1384 - val_loss: 0.1414\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1333 - val_loss: 0.1235\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1373 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1333 - val_loss: 0.1222\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1388 - val_loss: 0.1197\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1341 - val_loss: 0.1596\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1314 - val_loss: 0.1366\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1335 - val_loss: 0.1241\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1317 - val_loss: 0.1258\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1333 - val_loss: 0.1307\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1346 - val_loss: 0.1391\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1315 - val_loss: 0.1229\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1331 - val_loss: 0.1243\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1331 - val_loss: 0.1397\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1309 - val_loss: 0.1305\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1301 - val_loss: 0.1404\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1301 - val_loss: 0.1323\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1312 - val_loss: 0.1139\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1293 - val_loss: 0.1251\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1309 - val_loss: 0.1675\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1304 - val_loss: 0.1448\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1330 - val_loss: 0.1331\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1293 - val_loss: 0.1238\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1303 - val_loss: 0.1704\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1297 - val_loss: 0.1190\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 554us/step - loss: 0.1274 - val_loss: 0.1161\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1282 - val_loss: 0.1368\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1281 - val_loss: 0.1178\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1305 - val_loss: 0.1323\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1261 - val_loss: 0.1218\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1267 - val_loss: 0.1251\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1269 - val_loss: 0.1310\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1255 - val_loss: 0.1520\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1272 - val_loss: 0.1292\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 641us/step - loss: 0.1286 - val_loss: 0.1219\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1272 - val_loss: 0.1139\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1294 - val_loss: 0.1174\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1285 - val_loss: 0.1253\n",
      "Training completed. Number of epochs: 65 , Final training loss: 0.1285390555858612 , Final validation loss: 0.12527833878993988\n",
      "Training network # 11\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 1.0688 - val_loss: 0.4434\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.4813 - val_loss: 0.4142\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.3664 - val_loss: 0.2906\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.2655 - val_loss: 0.3214\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.2349 - val_loss: 0.2008\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.2178 - val_loss: 0.2010\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1957 - val_loss: 0.1810\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1843 - val_loss: 0.1595\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1774 - val_loss: 0.1677\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1763 - val_loss: 0.1688\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1637 - val_loss: 0.1519\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1603 - val_loss: 0.1371\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1591 - val_loss: 0.1526\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1603 - val_loss: 0.1681\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1504 - val_loss: 0.1419\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1500 - val_loss: 0.1378\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1492 - val_loss: 0.1347\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1466 - val_loss: 0.1245\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1470 - val_loss: 0.1743\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1437 - val_loss: 0.1361\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1409 - val_loss: 0.1318\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1408 - val_loss: 0.1529\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1378 - val_loss: 0.1297\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1405 - val_loss: 0.1539\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1408 - val_loss: 0.1400\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1363 - val_loss: 0.1212\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1355 - val_loss: 0.1249\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1359 - val_loss: 0.1217\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1337 - val_loss: 0.1447\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1337 - val_loss: 0.1246\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1338 - val_loss: 0.1459\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1332 - val_loss: 0.1535\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1344 - val_loss: 0.1616\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1325 - val_loss: 0.1216\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1325 - val_loss: 0.1180\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1338 - val_loss: 0.1266\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1325 - val_loss: 0.1393\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1317 - val_loss: 0.1296\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1310 - val_loss: 0.1538\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1314 - val_loss: 0.1256\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1300 - val_loss: 0.1271\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1313 - val_loss: 0.1285\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1294 - val_loss: 0.1220\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1337 - val_loss: 0.1219\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1307 - val_loss: 0.1344\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1316 - val_loss: 0.1300\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1292 - val_loss: 0.1429\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1290 - val_loss: 0.1231\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1285 - val_loss: 0.1169\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1267 - val_loss: 0.1373\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1277 - val_loss: 0.1151\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1274 - val_loss: 0.1433\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1276 - val_loss: 0.1304\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1274 - val_loss: 0.1121\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1276 - val_loss: 0.1403\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1267 - val_loss: 0.1359\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1255 - val_loss: 0.1203\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1282 - val_loss: 0.1183\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1265 - val_loss: 0.1178\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1264 - val_loss: 0.1494\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1250 - val_loss: 0.1227\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1306 - val_loss: 0.1227\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1244 - val_loss: 0.1564\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1260 - val_loss: 0.1200\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1259 - val_loss: 0.1210\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1283 - val_loss: 0.1362\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1247 - val_loss: 0.1163\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1239 - val_loss: 0.1375\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1234 - val_loss: 0.1171\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1262 - val_loss: 0.1170\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1235 - val_loss: 0.1489\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1244 - val_loss: 0.1372\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1248 - val_loss: 0.1211\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1233 - val_loss: 0.1219\n",
      "Training completed. Number of epochs: 74 , Final training loss: 0.1233421042561531 , Final validation loss: 0.12193537503480911\n",
      "Training network # 12\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 1.4294 - val_loss: 0.4900\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.4245 - val_loss: 0.3028\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.2863 - val_loss: 0.2221\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.2438 - val_loss: 0.2637\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.2210 - val_loss: 0.1870\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2025 - val_loss: 0.1903\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1885 - val_loss: 0.1742\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1768 - val_loss: 0.1511\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1656 - val_loss: 0.1446\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1608 - val_loss: 0.1385\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1562 - val_loss: 0.1377\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1508 - val_loss: 0.1448\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1536 - val_loss: 0.1355\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1453 - val_loss: 0.1518\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1437 - val_loss: 0.1289\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1417 - val_loss: 0.1431\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1399 - val_loss: 0.1186\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1359 - val_loss: 0.1578\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1372 - val_loss: 0.1349\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1320 - val_loss: 0.1501\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1363 - val_loss: 0.1485\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1328 - val_loss: 0.1236\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1294 - val_loss: 0.1515\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1290 - val_loss: 0.1149\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1322 - val_loss: 0.1529\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1308 - val_loss: 0.1155\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1265 - val_loss: 0.1122\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1262 - val_loss: 0.1187\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1281 - val_loss: 0.1261\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1248 - val_loss: 0.1095\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1242 - val_loss: 0.1082\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1247 - val_loss: 0.1134\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1227 - val_loss: 0.1145\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1257 - val_loss: 0.1156\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1239 - val_loss: 0.1139\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1209 - val_loss: 0.1276\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1215 - val_loss: 0.1257\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1230 - val_loss: 0.1151\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1201 - val_loss: 0.1156\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1207 - val_loss: 0.1072\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1196 - val_loss: 0.1148\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1191 - val_loss: 0.1146\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1171 - val_loss: 0.1092\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1192 - val_loss: 0.1474\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1175 - val_loss: 0.1487\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1162 - val_loss: 0.1131\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1169 - val_loss: 0.1084\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1164 - val_loss: 0.1161\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1166 - val_loss: 0.1036\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1180 - val_loss: 0.1037\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1175 - val_loss: 0.1150\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1150 - val_loss: 0.1159\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1143 - val_loss: 0.1184\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1150 - val_loss: 0.1037\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1141 - val_loss: 0.1054\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1149 - val_loss: 0.1015\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1157 - val_loss: 0.1020\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1116 - val_loss: 0.1083\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1114 - val_loss: 0.1082\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1144 - val_loss: 0.1014\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1144 - val_loss: 0.1005\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1127 - val_loss: 0.1138\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1128 - val_loss: 0.1039\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1104 - val_loss: 0.1028\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1139 - val_loss: 0.1019\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1102 - val_loss: 0.1344\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1117 - val_loss: 0.1046\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 0.1099 - val_loss: 0.1048\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1572 - val_loss: 0.1101\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1091 - val_loss: 0.0960\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1091 - val_loss: 0.1145\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1112 - val_loss: 0.1012\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1087 - val_loss: 0.1022\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1107 - val_loss: 0.1326\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1083 - val_loss: 0.1062\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1101 - val_loss: 0.1159\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1081 - val_loss: 0.1030\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1069 - val_loss: 0.1320\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1090 - val_loss: 0.0980\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1074 - val_loss: 0.1043\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1072 - val_loss: 0.0982\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1091 - val_loss: 0.1065\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1080 - val_loss: 0.1034\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1093 - val_loss: 0.1061\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1092 - val_loss: 0.1019\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1055 - val_loss: 0.1024\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1093 - val_loss: 0.1108\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1062 - val_loss: 0.1039\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1056 - val_loss: 0.1025\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1050 - val_loss: 0.1019\n",
      "Training completed. Number of epochs: 90 , Final training loss: 0.1050075888633728 , Final validation loss: 0.10189179331064224\n",
      "Training network # 13\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 1.1289 - val_loss: 0.4182\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.3931 - val_loss: 0.3989\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.2959 - val_loss: 0.2218\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.2543 - val_loss: 0.2474\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.2382 - val_loss: 0.2809\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.2226 - val_loss: 0.2053\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.2109 - val_loss: 0.1831\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.2032 - val_loss: 0.1890\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1874 - val_loss: 0.1718\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1800 - val_loss: 0.1887\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1757 - val_loss: 0.1820\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1694 - val_loss: 0.1564\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1635 - val_loss: 0.1595\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1637 - val_loss: 0.2165\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1519 - val_loss: 0.1420\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1543 - val_loss: 0.1463\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1507 - val_loss: 0.1325\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1441 - val_loss: 0.1578\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1419 - val_loss: 0.1300\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1396 - val_loss: 0.1553\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1379 - val_loss: 0.1375\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1385 - val_loss: 0.1283\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1403 - val_loss: 0.1181\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1362 - val_loss: 0.1221\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1333 - val_loss: 0.1326\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.1327 - val_loss: 0.1294\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1313 - val_loss: 0.1208\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1303 - val_loss: 0.1208\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1310 - val_loss: 0.1172\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1277 - val_loss: 0.1341\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1293 - val_loss: 0.1421\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 538us/step - loss: 0.1251 - val_loss: 0.1165\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1266 - val_loss: 0.1177\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1248 - val_loss: 0.1112\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1264 - val_loss: 0.1110\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1237 - val_loss: 0.1260\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1227 - val_loss: 0.1121\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1228 - val_loss: 0.1084\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1228 - val_loss: 0.1207\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1188 - val_loss: 0.1269\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1204 - val_loss: 0.1093\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1188 - val_loss: 0.1184\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1207 - val_loss: 0.1211\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1168 - val_loss: 0.1199\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1194 - val_loss: 0.1118\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1205 - val_loss: 0.1092\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1146 - val_loss: 0.1303\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1175 - val_loss: 0.1013\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1165 - val_loss: 0.1203\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 0.1160 - val_loss: 0.1236\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1180 - val_loss: 0.1112\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1238 - val_loss: 0.1095\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 539us/step - loss: 0.1171 - val_loss: 0.1102\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1135 - val_loss: 0.1068\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1155 - val_loss: 0.1078\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1157 - val_loss: 0.1525\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1119 - val_loss: 0.1109\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1132 - val_loss: 0.0994\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1110 - val_loss: 0.1048\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1121 - val_loss: 0.1014\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1124 - val_loss: 0.1287\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1103 - val_loss: 0.1257\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1109 - val_loss: 0.1096\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1107 - val_loss: 0.1058\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1127 - val_loss: 0.1053\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1092 - val_loss: 0.0963\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 551us/step - loss: 0.1098 - val_loss: 0.1288\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1099 - val_loss: 0.1033\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1086 - val_loss: 0.1078\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1070 - val_loss: 0.1115\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1112 - val_loss: 0.1077\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.1085 - val_loss: 0.1308\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1076 - val_loss: 0.1018\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1108 - val_loss: 0.1067\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.1083 - val_loss: 0.1069\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1076 - val_loss: 0.1119\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1080 - val_loss: 0.1084\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1097 - val_loss: 0.1015\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1075 - val_loss: 0.1346\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1079 - val_loss: 0.1017\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1045 - val_loss: 0.0973\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1083 - val_loss: 0.1028\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1076 - val_loss: 0.1115\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1079 - val_loss: 0.0935\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1082 - val_loss: 0.0985\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1080 - val_loss: 0.1084\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1087 - val_loss: 0.1009\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1053 - val_loss: 0.1091\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1067 - val_loss: 0.1116\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1066 - val_loss: 0.1045\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1049 - val_loss: 0.1364\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1082 - val_loss: 0.1021\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1059 - val_loss: 0.0997\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1052 - val_loss: 0.1046\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1083 - val_loss: 0.0951\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1067 - val_loss: 0.0973\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1060 - val_loss: 0.0993\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1057 - val_loss: 0.1126\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1840 - val_loss: 0.1589\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1263 - val_loss: 0.1085\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.12629015743732452 , Final validation loss: 0.10853715986013412\n",
      "Training network # 14\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 1.2064 - val_loss: 0.4581\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 551us/step - loss: 0.3624 - val_loss: 0.2737\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.2939 - val_loss: 0.3622\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.2507 - val_loss: 0.2347\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 537us/step - loss: 0.2226 - val_loss: 0.1876\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.2133 - val_loss: 0.2212\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2016 - val_loss: 0.2051\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1833 - val_loss: 0.1637\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1766 - val_loss: 0.1468\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.1701 - val_loss: 0.1401\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1629 - val_loss: 0.1706\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 551us/step - loss: 0.1606 - val_loss: 0.1311\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1565 - val_loss: 0.1584\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1492 - val_loss: 0.1321\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1478 - val_loss: 0.1273\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1487 - val_loss: 0.1719\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1489 - val_loss: 0.1280\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1390 - val_loss: 0.1323\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1355 - val_loss: 0.1211\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1371 - val_loss: 0.1249\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1379 - val_loss: 0.1569\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1362 - val_loss: 0.1405\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.1329 - val_loss: 0.1259\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1330 - val_loss: 0.1188\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1279 - val_loss: 0.1168\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1300 - val_loss: 0.1163\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1330 - val_loss: 0.1142\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1286 - val_loss: 0.1176\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1310 - val_loss: 0.1131\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1264 - val_loss: 0.1220\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1256 - val_loss: 0.1265\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1271 - val_loss: 0.1268\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1231 - val_loss: 0.1052\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1220 - val_loss: 0.1305\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1230 - val_loss: 0.2113\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1233 - val_loss: 0.1092\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1206 - val_loss: 0.1402\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1216 - val_loss: 0.1093\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1187 - val_loss: 0.1443\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1195 - val_loss: 0.1095\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1174 - val_loss: 0.1135\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.1164 - val_loss: 0.1220\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1177 - val_loss: 0.1085\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1166 - val_loss: 0.1329\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 641us/step - loss: 0.1163 - val_loss: 0.1087\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1142 - val_loss: 0.1124\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1135 - val_loss: 0.1117\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1134 - val_loss: 0.1075\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1111 - val_loss: 0.1030\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1159 - val_loss: 0.1263\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1134 - val_loss: 0.1117\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1099 - val_loss: 0.1062\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1126 - val_loss: 0.1160\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1129 - val_loss: 0.1053\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1100 - val_loss: 0.1309\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1119 - val_loss: 0.1004\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1112 - val_loss: 0.1127\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1100 - val_loss: 0.1119\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1126 - val_loss: 0.0995\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1092 - val_loss: 0.1081\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1098 - val_loss: 0.1050\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1105 - val_loss: 0.1189\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.1101 - val_loss: 0.0981\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1125 - val_loss: 0.0998\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1081 - val_loss: 0.1133\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1096 - val_loss: 0.0992\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1079 - val_loss: 0.1093\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1095 - val_loss: 0.1029\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1098 - val_loss: 0.1022\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1096 - val_loss: 0.1340\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1080 - val_loss: 0.1091\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1075 - val_loss: 0.1053\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1077 - val_loss: 0.1078\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1074 - val_loss: 0.1039\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1086 - val_loss: 0.1057\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1060 - val_loss: 0.1103\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1087 - val_loss: 0.1075\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1094 - val_loss: 0.0985\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1070 - val_loss: 0.1018\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 639us/step - loss: 0.1047 - val_loss: 0.0997\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1060 - val_loss: 0.1040\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1077 - val_loss: 0.1001\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1060 - val_loss: 0.1040\n",
      "Training completed. Number of epochs: 83 , Final training loss: 0.10597242414951324 , Final validation loss: 0.10402503609657288\n",
      "Training network # 15\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 639us/step - loss: 2.3797 - val_loss: 0.4702\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.4375 - val_loss: 0.3292\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.3011 - val_loss: 0.2355\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.2546 - val_loss: 0.3379\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.2255 - val_loss: 0.1869\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.2080 - val_loss: 0.1734\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1883 - val_loss: 0.1624\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 0.1793 - val_loss: 0.1646\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1736 - val_loss: 0.1580\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1597 - val_loss: 0.1757\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1541 - val_loss: 0.1590\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 742us/step - loss: 0.1541 - val_loss: 0.1550\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1520 - val_loss: 0.1256\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.1457 - val_loss: 0.1600\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1430 - val_loss: 0.1286\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1404 - val_loss: 0.1454\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1370 - val_loss: 0.1263\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1348 - val_loss: 0.1396\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1308 - val_loss: 0.1221\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1335 - val_loss: 0.1254\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1303 - val_loss: 0.1432\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1299 - val_loss: 0.1297\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1273 - val_loss: 0.1117\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1261 - val_loss: 0.1177\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1244 - val_loss: 0.1530\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1242 - val_loss: 0.1202\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1213 - val_loss: 0.1154\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1233 - val_loss: 0.1113\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1199 - val_loss: 0.1185\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1219 - val_loss: 0.1520\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1254 - val_loss: 0.1163\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1186 - val_loss: 0.1104\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1187 - val_loss: 0.1119\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1190 - val_loss: 0.1003\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1187 - val_loss: 0.1419\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1184 - val_loss: 0.1047\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1160 - val_loss: 0.1105\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1177 - val_loss: 0.1049\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1160 - val_loss: 0.1127\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1144 - val_loss: 0.1100\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1187 - val_loss: 0.1087\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1146 - val_loss: 0.1508\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1133 - val_loss: 0.1022\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1146 - val_loss: 0.1229\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1140 - val_loss: 0.1098\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1138 - val_loss: 0.1511\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1136 - val_loss: 0.1087\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1131 - val_loss: 0.1133\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1131 - val_loss: 0.1720\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1113 - val_loss: 0.0999\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1133 - val_loss: 0.1108\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1109 - val_loss: 0.1019\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1119 - val_loss: 0.0980\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1124 - val_loss: 0.1024\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1094 - val_loss: 0.1070\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1096 - val_loss: 0.1189\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1103 - val_loss: 0.1106\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1093 - val_loss: 0.1019\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1097 - val_loss: 0.0997\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1098 - val_loss: 0.1679\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1095 - val_loss: 0.0977\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1085 - val_loss: 0.1075\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1075 - val_loss: 0.1328\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1086 - val_loss: 0.0974\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1086 - val_loss: 0.1010\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1076 - val_loss: 0.1161\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1092 - val_loss: 0.0963\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1070 - val_loss: 0.0942\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1066 - val_loss: 0.1061\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1054 - val_loss: 0.0997\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1065 - val_loss: 0.1027\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1055 - val_loss: 0.0988\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1079 - val_loss: 0.1078\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1055 - val_loss: 0.1008\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1079 - val_loss: 0.1071\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1059 - val_loss: 0.1184\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1068 - val_loss: 0.0975\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1044 - val_loss: 0.0971\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1046 - val_loss: 0.1081\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1069 - val_loss: 0.1018\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1052 - val_loss: 0.1472\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1031 - val_loss: 0.1094\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1043 - val_loss: 0.1052\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1064 - val_loss: 0.0953\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1051 - val_loss: 0.1128\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1041 - val_loss: 0.1145\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1047 - val_loss: 0.0967\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1064 - val_loss: 0.0970\n",
      "Training completed. Number of epochs: 88 , Final training loss: 0.10635892301797867 , Final validation loss: 0.0969747006893158\n",
      "Training network # 16\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 1.0772 - val_loss: 0.5049\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.4420 - val_loss: 0.3580\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 560us/step - loss: 0.3415 - val_loss: 0.2956\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.2752 - val_loss: 0.2393\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.2353 - val_loss: 0.1917\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.2160 - val_loss: 0.1941\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1993 - val_loss: 0.1640\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1867 - val_loss: 0.2081\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1774 - val_loss: 0.1881\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1721 - val_loss: 0.2115\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1674 - val_loss: 0.1515\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1642 - val_loss: 0.1563\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1597 - val_loss: 0.1483\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1553 - val_loss: 0.1489\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 541us/step - loss: 0.1597 - val_loss: 0.1498\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1528 - val_loss: 0.1336\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1498 - val_loss: 0.1453\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1504 - val_loss: 0.1340\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1478 - val_loss: 0.1507\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1455 - val_loss: 0.1507\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1489 - val_loss: 0.1324\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1447 - val_loss: 0.1279\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1444 - val_loss: 0.1240\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1389 - val_loss: 0.1399\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1418 - val_loss: 0.1297\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1459 - val_loss: 0.1342\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 542us/step - loss: 0.1424 - val_loss: 0.1321\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1388 - val_loss: 0.1243\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1389 - val_loss: 0.1448\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1373 - val_loss: 0.1396\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1382 - val_loss: 0.1366\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1350 - val_loss: 0.1307\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1364 - val_loss: 0.1549\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1343 - val_loss: 0.1270\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1364 - val_loss: 0.1537\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1328 - val_loss: 0.1373\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1334 - val_loss: 0.1389\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1324 - val_loss: 0.1298\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1306 - val_loss: 0.1208\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1314 - val_loss: 0.1243\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1325 - val_loss: 0.1367\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1321 - val_loss: 0.2670\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1291 - val_loss: 0.1365\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1309 - val_loss: 0.1197\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1296 - val_loss: 0.1296\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1283 - val_loss: 0.1505\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1305 - val_loss: 0.1319\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1292 - val_loss: 0.1249\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1339 - val_loss: 0.1365\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1285 - val_loss: 0.2156\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1288 - val_loss: 0.1196\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1266 - val_loss: 0.1352\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1303 - val_loss: 0.1186\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1253 - val_loss: 0.1190\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1264 - val_loss: 0.1250\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1266 - val_loss: 0.1196\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1279 - val_loss: 0.1164\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1291 - val_loss: 0.1343\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1252 - val_loss: 0.1238\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1250 - val_loss: 0.1188\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1545 - val_loss: 0.1478\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1340 - val_loss: 0.1238\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1248 - val_loss: 0.1516\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1253 - val_loss: 0.1204\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1240 - val_loss: 0.1463\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1256 - val_loss: 0.1220\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1234 - val_loss: 0.1158\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1224 - val_loss: 0.1398\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1378 - val_loss: 0.1203\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1268 - val_loss: 0.1185\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1265 - val_loss: 0.1122\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1283 - val_loss: 0.1210\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1257 - val_loss: 0.1158\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1237 - val_loss: 0.1262\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1217 - val_loss: 0.1182\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1256 - val_loss: 0.1303\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.1472 - val_loss: 0.1523\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1244 - val_loss: 0.1307\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2268 - val_loss: 0.1397\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1401 - val_loss: 0.1294\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1349 - val_loss: 0.1254\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1291 - val_loss: 0.1197\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1275 - val_loss: 0.1163\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1284 - val_loss: 0.1141\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1231 - val_loss: 0.1185\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1223 - val_loss: 0.1225\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1222 - val_loss: 0.1131\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1254 - val_loss: 0.1169\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1220 - val_loss: 0.1304\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.1258 - val_loss: 0.1255\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1394 - val_loss: 0.1497\n",
      "Training completed. Number of epochs: 91 , Final training loss: 0.13940590620040894 , Final validation loss: 0.1496604084968567\n",
      "Training network # 17\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 1.0672 - val_loss: 0.5764\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.4859 - val_loss: 0.3978\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.3606 - val_loss: 0.2679\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.2593 - val_loss: 0.2706\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.2294 - val_loss: 0.3561\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.2098 - val_loss: 0.1929\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1965 - val_loss: 0.1725\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1952 - val_loss: 0.1577\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1822 - val_loss: 0.1677\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1786 - val_loss: 0.1658\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1769 - val_loss: 0.2606\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1721 - val_loss: 0.1783\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1671 - val_loss: 0.2072\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1678 - val_loss: 0.1944\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1596 - val_loss: 0.1506\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1573 - val_loss: 0.2537\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1579 - val_loss: 0.1351\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1547 - val_loss: 0.1427\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1543 - val_loss: 0.1471\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1473 - val_loss: 0.1302\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1527 - val_loss: 0.1505\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1494 - val_loss: 0.1443\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1477 - val_loss: 0.1660\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1471 - val_loss: 0.1298\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1447 - val_loss: 0.1247\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1417 - val_loss: 0.1270\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1415 - val_loss: 0.1482\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1417 - val_loss: 0.1400\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1412 - val_loss: 0.1308\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1419 - val_loss: 0.1490\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1393 - val_loss: 0.1636\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1382 - val_loss: 0.1366\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1392 - val_loss: 0.1551\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1381 - val_loss: 0.1327\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1380 - val_loss: 0.1425\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.1342 - val_loss: 0.1224\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1353 - val_loss: 0.1253\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1367 - val_loss: 0.1278\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1330 - val_loss: 0.1342\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1334 - val_loss: 0.1318\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 641us/step - loss: 0.1340 - val_loss: 0.1203\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1313 - val_loss: 0.1257\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1350 - val_loss: 0.3010\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1312 - val_loss: 0.1314\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1351 - val_loss: 0.1233\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1295 - val_loss: 0.1242\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1299 - val_loss: 0.1385\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 552us/step - loss: 0.1305 - val_loss: 0.1472\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1309 - val_loss: 0.1166\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1299 - val_loss: 0.1269\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1293 - val_loss: 0.1455\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1315 - val_loss: 0.1200\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1286 - val_loss: 0.1147\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1301 - val_loss: 0.1310\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1275 - val_loss: 0.1179\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1268 - val_loss: 0.1191\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1261 - val_loss: 0.1449\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1289 - val_loss: 0.1234\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1268 - val_loss: 0.1156\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1277 - val_loss: 0.1220\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1262 - val_loss: 0.1209\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1256 - val_loss: 0.1465\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1244 - val_loss: 0.1242\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1261 - val_loss: 0.1341\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1250 - val_loss: 0.1246\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1248 - val_loss: 0.1279\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1257 - val_loss: 0.1335\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1227 - val_loss: 0.1145\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1243 - val_loss: 0.1292\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1202 - val_loss: 0.1174\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1238 - val_loss: 0.1408\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.1225 - val_loss: 0.1108\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.1232 - val_loss: 0.1235\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1220 - val_loss: 0.1114\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1207 - val_loss: 0.1107\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1220 - val_loss: 0.1140\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1207 - val_loss: 0.1170\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1224 - val_loss: 0.1093\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1207 - val_loss: 0.1104\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1182 - val_loss: 0.1226\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1200 - val_loss: 0.1177\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1207 - val_loss: 0.1430\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1195 - val_loss: 0.1091\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1199 - val_loss: 0.1115\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1189 - val_loss: 0.1171\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1170 - val_loss: 0.1142\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1176 - val_loss: 0.1088\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1148 - val_loss: 0.1106\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1165 - val_loss: 0.1367\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1166 - val_loss: 0.1212\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1156 - val_loss: 0.1309\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1135 - val_loss: 0.1257\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1163 - val_loss: 0.1043\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1136 - val_loss: 0.1046\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1129 - val_loss: 0.1085\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1137 - val_loss: 0.1045\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1823 - val_loss: 0.1227\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1212 - val_loss: 0.1140\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1148 - val_loss: 0.1120\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1128 - val_loss: 0.1083\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11279400438070297 , Final validation loss: 0.10826697945594788\n",
      "Training network # 18\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 1.0262 - val_loss: 0.4257\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.4662 - val_loss: 0.4827\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.3590 - val_loss: 0.2896\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.2810 - val_loss: 0.2271\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.2465 - val_loss: 0.2194\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.2253 - val_loss: 0.2331\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.2236 - val_loss: 0.3349\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1970 - val_loss: 0.1781\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1874 - val_loss: 0.2125\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1875 - val_loss: 0.1758\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1797 - val_loss: 0.1615\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1705 - val_loss: 0.1977\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1635 - val_loss: 0.1441\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1615 - val_loss: 0.1381\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1619 - val_loss: 0.1518\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1609 - val_loss: 0.1665\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1585 - val_loss: 0.1408\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1505 - val_loss: 0.1290\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1539 - val_loss: 0.1419\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1486 - val_loss: 0.1260\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1488 - val_loss: 0.1284\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1456 - val_loss: 0.1301\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1417 - val_loss: 0.1465\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1424 - val_loss: 0.1430\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1449 - val_loss: 0.1287\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1393 - val_loss: 0.1264\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1410 - val_loss: 0.1369\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1387 - val_loss: 0.1335\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1359 - val_loss: 0.1443\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1391 - val_loss: 0.1257\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1350 - val_loss: 0.1394\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1375 - val_loss: 0.1161\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 557us/step - loss: 0.1338 - val_loss: 0.1283\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1361 - val_loss: 0.1310\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1329 - val_loss: 0.1349\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1337 - val_loss: 0.1295\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1296 - val_loss: 0.1584\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1297 - val_loss: 0.1145\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1298 - val_loss: 0.1337\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1266 - val_loss: 0.1298\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1263 - val_loss: 0.1182\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1260 - val_loss: 0.1489\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1245 - val_loss: 0.1166\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1258 - val_loss: 0.1301\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1258 - val_loss: 0.1520\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1241 - val_loss: 0.1513\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1233 - val_loss: 0.1110\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1196 - val_loss: 0.1216\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1216 - val_loss: 0.1203\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1216 - val_loss: 0.1165\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1201 - val_loss: 0.1481\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1182 - val_loss: 0.1081\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1185 - val_loss: 0.1162\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1203 - val_loss: 0.1083\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1176 - val_loss: 0.1457\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1182 - val_loss: 0.1181\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1167 - val_loss: 0.1087\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1201 - val_loss: 0.1325\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1160 - val_loss: 0.1041\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1172 - val_loss: 0.1139\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1161 - val_loss: 0.1043\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1169 - val_loss: 0.1051\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1164 - val_loss: 0.1030\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1150 - val_loss: 0.1048\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1169 - val_loss: 0.1057\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1169 - val_loss: 0.1074\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1166 - val_loss: 0.1422\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1150 - val_loss: 0.1132\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1142 - val_loss: 0.1033\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1155 - val_loss: 0.1166\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1139 - val_loss: 0.1126\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1162 - val_loss: 0.1304\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1149 - val_loss: 0.1145\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1128 - val_loss: 0.1183\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1156 - val_loss: 0.1419\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1146 - val_loss: 0.1240\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1135 - val_loss: 0.1122\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1136 - val_loss: 0.1141\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1117 - val_loss: 0.1202\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1122 - val_loss: 0.1183\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1158 - val_loss: 0.1162\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1121 - val_loss: 0.1156\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1146 - val_loss: 0.1104\n",
      "Training completed. Number of epochs: 83 , Final training loss: 0.11460383236408234 , Final validation loss: 0.11038149148225784\n",
      "Training network # 19\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 714us/step - loss: 1.3510 - val_loss: 0.4021\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.3916 - val_loss: 0.2996\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.2777 - val_loss: 0.3113\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.2336 - val_loss: 0.2171\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.2174 - val_loss: 0.1900\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.2038 - val_loss: 0.3312\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1893 - val_loss: 0.1509\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1715 - val_loss: 0.2503\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1664 - val_loss: 0.1455\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1638 - val_loss: 0.1416\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1516 - val_loss: 0.1569\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1530 - val_loss: 0.1463\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1450 - val_loss: 0.1210\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1453 - val_loss: 0.1218\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1394 - val_loss: 0.1143\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1395 - val_loss: 0.1335\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1370 - val_loss: 0.1152\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1405 - val_loss: 0.1519\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1390 - val_loss: 0.1329\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1314 - val_loss: 0.1191\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1315 - val_loss: 0.1430\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1316 - val_loss: 0.1165\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1318 - val_loss: 0.1174\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1287 - val_loss: 0.1188\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1271 - val_loss: 0.1083\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1250 - val_loss: 0.1225\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1270 - val_loss: 0.1298\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1264 - val_loss: 0.1328\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1255 - val_loss: 0.1257\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1226 - val_loss: 0.1174\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1207 - val_loss: 0.1132\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1228 - val_loss: 0.1097\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1255 - val_loss: 0.1411\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1217 - val_loss: 0.1051\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1205 - val_loss: 0.1068\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1204 - val_loss: 0.1111\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1196 - val_loss: 0.1293\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1205 - val_loss: 0.1141\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1178 - val_loss: 0.1119\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1182 - val_loss: 0.1098\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1192 - val_loss: 0.1152\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1168 - val_loss: 0.1549\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1179 - val_loss: 0.1102\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1168 - val_loss: 0.1355\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1166 - val_loss: 0.1584\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1164 - val_loss: 0.1102\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1162 - val_loss: 0.1086\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1161 - val_loss: 0.1083\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1163 - val_loss: 0.1423\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1168 - val_loss: 0.1112\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1161 - val_loss: 0.1551\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1167 - val_loss: 0.1015\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1170 - val_loss: 0.1091\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1142 - val_loss: 0.1134\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1143 - val_loss: 0.1054\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1134 - val_loss: 0.1117\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1141 - val_loss: 0.1160\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1164 - val_loss: 0.1065\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1128 - val_loss: 0.1052\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1139 - val_loss: 0.1075\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1137 - val_loss: 0.1136\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1137 - val_loss: 0.1143\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1127 - val_loss: 0.1088\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1152 - val_loss: 0.1157\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1131 - val_loss: 0.1087\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1099 - val_loss: 0.1089\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1117 - val_loss: 0.1079\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1121 - val_loss: 0.1164\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1132 - val_loss: 0.1078\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1116 - val_loss: 0.0967\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1131 - val_loss: 0.1114\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1099 - val_loss: 0.0993\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1107 - val_loss: 0.1090\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1093 - val_loss: 0.1144\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1127 - val_loss: 0.1067\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1102 - val_loss: 0.1159\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1113 - val_loss: 0.1041\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1101 - val_loss: 0.1082\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1756 - val_loss: 0.1356\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1151 - val_loss: 0.1146\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1098 - val_loss: 0.1114\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1128 - val_loss: 0.1115\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1094 - val_loss: 0.1228\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1095 - val_loss: 0.1137\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1091 - val_loss: 0.1260\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1112 - val_loss: 0.1072\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1100 - val_loss: 0.1086\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1092 - val_loss: 0.1005\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1082 - val_loss: 0.1162\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1219 - val_loss: 0.2274\n",
      "Training completed. Number of epochs: 90 , Final training loss: 0.1218920350074768 , Final validation loss: 0.22735713422298431\n",
      "Training network # 20\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 639us/step - loss: 1.0228 - val_loss: 0.5466\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.4743 - val_loss: 0.3806\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.3722 - val_loss: 0.3767\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.2781 - val_loss: 0.4216\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.2518 - val_loss: 0.2300\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.2239 - val_loss: 0.1876\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.2037 - val_loss: 0.1827\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1960 - val_loss: 0.1919\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1855 - val_loss: 0.1810\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1781 - val_loss: 0.1620\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1731 - val_loss: 0.1692\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1710 - val_loss: 0.1522\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1650 - val_loss: 0.1522\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1600 - val_loss: 0.1520\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1580 - val_loss: 0.1690\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1534 - val_loss: 0.1610\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1505 - val_loss: 0.1531\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1484 - val_loss: 0.1447\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1508 - val_loss: 0.2629\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1492 - val_loss: 0.1256\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1441 - val_loss: 0.1537\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1405 - val_loss: 0.1335\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1411 - val_loss: 0.1214\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1409 - val_loss: 0.1288\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1385 - val_loss: 0.1418\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1395 - val_loss: 0.1252\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1380 - val_loss: 0.1600\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1393 - val_loss: 0.1215\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1344 - val_loss: 0.1255\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1340 - val_loss: 0.1255\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1338 - val_loss: 0.1240\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1325 - val_loss: 0.1266\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1346 - val_loss: 0.1511\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1334 - val_loss: 0.1281\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1338 - val_loss: 0.1246\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1330 - val_loss: 0.1255\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1294 - val_loss: 0.1449\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1318 - val_loss: 0.1219\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1284 - val_loss: 0.1148\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1312 - val_loss: 0.1435\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1304 - val_loss: 0.1381\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1306 - val_loss: 0.1230\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1316 - val_loss: 0.1347\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1282 - val_loss: 0.1144\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1301 - val_loss: 0.1258\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1283 - val_loss: 0.1246\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1282 - val_loss: 0.1250\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1287 - val_loss: 0.1204\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1256 - val_loss: 0.1241\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1305 - val_loss: 0.1258\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1267 - val_loss: 0.1251\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1286 - val_loss: 0.1163\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1267 - val_loss: 0.1180\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1253 - val_loss: 0.1145\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1261 - val_loss: 0.1321\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1255 - val_loss: 0.1191\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1248 - val_loss: 0.1180\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.1248 - val_loss: 0.1191\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1278 - val_loss: 0.1209\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1244 - val_loss: 0.1131\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1274 - val_loss: 0.1258\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 651us/step - loss: 0.1268 - val_loss: 0.1164\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1242 - val_loss: 0.1128\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1387 - val_loss: 0.1266\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1281 - val_loss: 0.1174\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1264 - val_loss: 0.1275\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1248 - val_loss: 0.1175\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1245 - val_loss: 0.1197\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.1260 - val_loss: 0.1217\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1257 - val_loss: 0.1287\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1232 - val_loss: 0.1198\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1249 - val_loss: 0.1218\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1270 - val_loss: 0.1303\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 0.1242 - val_loss: 0.1215\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1250 - val_loss: 0.1172\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1241 - val_loss: 0.1182\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1249 - val_loss: 0.1497\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1225 - val_loss: 0.1207\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1247 - val_loss: 0.1185\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1212 - val_loss: 0.1269\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1239 - val_loss: 0.1186\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1214 - val_loss: 0.1448\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1228 - val_loss: 0.1172\n",
      "Training completed. Number of epochs: 83 , Final training loss: 0.12284437566995621 , Final validation loss: 0.11715652793645859\n",
      "Training network # 21\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 1.0311 - val_loss: 0.4481\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.4694 - val_loss: 0.3812\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.3368 - val_loss: 0.2328\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.2551 - val_loss: 0.2212\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.2265 - val_loss: 0.1910\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.2095 - val_loss: 0.1891\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1972 - val_loss: 0.1708\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1796 - val_loss: 0.1581\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1740 - val_loss: 0.1587\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1661 - val_loss: 0.1435\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1621 - val_loss: 0.1441\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1573 - val_loss: 0.1406\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1538 - val_loss: 0.1361\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1522 - val_loss: 0.1408\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1476 - val_loss: 0.1400\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1459 - val_loss: 0.1257\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1454 - val_loss: 0.1374\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1444 - val_loss: 0.1282\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1407 - val_loss: 0.1293\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1419 - val_loss: 0.1250\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1407 - val_loss: 0.1276\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1371 - val_loss: 0.1270\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1345 - val_loss: 0.1258\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1375 - val_loss: 0.1809\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1349 - val_loss: 0.1354\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1346 - val_loss: 0.1744\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1348 - val_loss: 0.1282\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1326 - val_loss: 0.1184\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1327 - val_loss: 0.1642\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1320 - val_loss: 0.1366\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1312 - val_loss: 0.1386\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1291 - val_loss: 0.1511\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1298 - val_loss: 0.1200\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1311 - val_loss: 0.1211\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1294 - val_loss: 0.1175\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1270 - val_loss: 0.1442\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1283 - val_loss: 0.1244\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1289 - val_loss: 0.1242\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1264 - val_loss: 0.1301\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1236 - val_loss: 0.2074\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1267 - val_loss: 0.1164\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1283 - val_loss: 0.1632\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1259 - val_loss: 0.1128\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1259 - val_loss: 0.1179\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1246 - val_loss: 0.1156\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1238 - val_loss: 0.1380\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1217 - val_loss: 0.1155\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1248 - val_loss: 0.1526\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1240 - val_loss: 0.1187\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1232 - val_loss: 0.1356\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1216 - val_loss: 0.1210\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1212 - val_loss: 0.1109\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1207 - val_loss: 0.1630\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1228 - val_loss: 0.1329\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1208 - val_loss: 0.1302\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1227 - val_loss: 0.1188\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1187 - val_loss: 0.1332\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1196 - val_loss: 0.1096\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1187 - val_loss: 0.1224\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1189 - val_loss: 0.1304\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1190 - val_loss: 0.1130\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1193 - val_loss: 0.1164\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1187 - val_loss: 0.1126\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1166 - val_loss: 0.1049\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1168 - val_loss: 0.1147\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1203 - val_loss: 0.1249\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1179 - val_loss: 0.1152\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1164 - val_loss: 0.1107\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1169 - val_loss: 0.1154\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1164 - val_loss: 0.1064\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1144 - val_loss: 0.1121\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1192 - val_loss: 0.1162\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1165 - val_loss: 0.1181\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1147 - val_loss: 0.1086\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1164 - val_loss: 0.1100\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1156 - val_loss: 0.1714\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1150 - val_loss: 0.1015\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1144 - val_loss: 0.1116\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1144 - val_loss: 0.1108\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1170 - val_loss: 0.1158\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1139 - val_loss: 0.1148\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1147 - val_loss: 0.1082\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1153 - val_loss: 0.1114\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1147 - val_loss: 0.1240\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1139 - val_loss: 0.1141\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1169 - val_loss: 0.1048\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1131 - val_loss: 0.1138\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1125 - val_loss: 0.1209\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1144 - val_loss: 0.1100\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1138 - val_loss: 0.1311\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1113 - val_loss: 0.1050\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1123 - val_loss: 0.1112\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1149 - val_loss: 0.1401\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1138 - val_loss: 0.1184\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1121 - val_loss: 0.1080\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1117 - val_loss: 0.1027\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1125 - val_loss: 0.1127\n",
      "Training completed. Number of epochs: 97 , Final training loss: 0.11245014518499374 , Final validation loss: 0.11267862468957901\n",
      "Training network # 22\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 1.0868 - val_loss: 0.4392\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.4225 - val_loss: 0.3856\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.3268 - val_loss: 0.2692\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.2750 - val_loss: 0.3085\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.2527 - val_loss: 0.2294\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.2225 - val_loss: 0.1890\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.2079 - val_loss: 0.1840\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1935 - val_loss: 0.1665\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1886 - val_loss: 0.1693\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1794 - val_loss: 0.1677\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1651 - val_loss: 0.1499\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1667 - val_loss: 0.1433\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1651 - val_loss: 0.1542\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1596 - val_loss: 0.1892\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1591 - val_loss: 0.1502\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1541 - val_loss: 0.1377\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1510 - val_loss: 0.1648\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1536 - val_loss: 0.1562\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1498 - val_loss: 0.1401\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1493 - val_loss: 0.1576\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1441 - val_loss: 0.1388\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1475 - val_loss: 0.1374\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1459 - val_loss: 0.1312\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1400 - val_loss: 0.1510\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1433 - val_loss: 0.1371\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1407 - val_loss: 0.1304\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1377 - val_loss: 0.1412\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1354 - val_loss: 0.1218\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1360 - val_loss: 0.1235\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1366 - val_loss: 0.1710\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1351 - val_loss: 0.1487\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1349 - val_loss: 0.1393\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1317 - val_loss: 0.1204\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.1326 - val_loss: 0.1204\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1298 - val_loss: 0.1364\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1303 - val_loss: 0.1199\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1305 - val_loss: 0.1213\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 553us/step - loss: 0.1282 - val_loss: 0.1329\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1311 - val_loss: 0.1349\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1282 - val_loss: 0.1307\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1268 - val_loss: 0.1232\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1271 - val_loss: 0.1235\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1280 - val_loss: 0.1184\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1257 - val_loss: 0.1486\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1259 - val_loss: 0.1179\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1270 - val_loss: 0.1242\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1241 - val_loss: 0.1100\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1244 - val_loss: 0.1220\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1231 - val_loss: 0.1274\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1246 - val_loss: 0.1105\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.1217 - val_loss: 0.1158\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1236 - val_loss: 0.1394\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1248 - val_loss: 0.1144\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1215 - val_loss: 0.1173\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1223 - val_loss: 0.1148\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1202 - val_loss: 0.1088\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1204 - val_loss: 0.1346\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1230 - val_loss: 0.1167\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1207 - val_loss: 0.1203\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1193 - val_loss: 0.1161\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1203 - val_loss: 0.1222\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1175 - val_loss: 0.1077\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1181 - val_loss: 0.1101\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1210 - val_loss: 0.1200\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1167 - val_loss: 0.1375\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1166 - val_loss: 0.1191\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1209 - val_loss: 0.1096\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1181 - val_loss: 0.1211\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1159 - val_loss: 0.1163\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 558us/step - loss: 0.1168 - val_loss: 0.1958\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1168 - val_loss: 0.1308\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1176 - val_loss: 0.1141\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1157 - val_loss: 0.1032\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.1168 - val_loss: 0.1031\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1142 - val_loss: 0.1278\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1146 - val_loss: 0.1264\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1137 - val_loss: 0.1092\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1124 - val_loss: 0.1073\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1146 - val_loss: 0.1198\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1140 - val_loss: 0.1150\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1130 - val_loss: 0.1174\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1151 - val_loss: 0.1027\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1120 - val_loss: 0.1101\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1128 - val_loss: 0.1107\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1150 - val_loss: 0.1031\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1102 - val_loss: 0.1354\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1121 - val_loss: 0.1046\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1138 - val_loss: 0.1072\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1107 - val_loss: 0.1292\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1133 - val_loss: 0.1073\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1129 - val_loss: 0.1264\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1101 - val_loss: 0.1082\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1107 - val_loss: 0.1098\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1119 - val_loss: 0.1012\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1085 - val_loss: 0.1037\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1106 - val_loss: 0.1441\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1117 - val_loss: 0.1092\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1099 - val_loss: 0.1066\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1104 - val_loss: 0.1041\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1090 - val_loss: 0.1110\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10900919139385223 , Final validation loss: 0.11104021221399307\n",
      "Training network # 23\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 1.4471 - val_loss: 0.4350\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.3453 - val_loss: 0.2541\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.2731 - val_loss: 0.2380\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.2386 - val_loss: 0.2530\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.2073 - val_loss: 0.1973\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1972 - val_loss: 0.2345\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1723 - val_loss: 0.1558\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1715 - val_loss: 0.1394\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1643 - val_loss: 0.1368\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1579 - val_loss: 0.1407\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 666us/step - loss: 0.1563 - val_loss: 0.1961\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1550 - val_loss: 0.1442\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1466 - val_loss: 0.1331\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1456 - val_loss: 0.1367\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1432 - val_loss: 0.2795\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1398 - val_loss: 0.1439\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1369 - val_loss: 0.1186\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1370 - val_loss: 0.1416\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1348 - val_loss: 0.1251\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1328 - val_loss: 0.1660\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1300 - val_loss: 0.1312\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1277 - val_loss: 0.1336\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1283 - val_loss: 0.1244\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1236 - val_loss: 0.1204\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1258 - val_loss: 0.1097\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1217 - val_loss: 0.1190\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1247 - val_loss: 0.1279\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1206 - val_loss: 0.1304\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1194 - val_loss: 0.1042\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1246 - val_loss: 0.1370\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1180 - val_loss: 0.1124\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1164 - val_loss: 0.1304\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1180 - val_loss: 0.1081\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1158 - val_loss: 0.1508\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1166 - val_loss: 0.1156\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1150 - val_loss: 0.1290\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1156 - val_loss: 0.1271\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1192 - val_loss: 0.1126\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1156 - val_loss: 0.1414\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1161 - val_loss: 0.1074\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1145 - val_loss: 0.1046\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1139 - val_loss: 0.1187\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1148 - val_loss: 0.1027\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1142 - val_loss: 0.1228\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1132 - val_loss: 0.1518\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1113 - val_loss: 0.1083\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1105 - val_loss: 0.0982\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1114 - val_loss: 0.1286\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1144 - val_loss: 0.0988\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1109 - val_loss: 0.1510\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1092 - val_loss: 0.1179\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1110 - val_loss: 0.0974\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1109 - val_loss: 0.1186\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1117 - val_loss: 0.1203\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1100 - val_loss: 0.1053\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1095 - val_loss: 0.1084\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1099 - val_loss: 0.1204\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1091 - val_loss: 0.1001\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1096 - val_loss: 0.1126\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1116 - val_loss: 0.1174\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1117 - val_loss: 0.1067\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1087 - val_loss: 0.1088\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1078 - val_loss: 0.0988\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1084 - val_loss: 0.0967\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1090 - val_loss: 0.1054\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.1094 - val_loss: 0.1039\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.1076 - val_loss: 0.1088\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1067 - val_loss: 0.1076\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1103 - val_loss: 0.1074\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1081 - val_loss: 0.1043\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1089 - val_loss: 0.1223\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1084 - val_loss: 0.1173\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1083 - val_loss: 0.1340\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1078 - val_loss: 0.1319\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1076 - val_loss: 0.1009\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1079 - val_loss: 0.1025\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1071 - val_loss: 0.1099\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1071 - val_loss: 0.1009\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1066 - val_loss: 0.1025\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1117 - val_loss: 0.1022\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1060 - val_loss: 0.1001\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1076 - val_loss: 0.0966\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1156 - val_loss: 0.0972\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1056 - val_loss: 0.1033\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1072 - val_loss: 0.1013\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1062 - val_loss: 0.1093\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1089 - val_loss: 0.0971\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1077 - val_loss: 0.1039\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1047 - val_loss: 0.1065\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1072 - val_loss: 0.1091\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1066 - val_loss: 0.1005\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1067 - val_loss: 0.1125\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1069 - val_loss: 0.1193\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1041 - val_loss: 0.1072\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1057 - val_loss: 0.1254\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1055 - val_loss: 0.1100\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1045 - val_loss: 0.1132\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1056 - val_loss: 0.1188\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1036 - val_loss: 0.1009\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10361605137586594 , Final validation loss: 0.10089477896690369\n",
      "Training network # 24\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 3.6755 - val_loss: 3.5338\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 3.6844 - val_loss: 3.4886\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 1.7365 - val_loss: 0.3813\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.2907 - val_loss: 0.6165\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.2338 - val_loss: 0.2229\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.2023 - val_loss: 0.1708\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1908 - val_loss: 0.1660\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1756 - val_loss: 0.1616\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1688 - val_loss: 0.1590\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1622 - val_loss: 0.1695\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1553 - val_loss: 0.1352\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1515 - val_loss: 0.1697\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1529 - val_loss: 0.1413\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1470 - val_loss: 0.2207\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1434 - val_loss: 0.1220\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1405 - val_loss: 0.1383\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1404 - val_loss: 0.1226\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1404 - val_loss: 0.1264\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1365 - val_loss: 0.1525\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1319 - val_loss: 0.1263\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1338 - val_loss: 0.1500\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1302 - val_loss: 0.1196\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1288 - val_loss: 0.1404\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1281 - val_loss: 0.1174\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1287 - val_loss: 0.1188\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1238 - val_loss: 0.1153\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1260 - val_loss: 0.1142\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1242 - val_loss: 0.1160\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1251 - val_loss: 0.1204\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1260 - val_loss: 0.1160\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1221 - val_loss: 0.1435\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1222 - val_loss: 0.1156\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1220 - val_loss: 0.1113\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1190 - val_loss: 0.1237\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1207 - val_loss: 0.1121\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1198 - val_loss: 0.1116\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1212 - val_loss: 0.1057\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1210 - val_loss: 0.1101\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1168 - val_loss: 0.1046\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1194 - val_loss: 0.1093\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1159 - val_loss: 0.1339\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1180 - val_loss: 0.1329\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1181 - val_loss: 0.1329\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1180 - val_loss: 0.1305\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1143 - val_loss: 0.1012\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1163 - val_loss: 0.1122\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1173 - val_loss: 0.1326\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1170 - val_loss: 0.1138\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1147 - val_loss: 0.1277\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1151 - val_loss: 0.1093\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1146 - val_loss: 0.1075\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1127 - val_loss: 0.1332\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1147 - val_loss: 0.1092\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1126 - val_loss: 0.1151\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1137 - val_loss: 0.1243\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1151 - val_loss: 0.1055\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1104 - val_loss: 0.1052\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1155 - val_loss: 0.1380\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1145 - val_loss: 0.1076\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1110 - val_loss: 0.1171\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1147 - val_loss: 0.1020\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1140 - val_loss: 0.1089\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1118 - val_loss: 0.1115\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1136 - val_loss: 0.1102\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1101 - val_loss: 0.1059\n",
      "Training completed. Number of epochs: 65 , Final training loss: 0.11008167266845703 , Final validation loss: 0.10591217130422592\n",
      "Training network # 25\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 3.6835 - val_loss: 3.7486\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 3.6341 - val_loss: 3.4865\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 2.2954 - val_loss: 0.4010\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.3287 - val_loss: 0.2421\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.2465 - val_loss: 0.2222\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.2194 - val_loss: 0.2654\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.2059 - val_loss: 0.1801\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1951 - val_loss: 0.1647\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1869 - val_loss: 0.1919\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1719 - val_loss: 0.1528\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1666 - val_loss: 0.2479\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1572 - val_loss: 0.1374\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1520 - val_loss: 0.2534\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1450 - val_loss: 0.1356\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1485 - val_loss: 0.1375\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1392 - val_loss: 0.1175\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1392 - val_loss: 0.1273\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1382 - val_loss: 0.1169\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1362 - val_loss: 0.1656\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 0.1330 - val_loss: 0.1233\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1323 - val_loss: 0.1119\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1267 - val_loss: 0.1289\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1289 - val_loss: 0.1142\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1281 - val_loss: 0.1237\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1267 - val_loss: 0.1294\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1273 - val_loss: 0.1117\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1239 - val_loss: 0.1098\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.1233 - val_loss: 0.1192\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1241 - val_loss: 0.1256\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1228 - val_loss: 0.1086\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1202 - val_loss: 0.1131\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1235 - val_loss: 0.1208\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1193 - val_loss: 0.1057\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1180 - val_loss: 0.1177\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1195 - val_loss: 0.1088\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1190 - val_loss: 0.1102\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1171 - val_loss: 0.1106\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1178 - val_loss: 0.1155\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1184 - val_loss: 0.1134\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1149 - val_loss: 0.1034\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1151 - val_loss: 0.1107\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1149 - val_loss: 0.1097\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1156 - val_loss: 0.1341\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1148 - val_loss: 0.0994\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1156 - val_loss: 0.1383\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1116 - val_loss: 0.1148\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1151 - val_loss: 0.1067\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1145 - val_loss: 0.1495\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1164 - val_loss: 0.1456\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1117 - val_loss: 0.1330\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1119 - val_loss: 0.1126\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.1112 - val_loss: 0.1065\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1137 - val_loss: 0.1008\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 0.1131 - val_loss: 0.1677\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1133 - val_loss: 0.1020\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1097 - val_loss: 0.1038\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1099 - val_loss: 0.1337\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1098 - val_loss: 0.1103\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1117 - val_loss: 0.1028\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1118 - val_loss: 0.0993\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1084 - val_loss: 0.0966\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1091 - val_loss: 0.1083\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1110 - val_loss: 0.1051\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1085 - val_loss: 0.0979\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1084 - val_loss: 0.1060\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1091 - val_loss: 0.0987\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1083 - val_loss: 0.1324\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1100 - val_loss: 0.1029\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1085 - val_loss: 0.1005\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1055 - val_loss: 0.0992\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1067 - val_loss: 0.1086\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1074 - val_loss: 0.1004\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1070 - val_loss: 0.1034\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1080 - val_loss: 0.1130\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1068 - val_loss: 0.0972\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1069 - val_loss: 0.1044\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1055 - val_loss: 0.1068\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1204 - val_loss: 0.1083\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1131 - val_loss: 0.1074\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1066 - val_loss: 0.1349\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.1072 - val_loss: 0.0951\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1071 - val_loss: 0.1194\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1065 - val_loss: 0.1022\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 655us/step - loss: 0.1043 - val_loss: 0.0961\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1073 - val_loss: 0.1230\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1056 - val_loss: 0.1027\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1037 - val_loss: 0.1194\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.1073 - val_loss: 0.1000\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 660us/step - loss: 0.2558 - val_loss: 0.1696\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1185 - val_loss: 0.1089\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1076 - val_loss: 0.0997\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1040 - val_loss: 0.0995\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1033 - val_loss: 0.1072\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1055 - val_loss: 0.1020\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 0.1055 - val_loss: 0.0961\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1051 - val_loss: 0.1072\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1062 - val_loss: 0.0979\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1049 - val_loss: 0.0979\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1043 - val_loss: 0.0988\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1049 - val_loss: 0.1023\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10488145053386688 , Final validation loss: 0.10234624147415161\n",
      "Training network # 26\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 641us/step - loss: 3.7134 - val_loss: 3.6782\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 3.6204 - val_loss: 1.6597\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 0.5169 - val_loss: 0.3391\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.3049 - val_loss: 0.2203\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.2373 - val_loss: 0.2036\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.2127 - val_loss: 0.2086\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1956 - val_loss: 0.1775\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1859 - val_loss: 0.1622\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1691 - val_loss: 0.1423\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1592 - val_loss: 0.1380\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1539 - val_loss: 0.1348\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1502 - val_loss: 0.1254\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1429 - val_loss: 0.1370\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1409 - val_loss: 0.1274\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1356 - val_loss: 0.1258\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1318 - val_loss: 0.1193\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1268 - val_loss: 0.1181\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1294 - val_loss: 0.1139\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1290 - val_loss: 0.1327\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1225 - val_loss: 0.1218\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1191 - val_loss: 0.1068\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1223 - val_loss: 0.1106\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1170 - val_loss: 0.2073\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1175 - val_loss: 0.1659\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1170 - val_loss: 0.1206\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1170 - val_loss: 0.1518\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1164 - val_loss: 0.1168\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.1154 - val_loss: 0.1012\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1136 - val_loss: 0.1130\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1128 - val_loss: 0.1050\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1119 - val_loss: 0.1027\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 651us/step - loss: 0.1122 - val_loss: 0.1077\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 0.1151 - val_loss: 0.1020\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 655us/step - loss: 0.1094 - val_loss: 0.0995\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.1114 - val_loss: 0.0965\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1108 - val_loss: 0.1024\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1096 - val_loss: 0.0983\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1095 - val_loss: 0.1234\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1096 - val_loss: 0.1196\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1088 - val_loss: 0.1026\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1069 - val_loss: 0.1158\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 0.1108 - val_loss: 0.0971\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1072 - val_loss: 0.0991\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.1097 - val_loss: 0.1044\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1080 - val_loss: 0.1226\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1065 - val_loss: 0.0965\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 0.1077 - val_loss: 0.1206\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1068 - val_loss: 0.1214\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1083 - val_loss: 0.1062\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1067 - val_loss: 0.1756\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1079 - val_loss: 0.0941\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1047 - val_loss: 0.1079\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1055 - val_loss: 0.0983\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1066 - val_loss: 0.0939\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1055 - val_loss: 0.1294\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1060 - val_loss: 0.1076\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1048 - val_loss: 0.0978\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1045 - val_loss: 0.1055\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1044 - val_loss: 0.0975\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1038 - val_loss: 0.0963\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1051 - val_loss: 0.1011\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1030 - val_loss: 0.1073\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1027 - val_loss: 0.0992\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1027 - val_loss: 0.1003\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1042 - val_loss: 0.1359\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 641us/step - loss: 0.1047 - val_loss: 0.1158\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.1088 - val_loss: 0.1029\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1022 - val_loss: 0.0947\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1025 - val_loss: 0.0961\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1008 - val_loss: 0.1065\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1040 - val_loss: 0.1100\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1028 - val_loss: 0.1000\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1000 - val_loss: 0.1020\n",
      "Training completed. Number of epochs: 74 , Final training loss: 0.0999593511223793 , Final validation loss: 0.10203211009502411\n",
      "Training network # 27\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 1.0844 - val_loss: 0.4639\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.4242 - val_loss: 0.3405\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.2928 - val_loss: 0.2236\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.2402 - val_loss: 0.2051\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.2245 - val_loss: 0.2127\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.2030 - val_loss: 0.2631\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.2011 - val_loss: 0.1972\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1900 - val_loss: 0.1601\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1900 - val_loss: 0.3042\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1763 - val_loss: 0.1726\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1718 - val_loss: 0.1544\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1704 - val_loss: 0.1440\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1655 - val_loss: 0.1727\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1618 - val_loss: 0.1391\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1590 - val_loss: 0.1440\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1593 - val_loss: 0.1407\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1506 - val_loss: 0.1532\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1498 - val_loss: 0.1341\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1514 - val_loss: 0.1312\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1463 - val_loss: 0.1366\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1464 - val_loss: 0.1386\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1451 - val_loss: 0.1189\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1420 - val_loss: 0.1266\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1402 - val_loss: 0.2021\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1446 - val_loss: 0.1248\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1385 - val_loss: 0.1221\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1379 - val_loss: 0.1217\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1326 - val_loss: 0.1242\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1364 - val_loss: 0.1185\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1398 - val_loss: 0.1282\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.1329 - val_loss: 0.1217\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1318 - val_loss: 0.1288\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.1291 - val_loss: 0.1234\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1351 - val_loss: 0.1246\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 0.1295 - val_loss: 0.1237\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1333 - val_loss: 0.1350\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1284 - val_loss: 0.1251\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1286 - val_loss: 0.1114\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1255 - val_loss: 0.1169\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1287 - val_loss: 0.1410\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1234 - val_loss: 0.1208\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1253 - val_loss: 0.1200\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1249 - val_loss: 0.1152\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1253 - val_loss: 0.1251\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.1223 - val_loss: 0.1165\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1232 - val_loss: 0.1806\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 645us/step - loss: 0.1225 - val_loss: 0.1156\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 658us/step - loss: 0.1227 - val_loss: 0.1405\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1205 - val_loss: 0.1126\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1203 - val_loss: 0.1209\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1197 - val_loss: 0.1123\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1192 - val_loss: 0.1105\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1192 - val_loss: 0.1100\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1190 - val_loss: 0.1045\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1149 - val_loss: 0.1089\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1179 - val_loss: 0.1067\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1182 - val_loss: 0.1272\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1169 - val_loss: 0.1195\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1187 - val_loss: 0.1349\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.1167 - val_loss: 0.1049\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1174 - val_loss: 0.1467\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1148 - val_loss: 0.1026\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1156 - val_loss: 0.1142\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1146 - val_loss: 0.1169\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.1144 - val_loss: 0.1036\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1142 - val_loss: 0.1323\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1138 - val_loss: 0.1435\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1144 - val_loss: 0.1933\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1138 - val_loss: 0.1271\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.1250 - val_loss: 0.1055\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1163 - val_loss: 0.1406\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1135 - val_loss: 0.1102\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1120 - val_loss: 0.1162\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1135 - val_loss: 0.1074\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1130 - val_loss: 0.1085\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1122 - val_loss: 0.1060\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1132 - val_loss: 0.1301\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1129 - val_loss: 0.1034\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1097 - val_loss: 0.1213\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1124 - val_loss: 0.1047\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1113 - val_loss: 0.0955\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1117 - val_loss: 0.1039\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1121 - val_loss: 0.1039\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 547us/step - loss: 0.1095 - val_loss: 0.1230\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1105 - val_loss: 0.1114\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1085 - val_loss: 0.0983\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1090 - val_loss: 0.1266\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1086 - val_loss: 0.1112\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1084 - val_loss: 0.1006\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1085 - val_loss: 0.1103\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1088 - val_loss: 0.0980\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1095 - val_loss: 0.1327\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1094 - val_loss: 0.1022\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1074 - val_loss: 0.1048\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1069 - val_loss: 0.0978\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1090 - val_loss: 0.1054\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1081 - val_loss: 0.1126\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1068 - val_loss: 0.1353\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.1066 - val_loss: 0.1041\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1114 - val_loss: 0.1236\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11144214868545532 , Final validation loss: 0.12356053292751312\n",
      "Training network # 28\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.9966 - val_loss: 0.4562\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.4893 - val_loss: 0.4200\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 648us/step - loss: 0.4228 - val_loss: 0.3533\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.3110 - val_loss: 0.2438\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.2449 - val_loss: 0.2225\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.2129 - val_loss: 0.1901\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.2004 - val_loss: 0.1806\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1885 - val_loss: 0.2082\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1803 - val_loss: 0.1662\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1781 - val_loss: 0.1786\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1661 - val_loss: 0.1518\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1651 - val_loss: 0.1638\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1666 - val_loss: 0.1580\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1585 - val_loss: 0.1465\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1573 - val_loss: 0.1422\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1552 - val_loss: 0.1387\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1551 - val_loss: 0.1485\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1476 - val_loss: 0.1495\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1526 - val_loss: 0.1355\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1475 - val_loss: 0.1726\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1489 - val_loss: 0.1491\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1466 - val_loss: 0.1409\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1487 - val_loss: 0.1575\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1445 - val_loss: 0.1337\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1407 - val_loss: 0.1480\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1414 - val_loss: 0.1305\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1424 - val_loss: 0.2102\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1411 - val_loss: 0.1268\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1402 - val_loss: 0.1284\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1383 - val_loss: 0.1761\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1402 - val_loss: 0.2119\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1400 - val_loss: 0.1513\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1367 - val_loss: 0.1327\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1368 - val_loss: 0.1282\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1392 - val_loss: 0.1276\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1361 - val_loss: 0.1418\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1368 - val_loss: 0.1264\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1378 - val_loss: 0.1192\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1371 - val_loss: 0.1239\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1360 - val_loss: 0.1335\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1365 - val_loss: 0.1568\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1370 - val_loss: 0.1328\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1344 - val_loss: 0.1571\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1369 - val_loss: 0.1391\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1329 - val_loss: 0.1326\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1330 - val_loss: 0.1174\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1321 - val_loss: 0.1353\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1330 - val_loss: 0.1257\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1323 - val_loss: 0.1376\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1326 - val_loss: 0.1263\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1325 - val_loss: 0.1271\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1303 - val_loss: 0.1147\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1309 - val_loss: 0.1473\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1320 - val_loss: 0.1229\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1320 - val_loss: 0.1177\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1307 - val_loss: 0.1308\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1282 - val_loss: 0.1508\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1310 - val_loss: 0.1383\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1270 - val_loss: 0.1140\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1302 - val_loss: 0.1738\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1296 - val_loss: 0.1165\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1282 - val_loss: 0.1184\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1281 - val_loss: 0.1208\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1275 - val_loss: 0.1216\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1280 - val_loss: 0.1149\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1272 - val_loss: 0.1296\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1265 - val_loss: 0.1284\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1284 - val_loss: 0.1181\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1281 - val_loss: 0.1286\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1250 - val_loss: 0.1334\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1266 - val_loss: 0.1182\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1257 - val_loss: 0.1245\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1252 - val_loss: 0.1174\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1256 - val_loss: 0.1191\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1265 - val_loss: 0.1221\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1248 - val_loss: 0.1322\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1245 - val_loss: 0.1323\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1235 - val_loss: 0.1346\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1232 - val_loss: 0.1187\n",
      "Training completed. Number of epochs: 79 , Final training loss: 0.12315866351127625 , Final validation loss: 0.11865334212779999\n",
      "Training network # 29\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 1.0964 - val_loss: 0.4269\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.4740 - val_loss: 0.3831\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.3441 - val_loss: 0.3269\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.2587 - val_loss: 0.2060\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.2225 - val_loss: 0.2017\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.2143 - val_loss: 0.3113\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1993 - val_loss: 0.3281\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1918 - val_loss: 0.2310\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1821 - val_loss: 0.1533\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1693 - val_loss: 0.1471\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 660us/step - loss: 0.1702 - val_loss: 0.1571\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.1650 - val_loss: 0.2006\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1636 - val_loss: 0.1566\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1589 - val_loss: 0.1557\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1552 - val_loss: 0.1361\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1593 - val_loss: 0.1439\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1496 - val_loss: 0.1407\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1523 - val_loss: 0.1325\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1488 - val_loss: 0.1543\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1487 - val_loss: 0.1694\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1476 - val_loss: 0.1509\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1443 - val_loss: 0.1347\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.1500 - val_loss: 0.1336\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1416 - val_loss: 0.1288\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1426 - val_loss: 0.1346\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 0.1418 - val_loss: 0.1864\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1394 - val_loss: 0.1447\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 658us/step - loss: 0.1422 - val_loss: 0.1423\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1426 - val_loss: 0.1362\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1390 - val_loss: 0.1289\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1382 - val_loss: 0.1258\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.1362 - val_loss: 0.1338\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1367 - val_loss: 0.1357\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1329 - val_loss: 0.1281\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1353 - val_loss: 0.1352\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1319 - val_loss: 0.1238\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1344 - val_loss: 0.1164\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1289 - val_loss: 0.1207\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1297 - val_loss: 0.1256\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1297 - val_loss: 0.1318\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1260 - val_loss: 0.1298\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1266 - val_loss: 0.1229\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1258 - val_loss: 0.1166\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1249 - val_loss: 0.1124\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1300 - val_loss: 0.1258\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 660us/step - loss: 0.1247 - val_loss: 0.1203\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1250 - val_loss: 0.1174\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1228 - val_loss: 0.1427\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1213 - val_loss: 0.1134\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1216 - val_loss: 0.1097\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1201 - val_loss: 0.1131\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1217 - val_loss: 0.1122\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.1229 - val_loss: 0.1237\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1187 - val_loss: 0.1037\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 657us/step - loss: 0.1199 - val_loss: 0.1060\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1175 - val_loss: 0.1125\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1175 - val_loss: 0.1158\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.1199 - val_loss: 0.1387\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1161 - val_loss: 0.1221\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1166 - val_loss: 0.1082\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1161 - val_loss: 0.1249\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1165 - val_loss: 0.1220\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1193 - val_loss: 0.1147\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1163 - val_loss: 0.1083\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1129 - val_loss: 0.1165\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1164 - val_loss: 0.1170\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1164 - val_loss: 0.1185\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1178 - val_loss: 0.1222\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1129 - val_loss: 0.1125\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1174 - val_loss: 0.1067\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1145 - val_loss: 0.1098\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1134 - val_loss: 0.1280\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1166 - val_loss: 0.1035\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1122 - val_loss: 0.1144\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1162 - val_loss: 0.1178\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1119 - val_loss: 0.1378\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1146 - val_loss: 0.1037\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1128 - val_loss: 0.1061\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1130 - val_loss: 0.1090\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1127 - val_loss: 0.1138\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1120 - val_loss: 0.1108\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1123 - val_loss: 0.1047\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1114 - val_loss: 0.1313\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1113 - val_loss: 0.1065\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1126 - val_loss: 0.1151\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1107 - val_loss: 0.0999\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1133 - val_loss: 0.1084\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.1108 - val_loss: 0.1080\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1122 - val_loss: 0.1029\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1109 - val_loss: 0.1183\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1133 - val_loss: 0.1065\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1105 - val_loss: 0.1107\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1103 - val_loss: 0.1022\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1109 - val_loss: 0.1114\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1107 - val_loss: 0.1000\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1105 - val_loss: 0.1000\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.1094 - val_loss: 0.1112\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1119 - val_loss: 0.1068\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1138 - val_loss: 0.1072\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1102 - val_loss: 0.1118\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11016158014535904 , Final validation loss: 0.11176802963018417\n",
      "Training network # 30\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 733us/step - loss: 3.6869 - val_loss: 3.5738\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 3.6815 - val_loss: 3.6154\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 3.6785 - val_loss: 3.4846\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 1.9991 - val_loss: 0.3354\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.3264 - val_loss: 0.3271\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2598 - val_loss: 0.2606\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.2197 - val_loss: 0.2386\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2008 - val_loss: 0.1666\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1858 - val_loss: 0.1732\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1698 - val_loss: 0.1473\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1718 - val_loss: 0.2741\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1576 - val_loss: 0.1372\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1605 - val_loss: 0.1674\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1490 - val_loss: 0.1220\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1496 - val_loss: 0.1390\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1437 - val_loss: 0.1327\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1448 - val_loss: 0.1389\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1435 - val_loss: 0.2165\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1370 - val_loss: 0.1271\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1374 - val_loss: 0.1242\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1357 - val_loss: 0.1360\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1362 - val_loss: 0.1268\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1298 - val_loss: 0.1224\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1312 - val_loss: 0.1229\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1334 - val_loss: 0.1149\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1265 - val_loss: 0.1121\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1252 - val_loss: 0.1239\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1283 - val_loss: 0.1725\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1222 - val_loss: 0.1173\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1241 - val_loss: 0.1092\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1245 - val_loss: 0.1302\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1219 - val_loss: 0.1159\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1230 - val_loss: 0.1161\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1206 - val_loss: 0.1098\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1241 - val_loss: 0.1334\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1189 - val_loss: 0.1366\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1185 - val_loss: 0.1101\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1209 - val_loss: 0.1049\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1186 - val_loss: 0.1079\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1155 - val_loss: 0.1055\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1199 - val_loss: 0.1470\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1149 - val_loss: 0.1002\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1161 - val_loss: 0.1135\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1165 - val_loss: 0.1093\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1158 - val_loss: 0.1103\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1185 - val_loss: 0.1225\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 0.1157 - val_loss: 0.1019\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1160 - val_loss: 0.1140\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1155 - val_loss: 0.1002\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1136 - val_loss: 0.1166\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1144 - val_loss: 0.1099\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1141 - val_loss: 0.1160\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1150 - val_loss: 0.1153\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1139 - val_loss: 0.1002\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1124 - val_loss: 0.1593\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1156 - val_loss: 0.1208\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1124 - val_loss: 0.1078\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1115 - val_loss: 0.1356\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1125 - val_loss: 0.1037\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1147 - val_loss: 0.1070\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1144 - val_loss: 0.1206\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1116 - val_loss: 0.0976\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1109 - val_loss: 0.1828\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1123 - val_loss: 0.1058\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1105 - val_loss: 0.1028\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1110 - val_loss: 0.1069\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1099 - val_loss: 0.1262\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1095 - val_loss: 0.1183\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1107 - val_loss: 0.1034\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1111 - val_loss: 0.1033\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1104 - val_loss: 0.1188\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1126 - val_loss: 0.1086\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1075 - val_loss: 0.1241\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1084 - val_loss: 0.1326\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1109 - val_loss: 0.1040\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1076 - val_loss: 0.1066\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1092 - val_loss: 0.1046\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1101 - val_loss: 0.1295\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1089 - val_loss: 0.0945\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1075 - val_loss: 0.1105\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1105 - val_loss: 0.1234\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1080 - val_loss: 0.1043\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1096 - val_loss: 0.1077\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.1062 - val_loss: 0.1020\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1065 - val_loss: 0.1115\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1082 - val_loss: 0.1156\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1953 - val_loss: 0.1217\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1140 - val_loss: 0.1036\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1114 - val_loss: 0.1028\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1079 - val_loss: 0.1134\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1078 - val_loss: 0.1249\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1081 - val_loss: 0.1124\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1100 - val_loss: 0.0958\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1055 - val_loss: 0.0980\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1062 - val_loss: 0.1027\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1083 - val_loss: 0.1013\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1046 - val_loss: 0.1433\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1101 - val_loss: 0.0987\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1068 - val_loss: 0.1276\n",
      "Training completed. Number of epochs: 99 , Final training loss: 0.10676697641611099 , Final validation loss: 0.12758195400238037\n",
      "Training network # 31\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 1.5202 - val_loss: 0.4144\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.4013 - val_loss: 0.2580\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.2625 - val_loss: 0.2251\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.2378 - val_loss: 0.1883\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.2104 - val_loss: 0.2033\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1961 - val_loss: 0.1695\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1829 - val_loss: 0.1632\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1754 - val_loss: 0.1434\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1662 - val_loss: 0.1446\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1667 - val_loss: 0.1341\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1559 - val_loss: 0.1413\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1510 - val_loss: 0.1494\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1537 - val_loss: 0.1449\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1450 - val_loss: 0.1337\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1413 - val_loss: 0.1257\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1407 - val_loss: 0.1156\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1367 - val_loss: 0.1256\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1350 - val_loss: 0.1498\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1313 - val_loss: 0.1305\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1323 - val_loss: 0.1610\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1282 - val_loss: 0.1284\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1316 - val_loss: 0.1139\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1259 - val_loss: 0.1491\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1251 - val_loss: 0.1201\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1226 - val_loss: 0.1181\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1245 - val_loss: 0.1088\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1258 - val_loss: 0.1175\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1212 - val_loss: 0.1168\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1215 - val_loss: 0.1089\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1214 - val_loss: 0.1298\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1208 - val_loss: 0.1271\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1186 - val_loss: 0.1146\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1178 - val_loss: 0.1038\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1199 - val_loss: 0.1148\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1173 - val_loss: 0.1100\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1199 - val_loss: 0.1201\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1151 - val_loss: 0.1058\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1173 - val_loss: 0.1141\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1161 - val_loss: 0.1173\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1173 - val_loss: 0.1197\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1177 - val_loss: 0.1427\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1147 - val_loss: 0.1035\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1137 - val_loss: 0.1042\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1154 - val_loss: 0.1279\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1131 - val_loss: 0.1073\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1134 - val_loss: 0.0990\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1124 - val_loss: 0.0999\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1131 - val_loss: 0.1060\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1103 - val_loss: 0.1048\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1098 - val_loss: 0.1146\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1119 - val_loss: 0.1079\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1134 - val_loss: 0.1027\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1090 - val_loss: 0.0995\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1128 - val_loss: 0.1133\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1129 - val_loss: 0.1021\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1107 - val_loss: 0.1073\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1101 - val_loss: 0.1139\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1120 - val_loss: 0.1308\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1094 - val_loss: 0.0950\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1083 - val_loss: 0.0961\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1106 - val_loss: 0.1351\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1099 - val_loss: 0.1022\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1083 - val_loss: 0.1029\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1093 - val_loss: 0.0982\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1072 - val_loss: 0.0975\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1085 - val_loss: 0.1007\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1081 - val_loss: 0.0984\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1083 - val_loss: 0.1011\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1074 - val_loss: 0.1073\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1083 - val_loss: 0.1224\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1088 - val_loss: 0.1048\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1060 - val_loss: 0.0971\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1076 - val_loss: 0.1122\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1055 - val_loss: 0.1358\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1070 - val_loss: 0.1008\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1065 - val_loss: 0.1012\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1038 - val_loss: 0.1067\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1030 - val_loss: 0.1112\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1087 - val_loss: 0.1113\n",
      "Training completed. Number of epochs: 79 , Final training loss: 0.10868427157402039 , Final validation loss: 0.11134231835603714\n",
      "Training network # 32\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 2.8516 - val_loss: 0.5464\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.4169 - val_loss: 0.3052\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.2709 - val_loss: 0.2164\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.2167 - val_loss: 0.2166\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1988 - val_loss: 0.1907\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1883 - val_loss: 0.1605\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1775 - val_loss: 0.1636\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1721 - val_loss: 0.1521\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1598 - val_loss: 0.1426\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1606 - val_loss: 0.1688\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1564 - val_loss: 0.1313\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1487 - val_loss: 0.1391\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1476 - val_loss: 0.1379\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1430 - val_loss: 0.1321\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1421 - val_loss: 0.1165\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1365 - val_loss: 0.2191\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1360 - val_loss: 0.1980\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1345 - val_loss: 0.1214\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1323 - val_loss: 0.1384\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1302 - val_loss: 0.1175\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1263 - val_loss: 0.1166\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1275 - val_loss: 0.1226\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1291 - val_loss: 0.1130\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1234 - val_loss: 0.1073\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1238 - val_loss: 0.1095\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1232 - val_loss: 0.1121\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1222 - val_loss: 0.1135\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1243 - val_loss: 0.1273\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1191 - val_loss: 0.1062\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1197 - val_loss: 0.1019\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1219 - val_loss: 0.1194\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1186 - val_loss: 0.1109\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1173 - val_loss: 0.1282\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1178 - val_loss: 0.1010\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1171 - val_loss: 0.1173\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1166 - val_loss: 0.1079\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1178 - val_loss: 0.1109\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1157 - val_loss: 0.1179\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1141 - val_loss: 0.1092\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1142 - val_loss: 0.1155\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1175 - val_loss: 0.1016\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1172 - val_loss: 0.1496\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1135 - val_loss: 0.1050\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1141 - val_loss: 0.1096\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1151 - val_loss: 0.1237\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1162 - val_loss: 0.1019\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1149 - val_loss: 0.1123\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1147 - val_loss: 0.1054\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1149 - val_loss: 0.1442\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1132 - val_loss: 0.1023\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1135 - val_loss: 0.1035\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1138 - val_loss: 0.1544\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1128 - val_loss: 0.1077\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1128 - val_loss: 0.1080\n",
      "Training completed. Number of epochs: 54 , Final training loss: 0.11284627765417099 , Final validation loss: 0.10797546058893204\n",
      "Training network # 33\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 1.0310 - val_loss: 0.4669\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.4934 - val_loss: 0.4501\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 562us/step - loss: 0.3897 - val_loss: 0.2764\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.2681 - val_loss: 0.2325\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.2256 - val_loss: 0.2623\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.2155 - val_loss: 0.1788\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1996 - val_loss: 0.1819\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1933 - val_loss: 0.1721\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1795 - val_loss: 0.1607\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1722 - val_loss: 0.1633\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1655 - val_loss: 0.1442\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1634 - val_loss: 0.1404\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1583 - val_loss: 0.1507\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1562 - val_loss: 0.1540\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1479 - val_loss: 0.1425\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 559us/step - loss: 0.1560 - val_loss: 0.1598\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1501 - val_loss: 0.1501\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1467 - val_loss: 0.1705\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1482 - val_loss: 0.1447\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1435 - val_loss: 0.1403\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1489 - val_loss: 0.1275\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1430 - val_loss: 0.1311\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1401 - val_loss: 0.1357\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1409 - val_loss: 0.1247\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1414 - val_loss: 0.1322\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1407 - val_loss: 0.1201\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1418 - val_loss: 0.1272\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1362 - val_loss: 0.1265\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1394 - val_loss: 0.1299\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1357 - val_loss: 0.1329\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1371 - val_loss: 0.1359\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1379 - val_loss: 0.1233\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1367 - val_loss: 0.1283\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1360 - val_loss: 0.1413\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1352 - val_loss: 0.1516\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1365 - val_loss: 0.1273\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1355 - val_loss: 0.1220\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1374 - val_loss: 0.1208\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1331 - val_loss: 0.1387\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1321 - val_loss: 0.1397\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1326 - val_loss: 0.1237\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1316 - val_loss: 0.1184\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1326 - val_loss: 0.1227\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1339 - val_loss: 0.1340\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1314 - val_loss: 0.1219\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1300 - val_loss: 0.1348\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1327 - val_loss: 0.1244\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1312 - val_loss: 0.1476\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1293 - val_loss: 0.1395\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1287 - val_loss: 0.1274\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 563us/step - loss: 0.1359 - val_loss: 0.1586\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1316 - val_loss: 0.1351\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1333 - val_loss: 0.1270\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1305 - val_loss: 0.1254\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1277 - val_loss: 0.1205\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1276 - val_loss: 0.1442\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1287 - val_loss: 0.1174\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1303 - val_loss: 0.1350\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1275 - val_loss: 0.1251\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1298 - val_loss: 0.1176\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1262 - val_loss: 0.1242\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1278 - val_loss: 0.1414\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1272 - val_loss: 0.1381\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1291 - val_loss: 0.1484\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1281 - val_loss: 0.1308\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1281 - val_loss: 0.1267\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1251 - val_loss: 0.1211\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1285 - val_loss: 0.1961\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 565us/step - loss: 0.1304 - val_loss: 0.1452\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1266 - val_loss: 0.1155\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1260 - val_loss: 0.1187\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1275 - val_loss: 0.1200\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1249 - val_loss: 0.1266\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1273 - val_loss: 0.1425\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1246 - val_loss: 0.1305\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1256 - val_loss: 0.1179\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1227 - val_loss: 0.1414\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1254 - val_loss: 0.1270\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1239 - val_loss: 0.1218\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1235 - val_loss: 0.1134\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1232 - val_loss: 0.1357\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1234 - val_loss: 0.1191\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1213 - val_loss: 0.1195\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1244 - val_loss: 0.1142\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1216 - val_loss: 0.1230\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1226 - val_loss: 0.1178\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1226 - val_loss: 0.1145\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1208 - val_loss: 0.1255\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1215 - val_loss: 0.1131\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1213 - val_loss: 0.1425\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1211 - val_loss: 0.1338\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1232 - val_loss: 0.1233\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1207 - val_loss: 0.1154\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1207 - val_loss: 0.1118\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.5288 - val_loss: 0.1401\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1357 - val_loss: 0.2058\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1337 - val_loss: 0.1426\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1298 - val_loss: 0.1151\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1251 - val_loss: 0.1270\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1272 - val_loss: 0.1275\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.12718601524829865 , Final validation loss: 0.12752088904380798\n",
      "Training network # 34\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 3.7129 - val_loss: 3.4895\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 3.3128 - val_loss: 0.5120\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.4305 - val_loss: 0.3065\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.3028 - val_loss: 0.2839\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2513 - val_loss: 0.1996\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.2127 - val_loss: 0.1921\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1908 - val_loss: 0.1675\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1801 - val_loss: 0.2604\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1699 - val_loss: 0.1451\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1642 - val_loss: 0.1503\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1623 - val_loss: 0.1618\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1524 - val_loss: 0.1567\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1522 - val_loss: 0.1584\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1464 - val_loss: 0.1388\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1453 - val_loss: 0.1276\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1448 - val_loss: 0.1760\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1413 - val_loss: 0.1605\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1411 - val_loss: 0.1228\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1414 - val_loss: 0.1666\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1403 - val_loss: 0.1398\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1340 - val_loss: 0.1194\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1346 - val_loss: 0.1355\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1335 - val_loss: 0.1186\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1296 - val_loss: 0.1178\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1312 - val_loss: 0.1194\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1292 - val_loss: 0.1271\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1313 - val_loss: 0.1205\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1285 - val_loss: 0.1136\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1283 - val_loss: 0.1126\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1311 - val_loss: 0.1216\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1279 - val_loss: 0.1158\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1265 - val_loss: 0.1244\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1254 - val_loss: 0.1280\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1263 - val_loss: 0.1223\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1265 - val_loss: 0.1366\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1239 - val_loss: 0.1191\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1229 - val_loss: 0.1226\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1238 - val_loss: 0.1193\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1226 - val_loss: 0.1163\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1217 - val_loss: 0.1120\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1194 - val_loss: 0.1150\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1201 - val_loss: 0.1097\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1203 - val_loss: 0.1191\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1217 - val_loss: 0.1227\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1214 - val_loss: 0.1264\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1209 - val_loss: 0.1093\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1200 - val_loss: 0.1084\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1172 - val_loss: 0.1272\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1214 - val_loss: 0.1154\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1201 - val_loss: 0.1162\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1184 - val_loss: 0.1192\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1178 - val_loss: 0.1295\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1179 - val_loss: 0.1174\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1177 - val_loss: 0.1413\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1179 - val_loss: 0.1290\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1184 - val_loss: 0.1059\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1186 - val_loss: 0.1085\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1176 - val_loss: 0.1068\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1154 - val_loss: 0.1230\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1174 - val_loss: 0.1127\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1158 - val_loss: 0.1223\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1158 - val_loss: 0.0994\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1148 - val_loss: 0.1057\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1151 - val_loss: 0.1403\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1144 - val_loss: 0.1082\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1149 - val_loss: 0.1045\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1155 - val_loss: 0.1161\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1132 - val_loss: 0.1108\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1143 - val_loss: 0.1095\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1132 - val_loss: 0.1124\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1135 - val_loss: 0.1072\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1137 - val_loss: 0.1176\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1107 - val_loss: 0.1089\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1133 - val_loss: 0.1029\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1128 - val_loss: 0.1158\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1111 - val_loss: 0.1070\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1136 - val_loss: 0.1033\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1114 - val_loss: 0.1088\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1116 - val_loss: 0.1432\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1105 - val_loss: 0.1133\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1121 - val_loss: 0.1127\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1104 - val_loss: 0.1220\n",
      "Training completed. Number of epochs: 82 , Final training loss: 0.11044877022504807 , Final validation loss: 0.12200554460287094\n",
      "Training network # 35\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 2.8200 - val_loss: 0.4368\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.4690 - val_loss: 0.4075\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.3020 - val_loss: 0.2139\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2534 - val_loss: 0.2027\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.2228 - val_loss: 0.1906\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.2047 - val_loss: 0.2314\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1917 - val_loss: 0.1991\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1889 - val_loss: 0.1389\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1618 - val_loss: 0.1412\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1605 - val_loss: 0.2005\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1586 - val_loss: 0.1434\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1508 - val_loss: 0.1370\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1407 - val_loss: 0.1283\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1406 - val_loss: 0.1239\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1377 - val_loss: 0.1726\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 546us/step - loss: 0.1345 - val_loss: 0.1214\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1316 - val_loss: 0.1175\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1322 - val_loss: 0.1193\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1285 - val_loss: 0.1230\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1260 - val_loss: 0.1179\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1313 - val_loss: 0.1307\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1250 - val_loss: 0.1331\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1258 - val_loss: 0.1103\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1240 - val_loss: 0.1119\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1183 - val_loss: 0.1143\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1209 - val_loss: 0.1279\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1220 - val_loss: 0.1578\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1232 - val_loss: 0.1082\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1195 - val_loss: 0.1179\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1183 - val_loss: 0.1077\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1200 - val_loss: 0.1196\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1167 - val_loss: 0.1039\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1161 - val_loss: 0.1028\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1148 - val_loss: 0.1236\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1165 - val_loss: 0.1225\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1144 - val_loss: 0.1111\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1149 - val_loss: 0.1069\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1141 - val_loss: 0.1217\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1149 - val_loss: 0.1229\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1139 - val_loss: 0.1112\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1133 - val_loss: 0.1038\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1113 - val_loss: 0.1237\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1117 - val_loss: 0.0999\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1131 - val_loss: 0.1056\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1116 - val_loss: 0.0988\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1089 - val_loss: 0.1335\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1142 - val_loss: 0.1048\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1101 - val_loss: 0.1046\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1110 - val_loss: 0.1050\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1104 - val_loss: 0.1136\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1096 - val_loss: 0.1108\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1103 - val_loss: 0.1013\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1114 - val_loss: 0.0988\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1104 - val_loss: 0.1123\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1098 - val_loss: 0.1233\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1073 - val_loss: 0.1263\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1104 - val_loss: 0.1025\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1064 - val_loss: 0.0978\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1078 - val_loss: 0.1035\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1090 - val_loss: 0.1191\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1096 - val_loss: 0.0956\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1075 - val_loss: 0.1474\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1071 - val_loss: 0.1031\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1066 - val_loss: 0.1102\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1089 - val_loss: 0.1174\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1069 - val_loss: 0.0984\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1064 - val_loss: 0.0971\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1073 - val_loss: 0.0994\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1043 - val_loss: 0.1053\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1045 - val_loss: 0.1110\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1076 - val_loss: 0.1578\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1053 - val_loss: 0.1196\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1049 - val_loss: 0.1022\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1069 - val_loss: 0.0913\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1046 - val_loss: 0.1038\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1077 - val_loss: 0.1015\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1064 - val_loss: 0.1006\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1050 - val_loss: 0.0990\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1041 - val_loss: 0.0948\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1035 - val_loss: 0.0997\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1052 - val_loss: 0.1038\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1057 - val_loss: 0.1018\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1063 - val_loss: 0.0972\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1039 - val_loss: 0.1109\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1048 - val_loss: 0.1059\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1022 - val_loss: 0.0991\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1049 - val_loss: 0.1011\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1040 - val_loss: 0.1196\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1036 - val_loss: 0.1027\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1045 - val_loss: 0.1000\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1048 - val_loss: 0.0907\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1028 - val_loss: 0.0987\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1037 - val_loss: 0.1182\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1026 - val_loss: 0.1046\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1036 - val_loss: 0.1007\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1026 - val_loss: 0.1072\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1044 - val_loss: 0.1039\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1022 - val_loss: 0.0968\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1028 - val_loss: 0.0995\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1046 - val_loss: 0.1230\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10461322218179703 , Final validation loss: 0.12300065904855728\n",
      "Training network # 36\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 3.6583 - val_loss: 3.4847\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.9194 - val_loss: 0.4180\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.2729 - val_loss: 0.2194\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.2394 - val_loss: 0.2634\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.2099 - val_loss: 0.1812\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1908 - val_loss: 0.1547\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1741 - val_loss: 0.1615\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1655 - val_loss: 0.1686\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1621 - val_loss: 0.1904\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1573 - val_loss: 0.1544\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1553 - val_loss: 0.2058\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1488 - val_loss: 0.1364\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1468 - val_loss: 0.1257\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1424 - val_loss: 0.1441\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1400 - val_loss: 0.1234\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1358 - val_loss: 0.1132\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1327 - val_loss: 0.1143\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1326 - val_loss: 0.1276\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1290 - val_loss: 0.1110\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1304 - val_loss: 0.1225\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1261 - val_loss: 0.1180\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1269 - val_loss: 0.1053\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1277 - val_loss: 0.1240\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1240 - val_loss: 0.1286\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1238 - val_loss: 0.1075\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1218 - val_loss: 0.2344\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1194 - val_loss: 0.1150\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1218 - val_loss: 0.1150\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1218 - val_loss: 0.1094\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1220 - val_loss: 0.1231\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1171 - val_loss: 0.1103\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1178 - val_loss: 0.1190\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1186 - val_loss: 0.1339\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1197 - val_loss: 0.1082\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1178 - val_loss: 0.1023\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1169 - val_loss: 0.1139\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1143 - val_loss: 0.1072\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1130 - val_loss: 0.1069\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1156 - val_loss: 0.1091\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1144 - val_loss: 0.1353\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1152 - val_loss: 0.1246\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1141 - val_loss: 0.1020\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1119 - val_loss: 0.1013\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1102 - val_loss: 0.1075\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1098 - val_loss: 0.1011\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1130 - val_loss: 0.1133\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1120 - val_loss: 0.1263\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1096 - val_loss: 0.1002\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1096 - val_loss: 0.1128\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1115 - val_loss: 0.1312\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1061 - val_loss: 0.1115\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1095 - val_loss: 0.1319\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1059 - val_loss: 0.0936\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1070 - val_loss: 0.1227\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1069 - val_loss: 0.0938\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1089 - val_loss: 0.1025\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1045 - val_loss: 0.0988\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1069 - val_loss: 0.0947\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1075 - val_loss: 0.1066\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1045 - val_loss: 0.0933\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1051 - val_loss: 0.1001\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1046 - val_loss: 0.0997\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1036 - val_loss: 0.1013\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1047 - val_loss: 0.1011\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1072 - val_loss: 0.0984\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1032 - val_loss: 0.1137\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1057 - val_loss: 0.0968\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1054 - val_loss: 0.0996\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1052 - val_loss: 0.0954\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1031 - val_loss: 0.0984\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1047 - val_loss: 0.0967\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1034 - val_loss: 0.0949\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1006 - val_loss: 0.0991\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1026 - val_loss: 0.1098\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1026 - val_loss: 0.1133\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1017 - val_loss: 0.1014\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1030 - val_loss: 0.1232\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1029 - val_loss: 0.1120\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1023 - val_loss: 0.0982\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1050 - val_loss: 0.0999\n",
      "Training completed. Number of epochs: 80 , Final training loss: 0.10498373210430145 , Final validation loss: 0.0998522937297821\n",
      "Training network # 37\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 3.7161 - val_loss: 3.4879\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 2.6890 - val_loss: 0.3896\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.3707 - val_loss: 0.3056\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.2811 - val_loss: 0.2542\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.2364 - val_loss: 0.2158\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.2138 - val_loss: 0.1777\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.2025 - val_loss: 0.1980\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1817 - val_loss: 0.1551\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1748 - val_loss: 0.1515\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1635 - val_loss: 0.1430\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1546 - val_loss: 0.1419\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1509 - val_loss: 0.1636\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1457 - val_loss: 0.1235\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1417 - val_loss: 0.1954\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1397 - val_loss: 0.1296\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1357 - val_loss: 0.1409\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1319 - val_loss: 0.1148\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1300 - val_loss: 0.1170\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1261 - val_loss: 0.1220\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1271 - val_loss: 0.1042\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1230 - val_loss: 0.1115\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1233 - val_loss: 0.1377\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1206 - val_loss: 0.1375\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1224 - val_loss: 0.1358\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1173 - val_loss: 0.1326\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1181 - val_loss: 0.1364\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1180 - val_loss: 0.1243\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1176 - val_loss: 0.1164\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1172 - val_loss: 0.1027\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1169 - val_loss: 0.1121\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1141 - val_loss: 0.1112\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1151 - val_loss: 0.1019\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1125 - val_loss: 0.1104\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1125 - val_loss: 0.1061\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1150 - val_loss: 0.1040\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1122 - val_loss: 0.0987\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1128 - val_loss: 0.1228\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1134 - val_loss: 0.1143\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1116 - val_loss: 0.1033\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1110 - val_loss: 0.0999\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1114 - val_loss: 0.1320\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1108 - val_loss: 0.1134\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1119 - val_loss: 0.1152\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1067 - val_loss: 0.0993\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1093 - val_loss: 0.1007\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1110 - val_loss: 0.1207\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1096 - val_loss: 0.0991\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1096 - val_loss: 0.1007\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1099 - val_loss: 0.1125\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1089 - val_loss: 0.1079\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1117 - val_loss: 0.1078\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1068 - val_loss: 0.1116\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1086 - val_loss: 0.1004\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1058 - val_loss: 0.1033\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1067 - val_loss: 0.1186\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1079 - val_loss: 0.1050\n",
      "Training completed. Number of epochs: 56 , Final training loss: 0.10788498818874359 , Final validation loss: 0.10495913028717041\n",
      "Training network # 38\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 3.6913 - val_loss: 3.6973\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 2.2376 - val_loss: 0.4536\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.3846 - val_loss: 0.2709\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.2618 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.2235 - val_loss: 0.1844\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1941 - val_loss: 0.1629\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1848 - val_loss: 0.1554\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1660 - val_loss: 0.1425\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1630 - val_loss: 0.1420\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1532 - val_loss: 0.1554\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1489 - val_loss: 0.1383\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1395 - val_loss: 0.1173\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1405 - val_loss: 0.1239\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1348 - val_loss: 0.1226\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1316 - val_loss: 0.1370\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1355 - val_loss: 0.1126\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1283 - val_loss: 0.1414\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1282 - val_loss: 0.1095\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1250 - val_loss: 0.1427\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1254 - val_loss: 0.1077\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1247 - val_loss: 0.1123\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1215 - val_loss: 0.1297\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1231 - val_loss: 0.1111\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1193 - val_loss: 0.1249\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1193 - val_loss: 0.1361\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1181 - val_loss: 0.1070\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1188 - val_loss: 0.1418\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1171 - val_loss: 0.1328\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1162 - val_loss: 0.1042\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1157 - val_loss: 0.1251\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1158 - val_loss: 0.1143\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1137 - val_loss: 0.1036\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1166 - val_loss: 0.1332\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1168 - val_loss: 0.1072\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1124 - val_loss: 0.1040\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1167 - val_loss: 0.1113\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 567us/step - loss: 0.1151 - val_loss: 0.1066\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1129 - val_loss: 0.0995\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1117 - val_loss: 0.1251\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1121 - val_loss: 0.1080\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1112 - val_loss: 0.1068\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1104 - val_loss: 0.0965\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1129 - val_loss: 0.1271\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1122 - val_loss: 0.1087\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.1090 - val_loss: 0.1002\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1088 - val_loss: 0.1113\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1102 - val_loss: 0.1011\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1092 - val_loss: 0.1091\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1092 - val_loss: 0.1012\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 0.1093 - val_loss: 0.1251\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1076 - val_loss: 0.1329\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1100 - val_loss: 0.0940\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.1067 - val_loss: 0.0995\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1080 - val_loss: 0.0967\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1066 - val_loss: 0.1565\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1074 - val_loss: 0.1171\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1077 - val_loss: 0.1030\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1069 - val_loss: 0.1006\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1061 - val_loss: 0.1039\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1069 - val_loss: 0.0957\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1057 - val_loss: 0.0990\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1048 - val_loss: 0.1044\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1054 - val_loss: 0.1211\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1047 - val_loss: 0.1007\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1027 - val_loss: 0.0954\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1073 - val_loss: 0.0925\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.1044 - val_loss: 0.1107\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1042 - val_loss: 0.1042\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1030 - val_loss: 0.1062\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1025 - val_loss: 0.1398\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1036 - val_loss: 0.1045\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1029 - val_loss: 0.0978\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1047 - val_loss: 0.1210\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1036 - val_loss: 0.0980\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1018 - val_loss: 0.1107\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1036 - val_loss: 0.1061\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1049 - val_loss: 0.1007\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1021 - val_loss: 0.1016\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1024 - val_loss: 0.0911\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1003 - val_loss: 0.1142\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1020 - val_loss: 0.0963\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1022 - val_loss: 0.1146\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1034 - val_loss: 0.0994\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.0996 - val_loss: 0.0954\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1027 - val_loss: 0.0945\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1053 - val_loss: 0.1128\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1026 - val_loss: 0.1094\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1022 - val_loss: 0.1199\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1011 - val_loss: 0.0911\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1018 - val_loss: 0.1054\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1017 - val_loss: 0.0959\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1012 - val_loss: 0.0970\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.0986 - val_loss: 0.0907\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1019 - val_loss: 0.1060\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.0985 - val_loss: 0.1055\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1012 - val_loss: 0.1006\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1191 - val_loss: 0.1364\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1038 - val_loss: 0.0950\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1021 - val_loss: 0.1106\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.0985 - val_loss: 0.1275\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.0985342338681221 , Final validation loss: 0.12749814987182617\n",
      "Training network # 39\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 1.0756 - val_loss: 0.5478\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.4118 - val_loss: 0.3136\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.3084 - val_loss: 0.2717\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.2669 - val_loss: 0.2282\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.2354 - val_loss: 0.2020\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.2184 - val_loss: 0.2582\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.2033 - val_loss: 0.1667\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1888 - val_loss: 0.1605\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1831 - val_loss: 0.1649\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1699 - val_loss: 0.1721\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1675 - val_loss: 0.1414\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1640 - val_loss: 0.1787\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1604 - val_loss: 0.1573\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1569 - val_loss: 0.1359\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1559 - val_loss: 0.1474\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1477 - val_loss: 0.1842\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1529 - val_loss: 0.1590\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1483 - val_loss: 0.1667\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1499 - val_loss: 0.1783\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1435 - val_loss: 0.1358\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1446 - val_loss: 0.1473\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1436 - val_loss: 0.1264\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1448 - val_loss: 0.1514\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1390 - val_loss: 0.1234\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1394 - val_loss: 0.1291\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1407 - val_loss: 0.1442\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1393 - val_loss: 0.1329\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1385 - val_loss: 0.1289\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1352 - val_loss: 0.1188\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1358 - val_loss: 0.1226\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1344 - val_loss: 0.1284\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1362 - val_loss: 0.1370\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1365 - val_loss: 0.1305\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1338 - val_loss: 0.1443\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1319 - val_loss: 0.1445\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1421 - val_loss: 0.1270\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1371 - val_loss: 0.1337\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1365 - val_loss: 0.1355\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 639us/step - loss: 0.1399 - val_loss: 0.1258\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1319 - val_loss: 0.1234\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1305 - val_loss: 0.1301\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1290 - val_loss: 0.1241\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1274 - val_loss: 0.1397\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.1306 - val_loss: 0.1287\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.1324 - val_loss: 0.1179\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1294 - val_loss: 0.1175\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1305 - val_loss: 0.1175\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1289 - val_loss: 0.1372\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1265 - val_loss: 0.1262\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1312 - val_loss: 0.1274\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1297 - val_loss: 0.2022\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1353 - val_loss: 0.1220\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1278 - val_loss: 0.1215\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1264 - val_loss: 0.1378\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1265 - val_loss: 0.1152\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1258 - val_loss: 0.1361\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1257 - val_loss: 0.1260\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1247 - val_loss: 0.1243\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1245 - val_loss: 0.1322\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1265 - val_loss: 0.1200\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1275 - val_loss: 0.1224\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1241 - val_loss: 0.1210\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1236 - val_loss: 0.1516\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.1248 - val_loss: 0.1215\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1236 - val_loss: 0.1374\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1294 - val_loss: 0.1136\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1280 - val_loss: 0.1262\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1232 - val_loss: 0.1140\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1293 - val_loss: 0.1110\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1200 - val_loss: 0.1288\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1220 - val_loss: 0.1147\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1229 - val_loss: 0.1346\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1248 - val_loss: 0.1200\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1197 - val_loss: 0.1249\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1191 - val_loss: 0.1107\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1204 - val_loss: 0.1114\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1216 - val_loss: 0.1071\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1189 - val_loss: 0.1131\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1184 - val_loss: 0.1096\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1199 - val_loss: 0.1079\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1177 - val_loss: 0.1156\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1168 - val_loss: 0.1454\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1176 - val_loss: 0.1183\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1168 - val_loss: 0.1100\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1150 - val_loss: 0.1079\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1161 - val_loss: 0.1110\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1161 - val_loss: 0.1229\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1181 - val_loss: 0.1099\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1138 - val_loss: 0.1180\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1240 - val_loss: 0.1134\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1158 - val_loss: 0.1056\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1148 - val_loss: 0.1114\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1128 - val_loss: 0.1093\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1145 - val_loss: 0.1174\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1139 - val_loss: 0.1226\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1118 - val_loss: 0.1067\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1179 - val_loss: 0.1112\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1122 - val_loss: 0.1092\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1128 - val_loss: 0.1119\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1105 - val_loss: 0.1032\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11047366261482239 , Final validation loss: 0.10316021740436554\n",
      "Training network # 40\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 1.0317 - val_loss: 0.5432\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.4716 - val_loss: 0.4053\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.3588 - val_loss: 0.2676\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.2743 - val_loss: 0.2519\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.2410 - val_loss: 0.1988\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.2121 - val_loss: 0.1869\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.2062 - val_loss: 0.1737\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1863 - val_loss: 0.1830\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1814 - val_loss: 0.1644\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1687 - val_loss: 0.1556\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1642 - val_loss: 0.1484\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1600 - val_loss: 0.1409\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1594 - val_loss: 0.1501\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1529 - val_loss: 0.1850\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1494 - val_loss: 0.1731\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1486 - val_loss: 0.1363\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1428 - val_loss: 0.1606\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1452 - val_loss: 0.1230\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 655us/step - loss: 0.1354 - val_loss: 0.1423\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1385 - val_loss: 0.1241\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1330 - val_loss: 0.1205\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 0.1344 - val_loss: 0.1286\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1366 - val_loss: 0.1229\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1334 - val_loss: 0.1279\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1314 - val_loss: 0.1408\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1282 - val_loss: 0.1278\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1274 - val_loss: 0.1407\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1271 - val_loss: 0.1149\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1269 - val_loss: 0.1122\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1249 - val_loss: 0.1102\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1241 - val_loss: 0.1271\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1261 - val_loss: 0.1184\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1228 - val_loss: 0.1373\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1256 - val_loss: 0.1160\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1232 - val_loss: 0.1387\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1212 - val_loss: 0.1411\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1235 - val_loss: 0.1196\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1192 - val_loss: 0.1066\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1240 - val_loss: 0.1226\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1196 - val_loss: 0.1857\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1207 - val_loss: 0.1196\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.1197 - val_loss: 0.1170\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1185 - val_loss: 0.1102\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1211 - val_loss: 0.1195\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1218 - val_loss: 0.1528\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1161 - val_loss: 0.1124\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1188 - val_loss: 0.1157\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1181 - val_loss: 0.1266\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1163 - val_loss: 0.1065\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1156 - val_loss: 0.1114\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1170 - val_loss: 0.1449\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1181 - val_loss: 0.1071\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1182 - val_loss: 0.1115\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1153 - val_loss: 0.1035\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1159 - val_loss: 0.1021\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1140 - val_loss: 0.1133\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1132 - val_loss: 0.1177\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1166 - val_loss: 0.1371\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1148 - val_loss: 0.1102\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1134 - val_loss: 0.1120\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1146 - val_loss: 0.1303\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1134 - val_loss: 0.1036\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1140 - val_loss: 0.1174\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1127 - val_loss: 0.1023\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1141 - val_loss: 0.1211\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1127 - val_loss: 0.1131\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1113 - val_loss: 0.1211\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1129 - val_loss: 0.1019\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1126 - val_loss: 0.1042\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1118 - val_loss: 0.1090\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1108 - val_loss: 0.1017\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1134 - val_loss: 0.1070\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1115 - val_loss: 0.1021\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1117 - val_loss: 0.1087\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1125 - val_loss: 0.1264\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1117 - val_loss: 0.1085\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1088 - val_loss: 0.1314\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1109 - val_loss: 0.1077\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1119 - val_loss: 0.1188\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1114 - val_loss: 0.1218\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.1113 - val_loss: 0.1159\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1304 - val_loss: 0.1088\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1098 - val_loss: 0.1481\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1113 - val_loss: 0.1009\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1141 - val_loss: 0.1052\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1088 - val_loss: 0.1300\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1094 - val_loss: 0.1093\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1096 - val_loss: 0.1008\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1118 - val_loss: 0.1137\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1088 - val_loss: 0.1330\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1078 - val_loss: 0.1092\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1102 - val_loss: 0.1186\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1096 - val_loss: 0.1096\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1085 - val_loss: 0.0999\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1114 - val_loss: 0.1038\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1088 - val_loss: 0.1007\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1087 - val_loss: 0.1066\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1075 - val_loss: 0.1198\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1092 - val_loss: 0.1070\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1070 - val_loss: 0.1080\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10698546469211578 , Final validation loss: 0.10800591856241226\n",
      "Training network # 41\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 3.6522 - val_loss: 3.9308\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 2.0931 - val_loss: 0.4353\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.3460 - val_loss: 0.2543\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.2776 - val_loss: 0.2569\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.2423 - val_loss: 0.2144\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.2188 - val_loss: 0.1912\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.2003 - val_loss: 0.1699\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1794 - val_loss: 0.1703\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1721 - val_loss: 0.1464\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1592 - val_loss: 0.1344\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1537 - val_loss: 0.1898\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1461 - val_loss: 0.1263\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1378 - val_loss: 0.1334\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1402 - val_loss: 0.1205\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1336 - val_loss: 0.1144\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1304 - val_loss: 0.1186\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1326 - val_loss: 0.1235\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1335 - val_loss: 0.1159\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1265 - val_loss: 0.1046\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1243 - val_loss: 0.1090\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1241 - val_loss: 0.1205\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1245 - val_loss: 0.1181\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1230 - val_loss: 0.1102\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1211 - val_loss: 0.1063\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1180 - val_loss: 0.1046\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1206 - val_loss: 0.1106\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1186 - val_loss: 0.1061\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1173 - val_loss: 0.1113\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1172 - val_loss: 0.1133\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1186 - val_loss: 0.1357\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1156 - val_loss: 0.1043\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1168 - val_loss: 0.1335\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1166 - val_loss: 0.0982\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1136 - val_loss: 0.1131\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1161 - val_loss: 0.1404\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1148 - val_loss: 0.1130\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1149 - val_loss: 0.1217\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1127 - val_loss: 0.1089\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1141 - val_loss: 0.1092\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1131 - val_loss: 0.1186\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1133 - val_loss: 0.1002\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1115 - val_loss: 0.0998\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1131 - val_loss: 0.1183\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1119 - val_loss: 0.1006\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1116 - val_loss: 0.0970\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1114 - val_loss: 0.0978\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1125 - val_loss: 0.1115\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1111 - val_loss: 0.1229\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1125 - val_loss: 0.1041\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1091 - val_loss: 0.1307\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1090 - val_loss: 0.1063\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1102 - val_loss: 0.0953\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1076 - val_loss: 0.1137\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1089 - val_loss: 0.1015\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1097 - val_loss: 0.0996\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1089 - val_loss: 0.1057\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1080 - val_loss: 0.1071\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1109 - val_loss: 0.1057\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1067 - val_loss: 0.1233\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1084 - val_loss: 0.1036\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1076 - val_loss: 0.1239\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1085 - val_loss: 0.1241\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1057 - val_loss: 0.1033\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1062 - val_loss: 0.0968\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1064 - val_loss: 0.1030\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1089 - val_loss: 0.0978\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1052 - val_loss: 0.1237\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1065 - val_loss: 0.0951\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1059 - val_loss: 0.0993\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1042 - val_loss: 0.1124\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1049 - val_loss: 0.1239\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1049 - val_loss: 0.0982\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1042 - val_loss: 0.1080\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1031 - val_loss: 0.1119\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1057 - val_loss: 0.0972\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1042 - val_loss: 0.1163\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1019 - val_loss: 0.1150\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1101 - val_loss: 0.0936\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1039 - val_loss: 0.1010\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1038 - val_loss: 0.1112\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1046 - val_loss: 0.1212\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1032 - val_loss: 0.1096\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1041 - val_loss: 0.1129\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1038 - val_loss: 0.1059\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1037 - val_loss: 0.1083\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1030 - val_loss: 0.0922\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1039 - val_loss: 0.0926\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1061 - val_loss: 0.0929\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1030 - val_loss: 0.0967\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1026 - val_loss: 0.1369\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1041 - val_loss: 0.1053\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1033 - val_loss: 0.0909\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1011 - val_loss: 0.0987\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1020 - val_loss: 0.0955\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1014 - val_loss: 0.1006\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1030 - val_loss: 0.0894\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1005 - val_loss: 0.0938\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1010 - val_loss: 0.1196\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1019 - val_loss: 0.0947\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1010 - val_loss: 0.1035\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10095586627721786 , Final validation loss: 0.10353503376245499\n",
      "Training network # 42\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 3.6564 - val_loss: 3.5429\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 717us/step - loss: 1.3837 - val_loss: 0.3916\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.3996 - val_loss: 0.2743\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.2823 - val_loss: 0.3881\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.2487 - val_loss: 0.2459\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.2136 - val_loss: 0.1962\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1982 - val_loss: 0.1715\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1846 - val_loss: 0.1580\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1711 - val_loss: 0.1686\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1560 - val_loss: 0.1720\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1500 - val_loss: 0.1353\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1431 - val_loss: 0.1274\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1443 - val_loss: 0.1189\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1433 - val_loss: 0.1243\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1355 - val_loss: 0.1606\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1321 - val_loss: 0.1364\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1294 - val_loss: 0.1176\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1281 - val_loss: 0.1160\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1261 - val_loss: 0.1108\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1250 - val_loss: 0.1176\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1258 - val_loss: 0.1133\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1252 - val_loss: 0.1197\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1217 - val_loss: 0.1155\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1205 - val_loss: 0.1042\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1182 - val_loss: 0.1122\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1160 - val_loss: 0.1369\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 566us/step - loss: 0.1185 - val_loss: 0.1283\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1181 - val_loss: 0.1067\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1170 - val_loss: 0.0974\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1146 - val_loss: 0.1079\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1144 - val_loss: 0.1276\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1129 - val_loss: 0.1069\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1147 - val_loss: 0.1033\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1150 - val_loss: 0.1070\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1123 - val_loss: 0.1048\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1120 - val_loss: 0.1022\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1140 - val_loss: 0.1019\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1123 - val_loss: 0.1079\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 564us/step - loss: 0.1110 - val_loss: 0.1124\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1103 - val_loss: 0.1037\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.1132 - val_loss: 0.1299\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1116 - val_loss: 0.1051\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1118 - val_loss: 0.1038\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1082 - val_loss: 0.1302\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 569us/step - loss: 0.1103 - val_loss: 0.1151\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1097 - val_loss: 0.1040\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1114 - val_loss: 0.1119\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1060 - val_loss: 0.0975\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1079 - val_loss: 0.1070\n",
      "Training completed. Number of epochs: 49 , Final training loss: 0.1078685000538826 , Final validation loss: 0.10695650428533554\n",
      "Training network # 43\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 1.2036 - val_loss: 0.4340\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.4769 - val_loss: 0.4665\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.3857 - val_loss: 0.2603\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.2780 - val_loss: 0.2220\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.2251 - val_loss: 0.2380\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.2072 - val_loss: 0.2132\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1997 - val_loss: 0.1761\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1838 - val_loss: 0.1707\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1822 - val_loss: 0.1754\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1702 - val_loss: 0.1506\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1634 - val_loss: 0.1895\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1634 - val_loss: 0.1569\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1615 - val_loss: 0.1452\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1534 - val_loss: 0.1412\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1533 - val_loss: 0.1427\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1511 - val_loss: 0.1422\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1493 - val_loss: 0.1865\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1494 - val_loss: 0.1387\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1461 - val_loss: 0.1857\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1461 - val_loss: 0.1670\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1448 - val_loss: 0.1350\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1435 - val_loss: 0.1318\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1461 - val_loss: 0.1227\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1398 - val_loss: 0.1700\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1431 - val_loss: 0.1231\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1426 - val_loss: 0.1255\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1374 - val_loss: 0.1783\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1421 - val_loss: 0.1418\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1398 - val_loss: 0.1224\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1391 - val_loss: 0.1614\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1367 - val_loss: 0.1260\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1393 - val_loss: 0.1404\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1352 - val_loss: 0.1443\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1361 - val_loss: 0.1336\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1330 - val_loss: 0.1246\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1325 - val_loss: 0.1208\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1327 - val_loss: 0.1428\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1358 - val_loss: 0.1282\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1306 - val_loss: 0.1339\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1329 - val_loss: 0.1242\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1339 - val_loss: 0.1483\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1315 - val_loss: 0.1280\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1326 - val_loss: 0.1461\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1318 - val_loss: 0.1232\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1300 - val_loss: 0.1321\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1288 - val_loss: 0.1212\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1277 - val_loss: 0.1253\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1297 - val_loss: 0.1355\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1280 - val_loss: 0.1182\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1297 - val_loss: 0.1166\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1277 - val_loss: 0.1120\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1282 - val_loss: 0.1195\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1267 - val_loss: 0.1263\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1267 - val_loss: 0.1349\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1270 - val_loss: 0.1353\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1278 - val_loss: 0.1412\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1263 - val_loss: 0.1272\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1250 - val_loss: 0.1385\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1303 - val_loss: 0.1629\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1230 - val_loss: 0.1219\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1253 - val_loss: 0.1215\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1265 - val_loss: 0.1198\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1270 - val_loss: 0.1241\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1212 - val_loss: 0.1382\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1220 - val_loss: 0.1224\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1236 - val_loss: 0.1207\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1232 - val_loss: 0.1116\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1223 - val_loss: 0.1176\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1249 - val_loss: 0.1345\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1211 - val_loss: 0.1147\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1229 - val_loss: 0.1105\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1203 - val_loss: 0.1405\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1204 - val_loss: 0.1093\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1206 - val_loss: 0.1308\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1187 - val_loss: 0.1240\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 0.1188 - val_loss: 0.1098\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1187 - val_loss: 0.1061\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1362 - val_loss: 0.1235\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1165 - val_loss: 0.1225\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1257 - val_loss: 0.1186\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1147 - val_loss: 0.1357\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1162 - val_loss: 0.1175\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1173 - val_loss: 0.1114\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1171 - val_loss: 0.1070\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1157 - val_loss: 0.1055\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1169 - val_loss: 0.1030\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1159 - val_loss: 0.1194\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1165 - val_loss: 0.1140\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1154 - val_loss: 0.1583\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.1152 - val_loss: 0.1084\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1169 - val_loss: 0.1100\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1165 - val_loss: 0.1201\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1135 - val_loss: 0.1053\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1148 - val_loss: 0.1126\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1137 - val_loss: 0.1067\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1155 - val_loss: 0.1209\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1164 - val_loss: 0.1057\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1130 - val_loss: 0.1120\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1156 - val_loss: 0.1150\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1121 - val_loss: 0.1093\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11214909702539444 , Final validation loss: 0.10930755734443665\n",
      "Training network # 44\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 1.0779 - val_loss: 0.4160\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.4334 - val_loss: 0.3539\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.3054 - val_loss: 0.2245\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.2519 - val_loss: 0.1997\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.2353 - val_loss: 0.2997\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.2093 - val_loss: 0.2022\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.2018 - val_loss: 0.2155\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1917 - val_loss: 0.1934\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1896 - val_loss: 0.1641\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1823 - val_loss: 0.2271\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1714 - val_loss: 0.1489\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1686 - val_loss: 0.1812\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1685 - val_loss: 0.1422\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1608 - val_loss: 0.1715\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1605 - val_loss: 0.1603\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1595 - val_loss: 0.1368\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1549 - val_loss: 0.1422\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.1549 - val_loss: 0.1382\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1500 - val_loss: 0.1602\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1510 - val_loss: 0.1432\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1475 - val_loss: 0.1867\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1477 - val_loss: 0.1406\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1466 - val_loss: 0.1247\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1458 - val_loss: 0.1482\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1455 - val_loss: 0.1311\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1414 - val_loss: 0.1369\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1417 - val_loss: nan\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1406 - val_loss: 0.1395\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1421 - val_loss: 0.1484\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1363 - val_loss: 0.1453\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 647us/step - loss: 0.1362 - val_loss: 0.1250\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 0.1369 - val_loss: 0.1221\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1341 - val_loss: 0.1599\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1362 - val_loss: 0.1264\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1342 - val_loss: 0.1213\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1321 - val_loss: 0.1402\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 645us/step - loss: 0.1352 - val_loss: 0.1490\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1313 - val_loss: 0.1358\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1360 - val_loss: 0.1244\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.1329 - val_loss: 0.1269\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.1337 - val_loss: 0.1372\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1317 - val_loss: 0.1214\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1334 - val_loss: 0.1629\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1336 - val_loss: 0.1527\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1272 - val_loss: 0.1743\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1306 - val_loss: 0.1174\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1299 - val_loss: 0.1470\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.1312 - val_loss: 0.1425\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1274 - val_loss: 0.1149\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1272 - val_loss: 0.1545\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1275 - val_loss: 0.1208\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1260 - val_loss: 0.1256\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1298 - val_loss: 0.1396\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1285 - val_loss: 0.1197\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1260 - val_loss: 0.1261\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1261 - val_loss: 0.1193\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.1248 - val_loss: 0.1321\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1277 - val_loss: 0.1183\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1240 - val_loss: 0.1185\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 0.1242 - val_loss: 0.1256\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.1264 - val_loss: 0.1446\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1236 - val_loss: 0.1205\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1230 - val_loss: 0.1387\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1262 - val_loss: 0.1165\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1223 - val_loss: 0.1303\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1224 - val_loss: 0.1645\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1229 - val_loss: 0.1268\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1218 - val_loss: 0.1138\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1229 - val_loss: 0.1185\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1216 - val_loss: 0.1190\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1224 - val_loss: 0.1112\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1193 - val_loss: 0.1232\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1192 - val_loss: 0.1168\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 619us/step - loss: 0.1187 - val_loss: 0.1130\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1208 - val_loss: 0.1583\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1182 - val_loss: 0.1342\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1173 - val_loss: 0.1168\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1205 - val_loss: 0.1195\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1188 - val_loss: 0.1167\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1173 - val_loss: 0.1124\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1172 - val_loss: 0.1237\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1176 - val_loss: 0.1053\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1497 - val_loss: 0.1341\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1189 - val_loss: 0.1053\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1164 - val_loss: 0.1205\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1171 - val_loss: 0.1187\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1181 - val_loss: 0.1224\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1148 - val_loss: 0.1163\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1189 - val_loss: 0.1056\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1159 - val_loss: 0.1092\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1140 - val_loss: 0.1155\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 0.1178 - val_loss: 0.1211\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1168 - val_loss: 0.1076\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1155 - val_loss: 0.1470\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.1157 - val_loss: 0.1217\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1161 - val_loss: 0.1369\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.1125 - val_loss: 0.1103\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1124 - val_loss: 0.1162\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1124 - val_loss: 0.1091\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 0.1133 - val_loss: 0.1147\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.11329646408557892 , Final validation loss: 0.11469361931085587\n",
      "Training network # 45\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 1.4633 - val_loss: 0.4085\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.4149 - val_loss: 0.2706\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.2744 - val_loss: 0.2581\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.2393 - val_loss: 0.2536\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.2199 - val_loss: 0.1896\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1953 - val_loss: 0.1672\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1828 - val_loss: 0.1490\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1697 - val_loss: 0.1511\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1682 - val_loss: 0.1689\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1648 - val_loss: 0.1509\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1527 - val_loss: 0.1398\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1480 - val_loss: 0.1325\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1477 - val_loss: 0.1348\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1486 - val_loss: 0.1302\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1411 - val_loss: 0.1200\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1387 - val_loss: 0.1253\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1330 - val_loss: 0.1223\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1375 - val_loss: 0.1265\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1348 - val_loss: 0.1314\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1292 - val_loss: 0.2292\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1312 - val_loss: 0.1259\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1306 - val_loss: 0.1161\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1270 - val_loss: 0.1165\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1299 - val_loss: 0.1209\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1231 - val_loss: 0.1148\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1275 - val_loss: 0.1480\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1234 - val_loss: 0.1136\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1208 - val_loss: 0.1916\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1212 - val_loss: 0.1208\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1239 - val_loss: 0.1147\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1184 - val_loss: 0.1230\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1195 - val_loss: 0.1157\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1177 - val_loss: 0.1367\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1193 - val_loss: 0.1225\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1187 - val_loss: 0.1127\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 571us/step - loss: 0.1174 - val_loss: 0.1065\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1167 - val_loss: 0.1022\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1172 - val_loss: 0.1123\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1164 - val_loss: 0.1091\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1165 - val_loss: 0.1228\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1138 - val_loss: 0.1170\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1155 - val_loss: 0.1043\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1156 - val_loss: 0.1451\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1122 - val_loss: 0.1055\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1156 - val_loss: 0.1152\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1126 - val_loss: 0.1154\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1124 - val_loss: 0.1050\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1118 - val_loss: 0.1040\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1150 - val_loss: 0.1115\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1116 - val_loss: 0.1192\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1123 - val_loss: 0.1123\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 0.1115 - val_loss: 0.1070\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1173 - val_loss: 0.1280\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1110 - val_loss: 0.1015\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1107 - val_loss: 0.1078\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1094 - val_loss: 0.1073\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1101 - val_loss: 0.1011\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1084 - val_loss: 0.1081\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1106 - val_loss: 0.1147\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1074 - val_loss: 0.1108\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1090 - val_loss: 0.1008\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1102 - val_loss: 0.1079\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1089 - val_loss: 0.1126\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1069 - val_loss: 0.1181\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1086 - val_loss: 0.1033\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1087 - val_loss: 0.0955\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1070 - val_loss: 0.0961\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1064 - val_loss: 0.1083\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1092 - val_loss: 0.1035\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1082 - val_loss: 0.1040\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1086 - val_loss: 0.1126\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1081 - val_loss: 0.0966\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1048 - val_loss: 0.1114\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1062 - val_loss: 0.1156\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1073 - val_loss: 0.1160\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1044 - val_loss: 0.0971\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1054 - val_loss: 0.1043\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1070 - val_loss: 0.0961\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1062 - val_loss: 0.1294\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1045 - val_loss: 0.1060\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1048 - val_loss: 0.1161\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1055 - val_loss: 0.1032\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1046 - val_loss: 0.1203\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1048 - val_loss: 0.0970\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1061 - val_loss: 0.1061\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1050 - val_loss: 0.1479\n",
      "Training completed. Number of epochs: 86 , Final training loss: 0.10503687709569931 , Final validation loss: 0.14791239798069\n",
      "Training network # 46\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 1.8364 - val_loss: 0.4112\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 0.4116 - val_loss: 0.3020\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.2862 - val_loss: 0.2431\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.2344 - val_loss: 0.2498\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.2158 - val_loss: 0.2084\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.2013 - val_loss: 0.1629\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1827 - val_loss: 0.2020\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1782 - val_loss: 0.2988\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1733 - val_loss: 0.1601\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1611 - val_loss: 0.1547\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1628 - val_loss: 0.1609\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1529 - val_loss: 0.1716\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1487 - val_loss: 0.2233\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1497 - val_loss: 0.1458\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1425 - val_loss: 0.1242\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1469 - val_loss: 0.1414\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1383 - val_loss: 0.1399\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1352 - val_loss: 0.1601\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1383 - val_loss: 0.1221\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1338 - val_loss: 0.1353\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1321 - val_loss: 0.1194\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 592us/step - loss: 0.1321 - val_loss: 0.1236\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 0.1292 - val_loss: 0.1520\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1306 - val_loss: 0.1165\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1271 - val_loss: 0.1242\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1295 - val_loss: 0.1418\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1272 - val_loss: 0.1143\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1229 - val_loss: 0.1099\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 0.1221 - val_loss: 0.1120\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1212 - val_loss: 0.1388\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1193 - val_loss: 0.1063\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1189 - val_loss: 0.1132\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1166 - val_loss: 0.1283\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1165 - val_loss: 0.1062\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1152 - val_loss: 0.1032\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 620us/step - loss: 0.1153 - val_loss: 0.1124\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1130 - val_loss: 0.1075\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1149 - val_loss: 0.1175\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1151 - val_loss: 0.1126\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1122 - val_loss: 0.1103\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1126 - val_loss: 0.1245\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1112 - val_loss: 0.1137\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1103 - val_loss: 0.1337\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 0.1133 - val_loss: 0.0997\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 585us/step - loss: 0.1101 - val_loss: 0.0995\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1084 - val_loss: 0.1020\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1085 - val_loss: 0.1224\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1106 - val_loss: 0.1048\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1107 - val_loss: 0.1037\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1070 - val_loss: 0.1035\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1078 - val_loss: 0.1086\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1090 - val_loss: 0.1159\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1097 - val_loss: 0.0995\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.1057 - val_loss: 0.1023\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1087 - val_loss: 0.1023\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1070 - val_loss: 0.1489\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1061 - val_loss: 0.1010\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1049 - val_loss: 0.0966\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1050 - val_loss: 0.1108\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1057 - val_loss: 0.0955\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1051 - val_loss: 0.1292\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1048 - val_loss: 0.1051\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 0.1088 - val_loss: 0.0973\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1045 - val_loss: 0.1092\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1050 - val_loss: 0.0994\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1034 - val_loss: 0.1244\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1062 - val_loss: 0.0973\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1036 - val_loss: 0.0953\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1013 - val_loss: 0.0953\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1047 - val_loss: 0.1109\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1072 - val_loss: 0.1077\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 0.1034 - val_loss: 0.1142\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1039 - val_loss: 0.1049\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1018 - val_loss: 0.0987\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1027 - val_loss: 0.1088\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 0.1017 - val_loss: 0.1113\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1025 - val_loss: 0.1055\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1031 - val_loss: 0.0963\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1004 - val_loss: 0.0982\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.1046 - val_loss: 0.1105\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1027 - val_loss: 0.1065\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1010 - val_loss: 0.0915\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1033 - val_loss: 0.1225\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1023 - val_loss: 0.1118\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 600us/step - loss: 0.1018 - val_loss: 0.1134\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1002 - val_loss: 0.0969\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1014 - val_loss: 0.0905\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1411 - val_loss: 0.1218\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1027 - val_loss: 0.0993\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1028 - val_loss: 0.1056\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1026 - val_loss: 0.0995\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1024 - val_loss: 0.0926\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1018 - val_loss: 0.1069\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1028 - val_loss: 0.1014\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.0998 - val_loss: 0.1006\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 604us/step - loss: 0.1040 - val_loss: 0.1006\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1014 - val_loss: 0.0983\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1019 - val_loss: 0.1109\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1033 - val_loss: 0.0911\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.0988 - val_loss: 0.0936\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.09884940087795258 , Final validation loss: 0.09355863928794861\n",
      "Training network # 47\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 645us/step - loss: 1.1324 - val_loss: 0.4315\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.4499 - val_loss: 0.3605\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.3253 - val_loss: 0.2771\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.2559 - val_loss: 0.2319\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.2302 - val_loss: 0.2605\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.2175 - val_loss: 0.1946\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.2058 - val_loss: 0.2100\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1945 - val_loss: 0.1995\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1823 - val_loss: 0.1543\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1774 - val_loss: 0.1869\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1695 - val_loss: 0.1697\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1612 - val_loss: 0.1538\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1661 - val_loss: 0.1483\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 607us/step - loss: 0.1538 - val_loss: 0.1523\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1519 - val_loss: 0.1670\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1472 - val_loss: 0.2227\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1488 - val_loss: 0.1342\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1433 - val_loss: 0.1754\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1462 - val_loss: 0.1924\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1372 - val_loss: 0.1234\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1403 - val_loss: 0.1271\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1353 - val_loss: 0.1478\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.1347 - val_loss: 0.1222\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1340 - val_loss: 0.1300\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1359 - val_loss: 0.1354\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 0.1294 - val_loss: 0.1303\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1318 - val_loss: 0.1178\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1299 - val_loss: 0.1467\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1287 - val_loss: 0.1303\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 578us/step - loss: 0.1277 - val_loss: 0.1221\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1249 - val_loss: 0.1173\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1270 - val_loss: 0.1399\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1259 - val_loss: 0.1160\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1237 - val_loss: 0.1168\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1245 - val_loss: 0.1178\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 572us/step - loss: 0.1242 - val_loss: 0.1342\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 0.1219 - val_loss: 0.1202\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1224 - val_loss: 0.1166\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.1231 - val_loss: 0.1160\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1242 - val_loss: 0.1127\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1202 - val_loss: 0.1373\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1191 - val_loss: 0.1185\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.1201 - val_loss: 0.1328\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1193 - val_loss: 0.1216\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1188 - val_loss: 0.1365\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1205 - val_loss: 0.1124\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1191 - val_loss: 0.1117\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1184 - val_loss: 0.1422\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1173 - val_loss: 0.1143\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 0.1167 - val_loss: 0.1028\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 588us/step - loss: 0.1174 - val_loss: 0.1142\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1152 - val_loss: 0.1036\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1153 - val_loss: 0.1060\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1162 - val_loss: 0.1449\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1149 - val_loss: 0.1059\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1148 - val_loss: 0.1037\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1155 - val_loss: 0.1104\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1133 - val_loss: 0.1383\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 0.1134 - val_loss: 0.1205\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 0.1139 - val_loss: 0.1064\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1104 - val_loss: 0.1048\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1116 - val_loss: 0.1124\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1127 - val_loss: 0.1218\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.1119 - val_loss: 0.1009\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1139 - val_loss: 0.1059\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1116 - val_loss: 0.1181\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 0.1109 - val_loss: 0.1171\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1107 - val_loss: 0.1104\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1088 - val_loss: 0.1126\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1113 - val_loss: 0.1028\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1084 - val_loss: 0.1032\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.1107 - val_loss: 0.1595\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 583us/step - loss: 0.1106 - val_loss: 0.1054\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 573us/step - loss: 0.1097 - val_loss: 0.1198\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1091 - val_loss: 0.1106\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1088 - val_loss: 0.0998\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1102 - val_loss: 0.1018\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 581us/step - loss: 0.1091 - val_loss: 0.1031\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 0.1074 - val_loss: 0.1100\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 0.1070 - val_loss: 0.1042\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1099 - val_loss: 0.1091\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 0.1066 - val_loss: 0.1066\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 577us/step - loss: 0.1094 - val_loss: 0.1003\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 0.1055 - val_loss: 0.1003\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1069 - val_loss: 0.0975\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1061 - val_loss: 0.1071\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 0.1055 - val_loss: 0.0963\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.1040 - val_loss: 0.1008\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1067 - val_loss: 0.0989\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 575us/step - loss: 0.1099 - val_loss: 0.1013\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 0.1050 - val_loss: 0.0986\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 570us/step - loss: 0.1051 - val_loss: 0.1086\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 0.1077 - val_loss: 0.1062\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1046 - val_loss: 0.1011\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 0.1055 - val_loss: 0.1162\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 579us/step - loss: 0.1054 - val_loss: 0.1018\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 0.1051 - val_loss: 0.0998\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 0.1034 - val_loss: 0.1045\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 589us/step - loss: 0.1035 - val_loss: 0.1061\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.1068 - val_loss: 0.1000\n",
      "Training completed. Number of epochs: 100 , Final training loss: 0.10676224529743195 , Final validation loss: 0.10001420229673386\n",
      "Total training time: 2471.171231031418 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    \"\"\"Creates a network with the original architecture.\"\"\"\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed = None)\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(26, input_shape=[PCA_components], activation='sigmoid', kernel_initializer= initializer),\n",
    "        layers.Dense(26, activation='sigmoid', kernel_initializer= initializer),\n",
    "        layers.Dense(1, activation = 'linear', kernel_initializer= initializer)\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = model_learning_rate)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingCompletionCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to display training information only at the end of the training of one network.\"\"\"\n",
    "    def on_train_end(self, logs=None):\n",
    "        epochs = len(self.model.history.history['loss'])\n",
    "        final_loss = self.model.history.history['loss'][-1]\n",
    "        final_val_loss = self.model.history.history['val_loss'][-1]\n",
    "\n",
    "        print(\"Training completed. Number of epochs:\", epochs, \", Final training loss:\", final_loss, \", Final validation loss:\", final_val_loss)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_epochs, restore_best_weights=True)\n",
    "completion_callback = TrainingCompletionCallback()\n",
    "\n",
    "start_time = time.time() # Measure training time\n",
    "\n",
    "# Train a committee of 48 neural networks\n",
    "committee = []\n",
    "for i in range(N_committee):\n",
    "    model = create_model()\n",
    "    print(\"Training network #\",i)\n",
    "    learning = model.fit(x_train, y_train, epochs= N_epochs, verbose = 1, callbacks = [completion_callback, early_stopping], validation_data=(x_val,y_val))\n",
    "    committee.append(model)   \n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Total training time:\", training_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ec270266/anaconda3/envs/wavediff/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the committee models\n",
    "\n",
    "os.makedirs(\"../../../output/committee_models\", exist_ok=True)\n",
    "\n",
    "for i, model in enumerate(committee):\n",
    "    model.save(f\"../../../output/committee_models/model_{i}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a trained committee\n",
    "\n",
    "# committee = []\n",
    "# committee_path = '/Users/ec270266/Documents/Phd/Euclid/dev/output/committee_models'\n",
    "# # Load each model in the committee\n",
    "# for i in range(N_committee):\n",
    "#     model_path = os.path.join(committee_path, f'model_{i}.h5')\n",
    "#     loaded_model = tf.keras.models.load_model(model_path)\n",
    "#     committee.append(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.10742900632790547\n",
      "Variance:  [0.00409181 0.00988584 0.00653295 ... 0.00831788 0.01086278 0.00857842]\n",
      "\n",
      "F1 score: [0.52983871 0.59655938 0.56265271 0.55289053 0.63655172 0.62390101\n",
      " 0.5748544  0.42767296 0.47767748 0.48930157 0.92341913 0.97285068\n",
      " 0.99934768 0.        ]\n",
      "Average F1 score: 0.6436552279231085\n",
      "\n",
      "Confusion matrix:\n",
      "tf.Tensor(\n",
      "[[ 657  761   49    4    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 337 1075  105   18    5    1    0    0    0    0    0    0    0    2]\n",
      " [  12  174  806  414   56    9    0    0    0    0    0    0    0    1]\n",
      " [   2   45  396  899  225   26    8    1    1    0    0    0    0    0]\n",
      " [   0    6   34  290  923  175   21    7    0    0    0    0    0    0]\n",
      " [   1    0    3   21  208  958  324   43   16    2    0    0    0    0]\n",
      " [   0    0    0    2   22  294  839  324   50    4    0    0    0    0]\n",
      " [   0    0    0    1    2   21  135  646  657   73    7    0    0    0]\n",
      " [   0    0    0    0    2   10   47  374  979  166   14    0    0    1]\n",
      " [   0    0    0    0    1    0   10   83  796  606   38    0    0    0]\n",
      " [   0    0    0    0    0    1    0    1    7   92 1453   53    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0   28 1505    2    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0 1532    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]], shape=(14, 14), dtype=int32)\n",
      "\n",
      "Success rate: 0.9623\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "x=x_test\n",
    "y_true=y_test\n",
    "SED_true=SED_test\n",
    "\n",
    "committee_predictions = []\n",
    "for model in committee:\n",
    "    committee_predictions.append(model.predict(x, verbose = 0).reshape(-1)) # Predict the scalar parameter C using the committee   \n",
    "committee_predictions = np.array(committee_predictions)\n",
    "y_pred = np.mean(committee_predictions, axis=0)\n",
    "pred_variance = np.var(committee_predictions, axis=0)\n",
    "SED_pred = CtoSEDarray(y_pred,pred_variance)\n",
    "\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print('MSE:', mse)\n",
    "print(\"Variance: \", pred_variance)\n",
    "\n",
    "f1 = f1_score(SED_true, SED_pred, average = None)\n",
    "f1_mean = np.mean(f1[:13])\n",
    "print('\\nF1 score:', f1)\n",
    "print('Average F1 score:', f1_mean)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(SED_true, SED_pred) \n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "success_rate = calculate_success_rate(confusion_matrix)\n",
    "print('\\nSuccess rate:', success_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "star_class_labels = ['O5','B0','B5','A0','A5','F0','F5','G0','G5','K0','K5','M0','M5']\n",
    "\n",
    "plt.figure(figsize= (12,10))\n",
    "heatmap = plt.imshow(confusion_matrix[:13,:], cmap='Blues')\n",
    "plt.xticks(np.arange(14), star_class_labels + ['???'])\n",
    "plt.yticks(np.arange(13), star_class_labels)\n",
    "plt.colorbar(heatmap)\n",
    "plt.xlabel(\"Estimated spectral type\")\n",
    "plt.ylabel(\"True spectral type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x36be1d3d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the metrics\n",
    "\n",
    "plt.figure(figsize = (9,5))\n",
    "plt.bar(np.arange(13), height = f1[:13], tick_label = star_class_labels ,label = \"F1 score\")\n",
    "plt.axhline(f1_mean, color='red', linestyle='--', label = 'F1 score average')\n",
    "plt.axhline(success_rate, color='purple', label = 'Success rate')\n",
    "plt.xlabel(\"Spectral class\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 2471.171231031418 seconds\n",
      "Training loss: 0.10676224529743195 , Validation loss: 0.10001420229673386\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLot the loss function evolution\n",
    "\n",
    "loss_evolution = learning.history[\"loss\"]\n",
    "val_loss_evolution = learning.history[\"val_loss\"]\n",
    "\n",
    "plt.figure(figsize = (9,5))\n",
    "plt.plot(loss_evolution,label = \"Train set\")\n",
    "plt.plot(val_loss_evolution,label = \"Validation set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss function value\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss function evolution\")\n",
    "print(\"Total training time:\", training_time, \"seconds\")\n",
    "print(\"Training loss:\", loss_evolution[-1], \", Validation loss:\", val_loss_evolution[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 vs 0\n",
      "\u001b[4m7 vs 7\u001b[0m\n",
      "0 vs 1\n",
      "6 vs 4\n",
      "\u001b[4m8 vs 8\u001b[0m\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "\u001b[4m1 vs 1\u001b[0m\n",
      "\u001b[4m12 vs 12\u001b[0m\n",
      "9 vs 8\n",
      "\u001b[4m5 vs 5\u001b[0m\n",
      "7 vs 8\n",
      "9 vs 8\n",
      "0 vs 1\n",
      "9 vs 8\n",
      "\u001b[4m2 vs 2\u001b[0m\n",
      "\u001b[4m7 vs 7\u001b[0m\n",
      "2 vs 4\n",
      "2 vs 1\n",
      "1 vs 0\n",
      "8 vs 7\n",
      "4 vs 3\n",
      "\u001b[4m10 vs 10\u001b[0m\n",
      "9 vs 8\n",
      "\u001b[4m3 vs 3\u001b[0m\n",
      "\u001b[4m9 vs 9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    pred=str(SED_pred[i])\n",
    "    test=str(SED_true[i])\n",
    "    if pred == test:\n",
    "        print('\\033[4m'+test+' vs '+pred+'\\033[0m')\n",
    "    else:\n",
    "        print(test+' vs '+pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
